{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9032458e503ab28519db53568226f597adad35e1b11ccc360aee2243f83ff687"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Load Necessary Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessarily libraries for the binary classification task\n",
    "\n",
    "# libraries imported for data processing and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# libraries imported for learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import pipeline\n",
    "\n",
    "# libraries imported for performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "source": [
    "### Load & Clean Adult Income Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0       39   77516             13          2174             0              40   \n",
       "1       50   83311             13             0             0              13   \n",
       "2       38  215646              9             0             0              40   \n",
       "3       53  234721              7             0             0              40   \n",
       "4       28  338409             13             0             0              40   \n",
       "...    ...     ...            ...           ...           ...             ...   \n",
       "32556   27  257302             12             0             0              38   \n",
       "32557   40  154374              9             0             0              40   \n",
       "32558   58  151910              9             0             0              40   \n",
       "32559   22  201490              9             0             0              20   \n",
       "32560   52  287927              9         15024             0              40   \n",
       "\n",
       "       workclass_ Federal-gov  workclass_ Local-gov  workclass_ Private  \\\n",
       "0                           0                     0                   0   \n",
       "1                           0                     0                   0   \n",
       "2                           0                     0                   1   \n",
       "3                           0                     0                   1   \n",
       "4                           0                     0                   1   \n",
       "...                       ...                   ...                 ...   \n",
       "32556                       0                     0                   1   \n",
       "32557                       0                     0                   1   \n",
       "32558                       0                     0                   1   \n",
       "32559                       0                     0                   1   \n",
       "32560                       0                     0                   0   \n",
       "\n",
       "       workclass_ Self-emp-inc  ...  native country_ Puerto-Rico  \\\n",
       "0                            0  ...                            0   \n",
       "1                            0  ...                            0   \n",
       "2                            0  ...                            0   \n",
       "3                            0  ...                            0   \n",
       "4                            0  ...                            0   \n",
       "...                        ...  ...                          ...   \n",
       "32556                        0  ...                            0   \n",
       "32557                        0  ...                            0   \n",
       "32558                        0  ...                            0   \n",
       "32559                        0  ...                            0   \n",
       "32560                        1  ...                            0   \n",
       "\n",
       "       native country_ Scotland  native country_ South  \\\n",
       "0                             0                      0   \n",
       "1                             0                      0   \n",
       "2                             0                      0   \n",
       "3                             0                      0   \n",
       "4                             0                      0   \n",
       "...                         ...                    ...   \n",
       "32556                         0                      0   \n",
       "32557                         0                      0   \n",
       "32558                         0                      0   \n",
       "32559                         0                      0   \n",
       "32560                         0                      0   \n",
       "\n",
       "       native country_ Taiwan  native country_ Thailand  \\\n",
       "0                           0                         0   \n",
       "1                           0                         0   \n",
       "2                           0                         0   \n",
       "3                           0                         0   \n",
       "4                           0                         0   \n",
       "...                       ...                       ...   \n",
       "32556                       0                         0   \n",
       "32557                       0                         0   \n",
       "32558                       0                         0   \n",
       "32559                       0                         0   \n",
       "32560                       0                         0   \n",
       "\n",
       "       native country_ Trinadad&Tobago  native country_ United-States  \\\n",
       "0                                    0                              1   \n",
       "1                                    0                              1   \n",
       "2                                    0                              1   \n",
       "3                                    0                              1   \n",
       "4                                    0                              0   \n",
       "...                                ...                            ...   \n",
       "32556                                0                              1   \n",
       "32557                                0                              1   \n",
       "32558                                0                              1   \n",
       "32559                                0                              1   \n",
       "32560                                0                              1   \n",
       "\n",
       "       native country_ Vietnam  native country_ Yugoslavia  income>50K  \n",
       "0                            0                           0           0  \n",
       "1                            0                           0           0  \n",
       "2                            0                           0           0  \n",
       "3                            0                           0           0  \n",
       "4                            0                           0           0  \n",
       "...                        ...                         ...         ...  \n",
       "32556                        0                           0           0  \n",
       "32557                        0                           0           1  \n",
       "32558                        0                           0           0  \n",
       "32559                        0                           0           0  \n",
       "32560                        0                           0           1  \n",
       "\n",
       "[30162 rows x 105 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>fnlwgt</th>\n      <th>education-num</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>workclass_ Federal-gov</th>\n      <th>workclass_ Local-gov</th>\n      <th>workclass_ Private</th>\n      <th>workclass_ Self-emp-inc</th>\n      <th>...</th>\n      <th>native country_ Puerto-Rico</th>\n      <th>native country_ Scotland</th>\n      <th>native country_ South</th>\n      <th>native country_ Taiwan</th>\n      <th>native country_ Thailand</th>\n      <th>native country_ Trinadad&amp;Tobago</th>\n      <th>native country_ United-States</th>\n      <th>native country_ Vietnam</th>\n      <th>native country_ Yugoslavia</th>\n      <th>income&gt;50K</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>77516</td>\n      <td>13</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>83311</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>215646</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>234721</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>338409</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>32556</th>\n      <td>27</td>\n      <td>257302</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32557</th>\n      <td>40</td>\n      <td>154374</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32558</th>\n      <td>58</td>\n      <td>151910</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32559</th>\n      <td>22</td>\n      <td>201490</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>32560</th>\n      <td>52</td>\n      <td>287927</td>\n      <td>9</td>\n      <td>15024</td>\n      <td>0</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>30162 rows × 105 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# load 'Adult Income Census' data and names into pandas dataframe\n",
    "\n",
    "# make array of column names (based on adult.names)\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "    'martial-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "    'capital-loss', 'hours-per-week', 'native country', 'income>50K']\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/Adult_Income/adult.data\", names=column_names)\n",
    "\n",
    "# clean data\n",
    "# replace all '?' entries with NaN\n",
    "df = df.replace(to_replace=\" ?\", value=np.NaN)\n",
    "# change focus value (adult income > or <=50K) into binary value\n",
    "df = df.replace(to_replace=\" >50K\", value=1)\n",
    "df = df.replace(to_replace=\" <=50K\", value=0)\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "# save new cleaned up data into csv into dataset folder\n",
    "df.to_csv(\"datasets/Adult_Income/adult.csv\", index=False)\n",
    "\n",
    "# one-hot encode the dataframe\n",
    "encoded = pd.get_dummies(df)\n",
    "\n",
    "# move binary classifier(label) column to the end\n",
    "# hold column\n",
    "classifier = encoded['income>50K']\n",
    "# drop column from dataframe\n",
    "encoded.drop(columns=['income>50K'], inplace=True)\n",
    "# reinsert into dataframe at the end\n",
    "encoded['income>50K'] = classifier\n",
    "\n",
    "adultDF = encoded\n",
    "adultDF"
   ]
  },
  {
   "source": [
    "### Load & Clean Electrical Grid Stability Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0     2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1     9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2     8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3     0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4     3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  2.930406  9.487627  2.376523  6.187797  3.343416 -0.658054 -1.449106   \n",
       "9996  3.392299  1.274827  2.954947  6.894759  4.349512 -1.663661 -0.952437   \n",
       "9997  2.364034  2.842030  8.776391  1.008906  4.299976 -1.380719 -0.943884   \n",
       "9998  9.631511  3.994398  2.757071  7.821347  2.514755 -0.966330 -0.649915   \n",
       "9999  6.530527  6.781790  4.349695  8.673138  3.492807 -1.390285 -1.532193   \n",
       "\n",
       "            p4        g1        g2        g3        g4      stab  stabf  \n",
       "0    -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347      0  \n",
       "1    -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957      1  \n",
       "2    -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471      0  \n",
       "3    -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871      0  \n",
       "4    -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "9995 -1.236256  0.601709  0.779642  0.813512  0.608385  0.023892      0  \n",
       "9996 -1.733414  0.502079  0.567242  0.285880  0.366120 -0.025803      1  \n",
       "9997 -1.975373  0.487838  0.986505  0.149286  0.145984 -0.031810      1  \n",
       "9998 -0.898510  0.365246  0.587558  0.889118  0.818391  0.037789      0  \n",
       "9999 -0.570329  0.073056  0.505441  0.378761  0.942631  0.045263      0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tau1</th>\n      <th>tau2</th>\n      <th>tau3</th>\n      <th>tau4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n      <th>g1</th>\n      <th>g2</th>\n      <th>g3</th>\n      <th>g4</th>\n      <th>stab</th>\n      <th>stabf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.959060</td>\n      <td>3.079885</td>\n      <td>8.381025</td>\n      <td>9.780754</td>\n      <td>3.763085</td>\n      <td>-0.782604</td>\n      <td>-1.257395</td>\n      <td>-1.723086</td>\n      <td>0.650456</td>\n      <td>0.859578</td>\n      <td>0.887445</td>\n      <td>0.958034</td>\n      <td>0.055347</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.304097</td>\n      <td>4.902524</td>\n      <td>3.047541</td>\n      <td>1.369357</td>\n      <td>5.067812</td>\n      <td>-1.940058</td>\n      <td>-1.872742</td>\n      <td>-1.255012</td>\n      <td>0.413441</td>\n      <td>0.862414</td>\n      <td>0.562139</td>\n      <td>0.781760</td>\n      <td>-0.005957</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.971707</td>\n      <td>8.848428</td>\n      <td>3.046479</td>\n      <td>1.214518</td>\n      <td>3.405158</td>\n      <td>-1.207456</td>\n      <td>-1.277210</td>\n      <td>-0.920492</td>\n      <td>0.163041</td>\n      <td>0.766689</td>\n      <td>0.839444</td>\n      <td>0.109853</td>\n      <td>0.003471</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.716415</td>\n      <td>7.669600</td>\n      <td>4.486641</td>\n      <td>2.340563</td>\n      <td>3.963791</td>\n      <td>-1.027473</td>\n      <td>-1.938944</td>\n      <td>-0.997374</td>\n      <td>0.446209</td>\n      <td>0.976744</td>\n      <td>0.929381</td>\n      <td>0.362718</td>\n      <td>0.028871</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.134112</td>\n      <td>7.608772</td>\n      <td>4.943759</td>\n      <td>9.857573</td>\n      <td>3.525811</td>\n      <td>-1.125531</td>\n      <td>-1.845975</td>\n      <td>-0.554305</td>\n      <td>0.797110</td>\n      <td>0.455450</td>\n      <td>0.656947</td>\n      <td>0.820923</td>\n      <td>0.049860</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>2.930406</td>\n      <td>9.487627</td>\n      <td>2.376523</td>\n      <td>6.187797</td>\n      <td>3.343416</td>\n      <td>-0.658054</td>\n      <td>-1.449106</td>\n      <td>-1.236256</td>\n      <td>0.601709</td>\n      <td>0.779642</td>\n      <td>0.813512</td>\n      <td>0.608385</td>\n      <td>0.023892</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>3.392299</td>\n      <td>1.274827</td>\n      <td>2.954947</td>\n      <td>6.894759</td>\n      <td>4.349512</td>\n      <td>-1.663661</td>\n      <td>-0.952437</td>\n      <td>-1.733414</td>\n      <td>0.502079</td>\n      <td>0.567242</td>\n      <td>0.285880</td>\n      <td>0.366120</td>\n      <td>-0.025803</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>2.364034</td>\n      <td>2.842030</td>\n      <td>8.776391</td>\n      <td>1.008906</td>\n      <td>4.299976</td>\n      <td>-1.380719</td>\n      <td>-0.943884</td>\n      <td>-1.975373</td>\n      <td>0.487838</td>\n      <td>0.986505</td>\n      <td>0.149286</td>\n      <td>0.145984</td>\n      <td>-0.031810</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>9.631511</td>\n      <td>3.994398</td>\n      <td>2.757071</td>\n      <td>7.821347</td>\n      <td>2.514755</td>\n      <td>-0.966330</td>\n      <td>-0.649915</td>\n      <td>-0.898510</td>\n      <td>0.365246</td>\n      <td>0.587558</td>\n      <td>0.889118</td>\n      <td>0.818391</td>\n      <td>0.037789</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>6.530527</td>\n      <td>6.781790</td>\n      <td>4.349695</td>\n      <td>8.673138</td>\n      <td>3.492807</td>\n      <td>-1.390285</td>\n      <td>-1.532193</td>\n      <td>-0.570329</td>\n      <td>0.073056</td>\n      <td>0.505441</td>\n      <td>0.378761</td>\n      <td>0.942631</td>\n      <td>0.045263</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# load 'Electrical Grid Stability' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/Grid_Stability/grid_stability.csv\")\n",
    "\n",
    "# clean data\n",
    "# replace string label classifiers into binary values\n",
    "df = df.replace(to_replace=\"stable\", value=1)\n",
    "df = df.replace(to_replace=\"unstable\", value=0)\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "gridDF = df\n",
    "gridDF"
   ]
  },
  {
   "source": [
    "### Load & Clean Occupancy Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Temperature  Humidity       Light         CO2  HumidityRatio  Occupancy\n",
       "0          23.7000   26.2720  585.200000  749.200000       0.004764          1\n",
       "1          23.7180   26.2900  578.400000  760.400000       0.004773          1\n",
       "2          23.7300   26.2300  572.666667  769.666667       0.004765          1\n",
       "3          23.7225   26.1250  493.750000  774.750000       0.004744          1\n",
       "4          23.7540   26.2000  488.600000  779.000000       0.004767          1\n",
       "...            ...       ...         ...         ...            ...        ...\n",
       "20555      21.0500   36.0975  433.000000  787.250000       0.005579          1\n",
       "20556      21.0500   35.9950  433.000000  789.500000       0.005563          1\n",
       "20557      21.1000   36.0950  433.000000  798.500000       0.005596          1\n",
       "20558      21.1000   36.2600  433.000000  820.333333       0.005621          1\n",
       "20559      21.1000   36.2000  447.000000  821.000000       0.005612          1\n",
       "\n",
       "[20560 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Temperature</th>\n      <th>Humidity</th>\n      <th>Light</th>\n      <th>CO2</th>\n      <th>HumidityRatio</th>\n      <th>Occupancy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23.7000</td>\n      <td>26.2720</td>\n      <td>585.200000</td>\n      <td>749.200000</td>\n      <td>0.004764</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23.7180</td>\n      <td>26.2900</td>\n      <td>578.400000</td>\n      <td>760.400000</td>\n      <td>0.004773</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23.7300</td>\n      <td>26.2300</td>\n      <td>572.666667</td>\n      <td>769.666667</td>\n      <td>0.004765</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23.7225</td>\n      <td>26.1250</td>\n      <td>493.750000</td>\n      <td>774.750000</td>\n      <td>0.004744</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23.7540</td>\n      <td>26.2000</td>\n      <td>488.600000</td>\n      <td>779.000000</td>\n      <td>0.004767</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20555</th>\n      <td>21.0500</td>\n      <td>36.0975</td>\n      <td>433.000000</td>\n      <td>787.250000</td>\n      <td>0.005579</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20556</th>\n      <td>21.0500</td>\n      <td>35.9950</td>\n      <td>433.000000</td>\n      <td>789.500000</td>\n      <td>0.005563</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20557</th>\n      <td>21.1000</td>\n      <td>36.0950</td>\n      <td>433.000000</td>\n      <td>798.500000</td>\n      <td>0.005596</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20558</th>\n      <td>21.1000</td>\n      <td>36.2600</td>\n      <td>433.000000</td>\n      <td>820.333333</td>\n      <td>0.005621</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20559</th>\n      <td>21.1000</td>\n      <td>36.2000</td>\n      <td>447.000000</td>\n      <td>821.000000</td>\n      <td>0.005612</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>20560 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# load 'Occupancy' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "first = pd.read_csv(\"datasets/Occupancy/datatest.csv\")\n",
    "second = pd.read_csv(\"datasets/Occupancy/datatest2.csv\")\n",
    "third = pd.read_csv(\"datasets/Occupancy/datatraining.csv\")\n",
    "\n",
    "# clean data\n",
    "# concatenate all three data csv\n",
    "df = pd.concat([first, second, third])\n",
    "# reset index for dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# drop date data (unscalable)\n",
    "df.drop(columns=['date'], inplace=True)\n",
    "df.dropna()\n",
    "\n",
    "occupancyDF = df\n",
    "occupancyDF"
   ]
  },
  {
   "source": [
    "### Load & Clean HTRU2 Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         mean_int  stddev_int  excess_int  skew_int  mean_dmsnr  stddev_dmsnr  \\\n",
       "0      140.562500   55.683782   -0.234571 -0.699648    3.199833     19.110426   \n",
       "1      102.507812   58.882430    0.465318 -0.515088    1.677258     14.860146   \n",
       "2      103.015625   39.341649    0.323328  1.051164    3.121237     21.744669   \n",
       "3      136.750000   57.178449   -0.068415 -0.636238    3.642977     20.959280   \n",
       "4       88.726562   40.672225    0.600866  1.123492    1.178930     11.468720   \n",
       "...           ...         ...         ...       ...         ...           ...   \n",
       "17893  136.429688   59.847421   -0.187846 -0.738123    1.296823     12.166062   \n",
       "17894  122.554688   49.485605    0.127978  0.323061   16.409699     44.626893   \n",
       "17895  119.335938   59.935939    0.159363 -0.743025   21.430602     58.872000   \n",
       "17896  114.507812   53.902400    0.201161 -0.024789    1.946488     13.381731   \n",
       "17897   57.062500   85.797340    1.406391  0.089520  188.306020     64.712562   \n",
       "\n",
       "       excess_dmsnr  skew_dmsnr  class  \n",
       "0          7.975532   74.242225      0  \n",
       "1         10.576487  127.393580      0  \n",
       "2          7.735822   63.171909      0  \n",
       "3          6.896499   53.593661      0  \n",
       "4         14.269573  252.567306      0  \n",
       "...             ...         ...    ...  \n",
       "17893     15.450260  285.931022      0  \n",
       "17894      2.945244    8.297092      0  \n",
       "17895      2.499517    4.595173      0  \n",
       "17896     10.007967  134.238910      0  \n",
       "17897     -1.597527    1.429475      0  \n",
       "\n",
       "[17898 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_int</th>\n      <th>stddev_int</th>\n      <th>excess_int</th>\n      <th>skew_int</th>\n      <th>mean_dmsnr</th>\n      <th>stddev_dmsnr</th>\n      <th>excess_dmsnr</th>\n      <th>skew_dmsnr</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>140.562500</td>\n      <td>55.683782</td>\n      <td>-0.234571</td>\n      <td>-0.699648</td>\n      <td>3.199833</td>\n      <td>19.110426</td>\n      <td>7.975532</td>\n      <td>74.242225</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>102.507812</td>\n      <td>58.882430</td>\n      <td>0.465318</td>\n      <td>-0.515088</td>\n      <td>1.677258</td>\n      <td>14.860146</td>\n      <td>10.576487</td>\n      <td>127.393580</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>103.015625</td>\n      <td>39.341649</td>\n      <td>0.323328</td>\n      <td>1.051164</td>\n      <td>3.121237</td>\n      <td>21.744669</td>\n      <td>7.735822</td>\n      <td>63.171909</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>136.750000</td>\n      <td>57.178449</td>\n      <td>-0.068415</td>\n      <td>-0.636238</td>\n      <td>3.642977</td>\n      <td>20.959280</td>\n      <td>6.896499</td>\n      <td>53.593661</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>88.726562</td>\n      <td>40.672225</td>\n      <td>0.600866</td>\n      <td>1.123492</td>\n      <td>1.178930</td>\n      <td>11.468720</td>\n      <td>14.269573</td>\n      <td>252.567306</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17893</th>\n      <td>136.429688</td>\n      <td>59.847421</td>\n      <td>-0.187846</td>\n      <td>-0.738123</td>\n      <td>1.296823</td>\n      <td>12.166062</td>\n      <td>15.450260</td>\n      <td>285.931022</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17894</th>\n      <td>122.554688</td>\n      <td>49.485605</td>\n      <td>0.127978</td>\n      <td>0.323061</td>\n      <td>16.409699</td>\n      <td>44.626893</td>\n      <td>2.945244</td>\n      <td>8.297092</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17895</th>\n      <td>119.335938</td>\n      <td>59.935939</td>\n      <td>0.159363</td>\n      <td>-0.743025</td>\n      <td>21.430602</td>\n      <td>58.872000</td>\n      <td>2.499517</td>\n      <td>4.595173</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17896</th>\n      <td>114.507812</td>\n      <td>53.902400</td>\n      <td>0.201161</td>\n      <td>-0.024789</td>\n      <td>1.946488</td>\n      <td>13.381731</td>\n      <td>10.007967</td>\n      <td>134.238910</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17897</th>\n      <td>57.062500</td>\n      <td>85.797340</td>\n      <td>1.406391</td>\n      <td>0.089520</td>\n      <td>188.306020</td>\n      <td>64.712562</td>\n      <td>-1.597527</td>\n      <td>1.429475</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17898 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# load 'Electrical Grid Stability' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/HTRU2/HTRU_2.csv\")\n",
    "\n",
    "# clean data\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "htru2DF = df\n",
    "htru2DF"
   ]
  },
  {
   "source": [
    "### Declare & Initialize Algorithm Parameters and Parameter-Grid"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-declared values/arrays/functions to be used once inside the trial loop\n",
    "# C values for logistic regression regularization in range of 10(-8) to 10(4)\n",
    "Cvals = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# K values for k-nearest neighbors in range of 1 to 105 in steps of 4\n",
    "Kvals = np.linspace(1, 105, num=26, dtype=int).tolist()\n",
    "# max feature values for random forest similar to CNM06\n",
    "max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "# max depth values for decision trees (shallower = better)\n",
    "max_depths = np.linspace(1, 5, num=5, dtype=int).tolist()\n",
    "# array of performance metrics\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "# build parameter grids to be passed into GridSearchCV\n",
    "logreg_pgrid = {'classifier__penalty': ['l1','l2','none'], 'classifier__C': Cvals, 'classifier__max_iter': [5000]}\n",
    "knn_pgrid = {'classifier__weights': ['distance'], 'classifier__n_neighbors': Kvals}\n",
    "rforest_pgrid = {'classifier__n_estimators': [1024], 'classifier__max_features': max_features}\n",
    "dtree_pgrid = {'classifier__max_depth': max_depths}"
   ]
  },
  {
   "source": [
    "#### Create Data Structure to Store ALL Score Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most top level dictionary to hold each dataset\n",
    "## size of 4 (4 datasets) accessed by dataset name key value\n",
    "top_dict = {}\n",
    "\n",
    "# second top level array to hold the trials from each dataset\n",
    "## size of 5 (5 trials) accessed by trial number index\n",
    "### each index in array holds dictionary\n",
    "#### dictionary holds each trial's data\n",
    "##### each trial's data hold algorithms as key value\n",
    "###### accessing algorithm's key value results in another set of dictionaries\n",
    "####### those dictionaries hold all training and testing data scores resulted from fitting the\n",
    "####### model with best parameters for a specific scoring metric\n",
    "score_array = [{}, {}, {}, {}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    6.7s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   29.9s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   42.6s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    3.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    4.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   30.5s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.7s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    3.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    4.1s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   30.1s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.3s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    3.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    4.1s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   30.3s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.2s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    4.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    4.9s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   30.1s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.2s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   10.1s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   25.8s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   10.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   25.2s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   10.0s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   25.3s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   10.3s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   25.1s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   10.1s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   25.0s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.7s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.8s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   12.0s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.8s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.9s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.8s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.9s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.6s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.8s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.9s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.8s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.6s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   12.4s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.5s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   38.5s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.3s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   37.1s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.4s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   38.5s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.4s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   37.2s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 195 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    0.9s finished\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\howar\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.4s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   37.5s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for df, dataset in zip([adultDF, gridDF, occupancyDF, htru2DF], \n",
    "                ['Adult', 'Grid', 'Occupancy', 'HTRU2']):\n",
    "    # loop through this entire trial FIVE (5) times\n",
    "    for i in range(5):\n",
    "        # slice the dataframe to not include the binary classifier (label)\n",
    "        # last column is the label\n",
    "        X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "        # randomly pick 5000 samples with replacement for training set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "        # make pipeline for each algorithms to condense model call\n",
    "        logreg = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', LogisticRegression(n_jobs=-1))])\n",
    "        knn = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "        rforest = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "        dtree = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "        # 5-fold cross validation using Stratified KFold\n",
    "        k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "        # GridSearchCV classifier for each algorithm\n",
    "        logreg_clf = GridSearchCV(estimator=logreg, param_grid=logreg_pgrid, scoring=scoring, \n",
    "                                    n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "        knn_clf = GridSearchCV(estimator=knn, param_grid=knn_pgrid, scoring=scoring, \n",
    "                                    n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "        rforest_clf = GridSearchCV(estimator=rforest, param_grid=rforest_pgrid, scoring=scoring, \n",
    "                                    n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "        dtree_clf = GridSearchCV(estimator=dtree, param_grid=dtree_pgrid, scoring=scoring, \n",
    "                                    n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "\n",
    "\n",
    "\n",
    "        # for each classifier\n",
    "        for clf, clf_name in zip([logreg_clf, knn_clf, rforest_clf, dtree_clf], \n",
    "                    ['LogReg', 'KNN', 'Ran_For', 'Dec_Tree']):\n",
    "            # fit to training data of 5000 samples\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # get parameters for each scoring metric's best\n",
    "            best_acc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_accuracy']) ]\n",
    "            best_f1_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_f1_micro']) ]\n",
    "            best_roc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_roc_auc_ovr']) ]\n",
    "\n",
    "            # get pipeline based on current classifier\n",
    "            if (clf_name == 'LogReg'):\n",
    "                pipe = logreg\n",
    "            elif (clf_name == 'KNN'):\n",
    "                pipe = knn\n",
    "            elif (clf_name == 'Ran_For'):\n",
    "                pipe = rforest\n",
    "            elif (clf_name == 'Dec_Tree'):\n",
    "                pipe = dtree\n",
    "\n",
    "            # set pipeline parameters to the parameters for best accuracy\n",
    "            pipe.set_params(**best_acc_param)\n",
    "            # fit classifier with training data and new parameters for scoring metric\n",
    "            pipe.fit(X_train, y_train)\n",
    "            # get predictions for both training and testing data\n",
    "            y_train_pred = pipe.predict(X_train)\n",
    "            y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "            # get scores for all metrics from both training and testing data\n",
    "            acc_train = accuracy_score(y_train, y_train_pred)\n",
    "            f1_train = f1_score(y_train, y_train_pred)\n",
    "            roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "            acc_test = accuracy_score(y_test, y_test_pred)\n",
    "            f1_test = f1_score(y_test, y_test_pred)\n",
    "            roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "            # store all scores into a dictionary for accuracy metric\n",
    "            acc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                        'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "            \n",
    "            # do ^^^^^ all that for f1 score\n",
    "            pipe.set_params(**best_f1_param)\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_train_pred = pipe.predict(X_train)\n",
    "            y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "            acc_train = accuracy_score(y_train, y_train_pred)\n",
    "            f1_train = f1_score(y_train, y_train_pred)\n",
    "            roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "            acc_test = accuracy_score(y_test, y_test_pred)\n",
    "            f1_test = f1_score(y_test, y_test_pred)\n",
    "            roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "            f1_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                        'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "\n",
    "            # do ^^^^^ all that for roc_auc score\n",
    "            pipe.set_params(**best_roc_param)\n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_train_pred = pipe.predict(X_train)\n",
    "            y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "            acc_train = accuracy_score(y_train, y_train_pred)\n",
    "            f1_train = f1_score(y_train, y_train_pred)\n",
    "            roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "            acc_test = accuracy_score(y_test, y_test_pred)\n",
    "            f1_test = f1_score(y_test, y_test_pred)\n",
    "            roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "            roc_auc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                        'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "            # build final dictionary to store all scores from all three models and their best parameters\n",
    "            score_array[i][clf_name] = {'acc_dict': acc_dict, 'f1_dict': f1_dict, 'roc_auc_dict': roc_auc_dict}\n",
    "    top_dict[dataset] = score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Adult': [{'LogReg': {'acc_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'f1_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.883668903803132, 'roc_auc_train': 0.9139048460745325, 'acc_test': 0.9794541789424717, 'f1_test': 0.8792710706150343, 'roc_auc_test': 0.91139312507293}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9751124205303148, 'f1_test': 0.8469241773962805, 'roc_auc_test': 0.8792408265597675}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.978833927740735, 'f1_test': 0.8756264236902049, 'roc_auc_test': 0.9095059274874541}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9789114591409521, 'f1_test': 0.8761384335154827, 'roc_auc_test': 0.9099351120368104}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.8791704238052299, 'roc_auc_test': 0.9151309775553271}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'f1_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'roc_auc_dict': {'acc_train': 0.9828, 'f1_train': 0.9061135371179039, 'roc_auc_train': 0.9347809468406637, 'acc_test': 0.97790355093813, 'f1_test': 0.8730512249443206, 'roc_auc_test': 0.9163393736678289}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'f1_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.8904267589388696, 'roc_auc_train': 0.9204947337533976, 'acc_test': 0.9785238021398667, 'f1_test': 0.875505617977528, 'roc_auc_test': 0.9087629957619124}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9757326717320515, 'f1_test': 0.8525671219971738, 'roc_auc_test': 0.8810083019183776}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9785238021398667, 'f1_test': 0.8775961113566062, 'roc_auc_test': 0.9159825007062277}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8769298632554036, 'roc_auc_test': 0.9162771140019244}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8762749445676276, 'roc_auc_test': 0.9139972703352985}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8656716417910448, 'roc_auc_test': 0.9120849536884206}, 'f1_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8657894736842107, 'roc_auc_test': 0.9124649276328584}, 'roc_auc_dict': {'acc_train': 0.9834, 'f1_train': 0.9049255441008017, 'roc_auc_train': 0.9306932604398319, 'acc_test': 0.9764304543340053, 'f1_test': 0.8644067796610171, 'roc_auc_test': 0.9057107572817216}}}, {'LogReg': {'acc_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'f1_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'roc_auc_dict': {'acc_train': 0.9782, 'f1_train': 0.8768361581920904, 'roc_auc_train': 0.9044976734333857, 'acc_test': 0.9776709567374787, 'f1_test': 0.8660465116279069, 'roc_auc_test': 0.898173984280681}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9737168553264072, 'f1_test': 0.8337420304070623, 'roc_auc_test': 0.8645984023737369}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9797643045433401, 'f1_test': 0.881739918441323, 'roc_auc_test': 0.915607214132604}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.8824593128390597, 'roc_auc_test': 0.9168128840222122}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.882988298829883, 'roc_auc_test': 0.9187513272488141}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772057683361761, 'f1_test': 0.8650137741046832, 'roc_auc_test': 0.902182935657324}, 'f1_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772832997363933, 'f1_test': 0.8656579550664832, 'roc_auc_test': 0.9030009169016118}, 'roc_auc_dict': {'acc_train': 0.98, 'f1_train': 0.8863636363636365, 'roc_auc_train': 0.9073679321761451, 'acc_test': 0.9758102031322685, 'f1_test': 0.8518518518518519, 'roc_auc_test': 0.8839700754522619}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'roc_auc_dict': {'acc_train': 0.978, 'f1_train': 0.8635235732009925, 'roc_auc_train': 0.8982935380926139, 'acc_test': 0.9778260195379128, 'f1_test': 0.872207327971403, 'roc_auc_test': 0.9025418974291459}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9745697007287951, 'f1_test': 0.8470149253731344, 'roc_auc_test': 0.8754379060706958}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9796092417429059, 'f1_test': 0.8848007008322382, 'roc_auc_test': 0.9161794143604022}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8832309043020192, 'roc_auc_test': 0.9145624239490734}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.883128295254833, 'roc_auc_test': 0.9141902468157529}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'f1_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'roc_auc_dict': {'acc_train': 0.9822, 'f1_train': 0.8944246737841044, 'roc_auc_train': 0.9308276360436083, 'acc_test': 0.9790665219413862, 'f1_test': 0.8835202761000863, 'roc_auc_test': 0.9210905698447808}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.8874458874458874, 'roc_auc_train': 0.9164518996735541, 'acc_test': 0.9789114591409521, 'f1_test': 0.872539831302718, 'roc_auc_test': 0.9025269055972525}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9750348891300977, 'f1_test': 0.8429268292682927, 'roc_auc_test': 0.8741199982235643}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9794541789424717, 'f1_test': 0.8774849745723532, 'roc_auc_test': 0.9098848277597668}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792991161420376, 'f1_test': 0.8765603328710125, 'roc_auc_test': 0.9094074847152522}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8766233766233766, 'roc_auc_test': 0.9082733786324406}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'f1_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.9001051524710831, 'roc_auc_train': 0.9338592461327208, 'acc_test': 0.9776709567374787, 'f1_test': 0.8693284936479129, 'roc_auc_test': 0.9124359372918239}}}], 'Grid': [{'LogReg': {'acc_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'f1_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.883668903803132, 'roc_auc_train': 0.9139048460745325, 'acc_test': 0.9794541789424717, 'f1_test': 0.8792710706150343, 'roc_auc_test': 0.91139312507293}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9751124205303148, 'f1_test': 0.8469241773962805, 'roc_auc_test': 0.8792408265597675}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.978833927740735, 'f1_test': 0.8756264236902049, 'roc_auc_test': 0.9095059274874541}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9789114591409521, 'f1_test': 0.8761384335154827, 'roc_auc_test': 0.9099351120368104}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.8791704238052299, 'roc_auc_test': 0.9151309775553271}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'f1_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'roc_auc_dict': {'acc_train': 0.9828, 'f1_train': 0.9061135371179039, 'roc_auc_train': 0.9347809468406637, 'acc_test': 0.97790355093813, 'f1_test': 0.8730512249443206, 'roc_auc_test': 0.9163393736678289}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'f1_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.8904267589388696, 'roc_auc_train': 0.9204947337533976, 'acc_test': 0.9785238021398667, 'f1_test': 0.875505617977528, 'roc_auc_test': 0.9087629957619124}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9757326717320515, 'f1_test': 0.8525671219971738, 'roc_auc_test': 0.8810083019183776}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9785238021398667, 'f1_test': 0.8775961113566062, 'roc_auc_test': 0.9159825007062277}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8769298632554036, 'roc_auc_test': 0.9162771140019244}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8762749445676276, 'roc_auc_test': 0.9139972703352985}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8656716417910448, 'roc_auc_test': 0.9120849536884206}, 'f1_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8657894736842107, 'roc_auc_test': 0.9124649276328584}, 'roc_auc_dict': {'acc_train': 0.9834, 'f1_train': 0.9049255441008017, 'roc_auc_train': 0.9306932604398319, 'acc_test': 0.9764304543340053, 'f1_test': 0.8644067796610171, 'roc_auc_test': 0.9057107572817216}}}, {'LogReg': {'acc_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'f1_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'roc_auc_dict': {'acc_train': 0.9782, 'f1_train': 0.8768361581920904, 'roc_auc_train': 0.9044976734333857, 'acc_test': 0.9776709567374787, 'f1_test': 0.8660465116279069, 'roc_auc_test': 0.898173984280681}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9737168553264072, 'f1_test': 0.8337420304070623, 'roc_auc_test': 0.8645984023737369}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9797643045433401, 'f1_test': 0.881739918441323, 'roc_auc_test': 0.915607214132604}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.8824593128390597, 'roc_auc_test': 0.9168128840222122}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.882988298829883, 'roc_auc_test': 0.9187513272488141}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772057683361761, 'f1_test': 0.8650137741046832, 'roc_auc_test': 0.902182935657324}, 'f1_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772832997363933, 'f1_test': 0.8656579550664832, 'roc_auc_test': 0.9030009169016118}, 'roc_auc_dict': {'acc_train': 0.98, 'f1_train': 0.8863636363636365, 'roc_auc_train': 0.9073679321761451, 'acc_test': 0.9758102031322685, 'f1_test': 0.8518518518518519, 'roc_auc_test': 0.8839700754522619}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'roc_auc_dict': {'acc_train': 0.978, 'f1_train': 0.8635235732009925, 'roc_auc_train': 0.8982935380926139, 'acc_test': 0.9778260195379128, 'f1_test': 0.872207327971403, 'roc_auc_test': 0.9025418974291459}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9745697007287951, 'f1_test': 0.8470149253731344, 'roc_auc_test': 0.8754379060706958}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9796092417429059, 'f1_test': 0.8848007008322382, 'roc_auc_test': 0.9161794143604022}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8832309043020192, 'roc_auc_test': 0.9145624239490734}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.883128295254833, 'roc_auc_test': 0.9141902468157529}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'f1_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'roc_auc_dict': {'acc_train': 0.9822, 'f1_train': 0.8944246737841044, 'roc_auc_train': 0.9308276360436083, 'acc_test': 0.9790665219413862, 'f1_test': 0.8835202761000863, 'roc_auc_test': 0.9210905698447808}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.8874458874458874, 'roc_auc_train': 0.9164518996735541, 'acc_test': 0.9789114591409521, 'f1_test': 0.872539831302718, 'roc_auc_test': 0.9025269055972525}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9750348891300977, 'f1_test': 0.8429268292682927, 'roc_auc_test': 0.8741199982235643}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9794541789424717, 'f1_test': 0.8774849745723532, 'roc_auc_test': 0.9098848277597668}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792991161420376, 'f1_test': 0.8765603328710125, 'roc_auc_test': 0.9094074847152522}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8766233766233766, 'roc_auc_test': 0.9082733786324406}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'f1_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.9001051524710831, 'roc_auc_train': 0.9338592461327208, 'acc_test': 0.9776709567374787, 'f1_test': 0.8693284936479129, 'roc_auc_test': 0.9124359372918239}}}], 'Occupancy': [{'LogReg': {'acc_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'f1_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.883668903803132, 'roc_auc_train': 0.9139048460745325, 'acc_test': 0.9794541789424717, 'f1_test': 0.8792710706150343, 'roc_auc_test': 0.91139312507293}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9751124205303148, 'f1_test': 0.8469241773962805, 'roc_auc_test': 0.8792408265597675}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.978833927740735, 'f1_test': 0.8756264236902049, 'roc_auc_test': 0.9095059274874541}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9789114591409521, 'f1_test': 0.8761384335154827, 'roc_auc_test': 0.9099351120368104}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.8791704238052299, 'roc_auc_test': 0.9151309775553271}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'f1_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'roc_auc_dict': {'acc_train': 0.9828, 'f1_train': 0.9061135371179039, 'roc_auc_train': 0.9347809468406637, 'acc_test': 0.97790355093813, 'f1_test': 0.8730512249443206, 'roc_auc_test': 0.9163393736678289}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'f1_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.8904267589388696, 'roc_auc_train': 0.9204947337533976, 'acc_test': 0.9785238021398667, 'f1_test': 0.875505617977528, 'roc_auc_test': 0.9087629957619124}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9757326717320515, 'f1_test': 0.8525671219971738, 'roc_auc_test': 0.8810083019183776}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9785238021398667, 'f1_test': 0.8775961113566062, 'roc_auc_test': 0.9159825007062277}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8769298632554036, 'roc_auc_test': 0.9162771140019244}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8762749445676276, 'roc_auc_test': 0.9139972703352985}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8656716417910448, 'roc_auc_test': 0.9120849536884206}, 'f1_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8657894736842107, 'roc_auc_test': 0.9124649276328584}, 'roc_auc_dict': {'acc_train': 0.9834, 'f1_train': 0.9049255441008017, 'roc_auc_train': 0.9306932604398319, 'acc_test': 0.9764304543340053, 'f1_test': 0.8644067796610171, 'roc_auc_test': 0.9057107572817216}}}, {'LogReg': {'acc_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'f1_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'roc_auc_dict': {'acc_train': 0.9782, 'f1_train': 0.8768361581920904, 'roc_auc_train': 0.9044976734333857, 'acc_test': 0.9776709567374787, 'f1_test': 0.8660465116279069, 'roc_auc_test': 0.898173984280681}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9737168553264072, 'f1_test': 0.8337420304070623, 'roc_auc_test': 0.8645984023737369}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9797643045433401, 'f1_test': 0.881739918441323, 'roc_auc_test': 0.915607214132604}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.8824593128390597, 'roc_auc_test': 0.9168128840222122}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.882988298829883, 'roc_auc_test': 0.9187513272488141}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772057683361761, 'f1_test': 0.8650137741046832, 'roc_auc_test': 0.902182935657324}, 'f1_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772832997363933, 'f1_test': 0.8656579550664832, 'roc_auc_test': 0.9030009169016118}, 'roc_auc_dict': {'acc_train': 0.98, 'f1_train': 0.8863636363636365, 'roc_auc_train': 0.9073679321761451, 'acc_test': 0.9758102031322685, 'f1_test': 0.8518518518518519, 'roc_auc_test': 0.8839700754522619}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'roc_auc_dict': {'acc_train': 0.978, 'f1_train': 0.8635235732009925, 'roc_auc_train': 0.8982935380926139, 'acc_test': 0.9778260195379128, 'f1_test': 0.872207327971403, 'roc_auc_test': 0.9025418974291459}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9745697007287951, 'f1_test': 0.8470149253731344, 'roc_auc_test': 0.8754379060706958}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9796092417429059, 'f1_test': 0.8848007008322382, 'roc_auc_test': 0.9161794143604022}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8832309043020192, 'roc_auc_test': 0.9145624239490734}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.883128295254833, 'roc_auc_test': 0.9141902468157529}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'f1_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'roc_auc_dict': {'acc_train': 0.9822, 'f1_train': 0.8944246737841044, 'roc_auc_train': 0.9308276360436083, 'acc_test': 0.9790665219413862, 'f1_test': 0.8835202761000863, 'roc_auc_test': 0.9210905698447808}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.8874458874458874, 'roc_auc_train': 0.9164518996735541, 'acc_test': 0.9789114591409521, 'f1_test': 0.872539831302718, 'roc_auc_test': 0.9025269055972525}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9750348891300977, 'f1_test': 0.8429268292682927, 'roc_auc_test': 0.8741199982235643}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9794541789424717, 'f1_test': 0.8774849745723532, 'roc_auc_test': 0.9098848277597668}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792991161420376, 'f1_test': 0.8765603328710125, 'roc_auc_test': 0.9094074847152522}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8766233766233766, 'roc_auc_test': 0.9082733786324406}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'f1_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.9001051524710831, 'roc_auc_train': 0.9338592461327208, 'acc_test': 0.9776709567374787, 'f1_test': 0.8693284936479129, 'roc_auc_test': 0.9124359372918239}}}], 'HTRU2': [{'LogReg': {'acc_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'f1_dict': {'acc_train': 0.9792, 'f1_train': 0.8839285714285714, 'roc_auc_train': 0.9148492255715222, 'acc_test': 0.9793766475422546, 'f1_test': 0.8788706739526412, 'roc_auc_test': 0.9113505102259172}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.883668903803132, 'roc_auc_train': 0.9139048460745325, 'acc_test': 0.9794541789424717, 'f1_test': 0.8792710706150343, 'roc_auc_test': 0.91139312507293}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9782136765389983, 'f1_test': 0.8700878409616275, 'roc_auc_test': 0.9014336146644821}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9751124205303148, 'f1_test': 0.8469241773962805, 'roc_auc_test': 0.8792408265597675}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.978833927740735, 'f1_test': 0.8756264236902049, 'roc_auc_test': 0.9095059274874541}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9789114591409521, 'f1_test': 0.8761384335154827, 'roc_auc_test': 0.9099351120368104}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.8791704238052299, 'roc_auc_test': 0.9151309775553271}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'f1_dict': {'acc_train': 0.9786, 'f1_train': 0.8812430632630411, 'roc_auc_train': 0.9154621865974555, 'acc_test': 0.9774383625368274, 'f1_test': 0.8675466545289031, 'roc_auc_test': 0.9056463026224775}, 'roc_auc_dict': {'acc_train': 0.9828, 'f1_train': 0.9061135371179039, 'roc_auc_train': 0.9347809468406637, 'acc_test': 0.97790355093813, 'f1_test': 0.8730512249443206, 'roc_auc_test': 0.9163393736678289}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'f1_dict': {'acc_train': 0.9814, 'f1_train': 0.8929804372842348, 'roc_auc_train': 0.9226877162095379, 'acc_test': 0.9787563963405179, 'f1_test': 0.877019748653501, 'roc_auc_test': 0.9100309585683367}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.8904267589388696, 'roc_auc_train': 0.9204947337533976, 'acc_test': 0.9785238021398667, 'f1_test': 0.875505617977528, 'roc_auc_test': 0.9087629957619124}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9766630485346566, 'f1_test': 0.8655649843680215, 'roc_auc_test': 0.9058387982548329}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9757326717320515, 'f1_test': 0.8525671219971738, 'roc_auc_test': 0.8810083019183776}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9785238021398667, 'f1_test': 0.8775961113566062, 'roc_auc_test': 0.9159825007062277}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8769298632554036, 'roc_auc_test': 0.9162771140019244}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8762749445676276, 'roc_auc_test': 0.9139972703352985}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8656716417910448, 'roc_auc_test': 0.9120849536884206}, 'f1_dict': {'acc_train': 0.9858, 'f1_train': 0.9199549041713642, 'roc_auc_train': 0.9448376111934766, 'acc_test': 0.9762753915335711, 'f1_test': 0.8657894736842107, 'roc_auc_test': 0.9124649276328584}, 'roc_auc_dict': {'acc_train': 0.9834, 'f1_train': 0.9049255441008017, 'roc_auc_train': 0.9306932604398319, 'acc_test': 0.9764304543340053, 'f1_test': 0.8644067796610171, 'roc_auc_test': 0.9057107572817216}}}, {'LogReg': {'acc_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'f1_dict': {'acc_train': 0.98, 'f1_train': 0.8883928571428571, 'roc_auc_train': 0.914869307629164, 'acc_test': 0.9787563963405179, 'f1_test': 0.8741965105601468, 'roc_auc_test': 0.906911901183466}, 'roc_auc_dict': {'acc_train': 0.9782, 'f1_train': 0.8768361581920904, 'roc_auc_train': 0.9044976734333857, 'acc_test': 0.9776709567374787, 'f1_test': 0.8660465116279069, 'roc_auc_test': 0.898173984280681}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9778260195379128, 'f1_test': 0.8654750705550328, 'roc_auc_test': 0.8939946170894504}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9737168553264072, 'f1_test': 0.8337420304070623, 'roc_auc_test': 0.8645984023737369}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9797643045433401, 'f1_test': 0.881739918441323, 'roc_auc_test': 0.915607214132604}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.8824593128390597, 'roc_auc_test': 0.9168128840222122}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9798418359435571, 'f1_test': 0.882988298829883, 'roc_auc_test': 0.9187513272488141}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772057683361761, 'f1_test': 0.8650137741046832, 'roc_auc_test': 0.902182935657324}, 'f1_dict': {'acc_train': 0.9834, 'f1_train': 0.9082872928176795, 'roc_auc_train': 0.9289383264016063, 'acc_test': 0.9772832997363933, 'f1_test': 0.8656579550664832, 'roc_auc_test': 0.9030009169016118}, 'roc_auc_dict': {'acc_train': 0.98, 'f1_train': 0.8863636363636365, 'roc_auc_train': 0.9073679321761451, 'acc_test': 0.9758102031322685, 'f1_test': 0.8518518518518519, 'roc_auc_test': 0.8839700754522619}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8736196319018406, 'roc_auc_train': 0.9074006229171334, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9111273084971574}, 'roc_auc_dict': {'acc_train': 0.978, 'f1_train': 0.8635235732009925, 'roc_auc_train': 0.8982935380926139, 'acc_test': 0.9778260195379128, 'f1_test': 0.872207327971403, 'roc_auc_test': 0.9025418974291459}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.97790355093813, 'f1_test': 0.8728246318607765, 'roc_auc_test': 0.9033290123218026}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9745697007287951, 'f1_test': 0.8470149253731344, 'roc_auc_test': 0.8754379060706958}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9796092417429059, 'f1_test': 0.8848007008322382, 'roc_auc_test': 0.9161794143604022}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8832309043020192, 'roc_auc_test': 0.9145624239490734}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.883128295254833, 'roc_auc_test': 0.9141902468157529}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'f1_dict': {'acc_train': 0.9784, 'f1_train': 0.8705035971223021, 'roc_auc_train': 0.9141510785993852, 'acc_test': 0.9768956427353078, 'f1_test': 0.8706597222222223, 'roc_auc_test': 0.9120775525166137}, 'roc_auc_dict': {'acc_train': 0.9822, 'f1_train': 0.8944246737841044, 'roc_auc_train': 0.9308276360436083, 'acc_test': 0.9790665219413862, 'f1_test': 0.8835202761000863, 'roc_auc_test': 0.9210905698447808}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'f1_dict': {'acc_train': 0.9794, 'f1_train': 0.8886486486486487, 'roc_auc_train': 0.9174743945610797, 'acc_test': 0.9791440533416034, 'f1_test': 0.8740046838407494, 'roc_auc_test': 0.9034390312504625}, 'roc_auc_dict': {'acc_train': 0.9792, 'f1_train': 0.8874458874458874, 'roc_auc_train': 0.9164518996735541, 'acc_test': 0.9789114591409521, 'f1_test': 0.872539831302718, 'roc_auc_test': 0.9025269055972525}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977981082338347, 'f1_test': 0.8671655753040224, 'roc_auc_test': 0.900447291675919}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9750348891300977, 'f1_test': 0.8429268292682927, 'roc_auc_test': 0.8741199982235643}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9794541789424717, 'f1_test': 0.8774849745723532, 'roc_auc_test': 0.9098848277597668}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792991161420376, 'f1_test': 0.8765603328710125, 'roc_auc_test': 0.9094074847152522}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8766233766233766, 'roc_auc_test': 0.9082733786324406}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'f1_dict': {'acc_train': 0.9832, 'f1_train': 0.9098712446351932, 'roc_auc_train': 0.9314318691097744, 'acc_test': 0.9783687393394325, 'f1_test': 0.8711316397228636, 'roc_auc_test': 0.9069356486210418}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.9001051524710831, 'roc_auc_train': 0.9338592461327208, 'acc_test': 0.9776709567374787, 'f1_test': 0.8693284936479129, 'roc_auc_test': 0.9124359372918239}}}]}\n"
     ]
    }
   ],
   "source": [
    "print(top_dict)"
   ]
  },
  {
   "source": [
    "## Analyze Problem Dataset (Table 1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            #ATTR  TRAIN SIZE  TEST SIZE %POS\n",
       "PROBLEM                                      \n",
       "ADULT      14/104        5000      30162  24%\n",
       "GRID           13        5000      10000  36%\n",
       "HTRU2           8        5000      17898   9%\n",
       "OCCUPANCY       5        5000      20560  23%"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#ATTR</th>\n      <th>TRAIN SIZE</th>\n      <th>TEST SIZE</th>\n      <th>%POS</th>\n    </tr>\n    <tr>\n      <th>PROBLEM</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ADULT</th>\n      <td>14/104</td>\n      <td>5000</td>\n      <td>30162</td>\n      <td>24%</td>\n    </tr>\n    <tr>\n      <th>GRID</th>\n      <td>13</td>\n      <td>5000</td>\n      <td>10000</td>\n      <td>36%</td>\n    </tr>\n    <tr>\n      <th>HTRU2</th>\n      <td>8</td>\n      <td>5000</td>\n      <td>17898</td>\n      <td>9%</td>\n    </tr>\n    <tr>\n      <th>OCCUPANCY</th>\n      <td>5</td>\n      <td>5000</td>\n      <td>20560</td>\n      <td>23%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# array of dataset strings\n",
    "STRINGarray = ['ADULT', 'GRID', 'HTRU2', 'OCCUPANCY']\n",
    "\n",
    "# get attributes size\n",
    "adultATTR = adultDF.shape[1]\n",
    "gridATTR = gridDF.shape[1]\n",
    "htru2ATTR = htru2DF.shape[1]\n",
    "occupancyATTR = occupancyDF.shape[1]\n",
    "# process adult attributes to add attributes from non-encode and encode\n",
    "adultATTR = '14/' + str(adultATTR - 1)\n",
    "# store into array\n",
    "ATTRarray = [adultATTR, gridATTR - 1, htru2ATTR - 1, occupancyATTR - 1]\n",
    "\n",
    "# get test size\n",
    "adultSIZE = adultDF.shape[0]\n",
    "gridSIZE = gridDF.shape[0]\n",
    "htru2SIZE = htru2DF.shape[0]\n",
    "occupancySIZE = occupancyDF.shape[0]\n",
    "# store into array\n",
    "SIZEarray = [adultSIZE, gridSIZE, htru2SIZE, occupancySIZE]\n",
    "\n",
    "# get number of pos from each dataset\n",
    "adultPOS = adultDF.loc[adultDF['income>50K'] == 1].shape[0]\n",
    "gridPOS = gridDF.loc[gridDF['stabf'] == 1].shape[0]\n",
    "htru2POS = htru2DF.loc[htru2DF['class'] == 1].shape[0]\n",
    "occupancyPOS = occupancyDF.loc[occupancyDF['Occupancy'] == 1].shape[0]\n",
    "# process POS to get percentage\n",
    "adultPOS = str(int((adultPOS/adultSIZE) * 100)) + '%'\n",
    "gridPOS = str(int((gridPOS/gridSIZE) * 100)) + '%'\n",
    "htru2POS = str(int((htru2POS/htru2SIZE) * 100)) + '%'\n",
    "occupancyPOS = str(int((occupancyPOS/occupancySIZE) * 100)) + '%'\n",
    "# store into array\n",
    "POSarray = [adultPOS, gridPOS, htru2POS, occupancyPOS]\n",
    "\n",
    "t1 = {'PROBLEM': STRINGarray, '#ATTR': ATTRarray, 'TRAIN SIZE': [5000,5000,5000,5000], \n",
    "        'TEST SIZE': SIZEarray, '%POS': POSarray}\n",
    "t1 = pd.DataFrame.from_dict(t1)\n",
    "t1.set_index('PROBLEM', inplace=True)\n",
    "t1"
   ]
  },
  {
   "source": [
    "## Break Down and Analyze Scores Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Break Down Adult's Testing Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adult dataset score array\n",
    "adultDATA = top_dict['Adult']\n",
    "\n",
    "# make array for ADULT dataset BEST PARAM ACCURACY TEST Scores (for all algorithms)\n",
    "adult_logreg_acc = []\n",
    "adult_knn_acc = []\n",
    "adult_rforest_acc = []\n",
    "adult_dtree_acc = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    adult_logreg_acc.append(adultDATA[trial]['LogReg']['acc_dict']['acc_test'])\n",
    "    adult_knn_acc.append(adultDATA[trial]['KNN']['acc_dict']['acc_test'])\n",
    "    adult_rforest_acc.append(adultDATA[trial]['Ran_For']['acc_dict']['acc_test'])\n",
    "    adult_dtree_acc.append(adultDATA[trial]['Dec_Tree']['acc_dict']['acc_test'])\n",
    "\n",
    "\n",
    "# make array for ADULT dataset BEST PARAM F1 TEST Scores (for all algorithms)\n",
    "adult_logreg_f1 = []\n",
    "adult_knn_f1 = []\n",
    "adult_rforest_f1 = []\n",
    "adult_dtree_f1 = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    adult_logreg_f1.append(adultDATA[trial]['LogReg']['f1_dict']['f1_test'])\n",
    "    adult_knn_f1.append(adultDATA[trial]['KNN']['f1_dict']['f1_test'])\n",
    "    adult_rforest_f1.append(adultDATA[trial]['Ran_For']['f1_dict']['f1_test'])\n",
    "    adult_dtree_f1.append(adultDATA[trial]['Dec_Tree']['f1_dict']['f1_test'])\n",
    "\n",
    "\n",
    "# make array for ADULT dataset BEST PARAM ROC AUC TEST Scores (for all algorithms)\n",
    "adult_logreg_roc_auc = []\n",
    "adult_knn_roc_auc = []\n",
    "adult_rforest_roc_auc = []\n",
    "adult_dtree_roc_auc = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    adult_logreg_roc_auc.append(adultDATA[trial]['LogReg']['roc_auc_dict']['roc_auc_test'])\n",
    "    adult_knn_roc_auc.append(adultDATA[trial]['KNN']['roc_auc_dict']['roc_auc_test'])\n",
    "    adult_rforest_roc_auc.append(adultDATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_test'])\n",
    "    adult_dtree_roc_auc.append(adultDATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_test'])"
   ]
  },
  {
   "source": [
    "### Break Down Grid Testing Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GRID dataset score array\n",
    "gridDATA = top_dict['Grid']\n",
    "\n",
    "# make array for ADULT dataset BEST PARAM ACCURACY TEST Scores (for all algorithms)\n",
    "grid_logreg_acc = []\n",
    "grid_knn_acc = []\n",
    "grid_rforest_acc = []\n",
    "grid_dtree_acc = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    grid_logreg_acc.append(gridDATA[trial]['LogReg']['acc_dict']['acc_test'])\n",
    "    grid_knn_acc.append(gridDATA[trial]['KNN']['acc_dict']['acc_test'])\n",
    "    grid_rforest_acc.append(gridDATA[trial]['Ran_For']['acc_dict']['acc_test'])\n",
    "    grid_dtree_acc.append(gridDATA[trial]['Dec_Tree']['acc_dict']['acc_test'])\n",
    "\n",
    "\n",
    "# make array for GRID dataset BEST PARAM F1 TEST Scores (for all algorithms)\n",
    "grid_logreg_f1 = []\n",
    "grid_knn_f1 = []\n",
    "grid_rforest_f1 = []\n",
    "grid_dtree_f1 = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    grid_logreg_f1.append(gridDATA[trial]['LogReg']['f1_dict']['f1_test'])\n",
    "    grid_knn_f1.append(gridDATA[trial]['KNN']['f1_dict']['f1_test'])\n",
    "    grid_rforest_f1.append(gridDATA[trial]['Ran_For']['f1_dict']['f1_test'])\n",
    "    grid_dtree_f1.append(gridDATA[trial]['Dec_Tree']['f1_dict']['f1_test'])\n",
    "\n",
    "\n",
    "# make array for GRID dataset BEST PARAM ROC AUC TEST Scores (for all algorithms)\n",
    "grid_logreg_roc_auc = []\n",
    "grid_knn_roc_auc = []\n",
    "grid_rforest_roc_auc = []\n",
    "grid_dtree_roc_auc = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    grid_logreg_roc_auc.append(gridDATA[trial]['LogReg']['roc_auc_dict']['roc_auc_test'])\n",
    "    grid_knn_roc_auc.append(gridDATA[trial]['KNN']['roc_auc_dict']['roc_auc_test'])\n",
    "    grid_rforest_roc_auc.append(gridDATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_test'])\n",
    "    grid_dtree_roc_auc.append(gridDATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_test'])"
   ]
  },
  {
   "source": [
    "### Break Down HTRU2 Testing Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HTRU2 dataset score array\n",
    "htru2DATA = top_dict['HTRU2']\n",
    "\n",
    "# make array for HTRU2 dataset BEST PARAM ACCURACY TEST Scores (for all algorithms)\n",
    "htru2_logreg_acc = []\n",
    "htru2_knn_acc = []\n",
    "htru2_rforest_acc = []\n",
    "htru2_dtree_acc = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    htru2_logreg_acc.append(htru2DATA[trial]['LogReg']['acc_dict']['acc_test'])\n",
    "    htru2_knn_acc.append(htru2DATA[trial]['KNN']['acc_dict']['acc_test'])\n",
    "    htru2_rforest_acc.append(htru2DATA[trial]['Ran_For']['acc_dict']['acc_test'])\n",
    "    htru2_dtree_acc.append(htru2DATA[trial]['Dec_Tree']['acc_dict']['acc_test'])\n",
    "\n",
    "\n",
    "# make array for HTRU2 dataset BEST PARAM F1 TEST Scores (for all algorithms)\n",
    "htru2_logreg_f1 = []\n",
    "htru2_knn_f1 = []\n",
    "htru2_rforest_f1 = []\n",
    "htru2_dtree_f1 = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    htru2_logreg_f1.append(htru2DATA[trial]['LogReg']['f1_dict']['f1_test'])\n",
    "    htru2_knn_f1.append(htru2DATA[trial]['KNN']['f1_dict']['f1_test'])\n",
    "    htru2_rforest_f1.append(htru2DATA[trial]['Ran_For']['f1_dict']['f1_test'])\n",
    "    htru2_dtree_f1.append(htru2DATA[trial]['Dec_Tree']['f1_dict']['f1_test'])\n",
    "\n",
    "\n",
    "# make array for HTRU2 dataset BEST PARAM ROC AUC TEST Scores (for all algorithms)\n",
    "htru2_logreg_roc_auc = []\n",
    "htru2_knn_roc_auc = []\n",
    "htru2_rforest_roc_auc = []\n",
    "htru2_dtree_roc_auc = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    htru2_logreg_roc_auc.append(htru2DATA[trial]['LogReg']['roc_auc_dict']['roc_auc_test'])\n",
    "    htru2_knn_roc_auc.append(htru2DATA[trial]['KNN']['roc_auc_dict']['roc_auc_test'])\n",
    "    htru2_rforest_roc_auc.append(htru2DATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_test'])\n",
    "    htru2_dtree_roc_auc.append(htru2DATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_test'])"
   ]
  },
  {
   "source": [
    "### Break Down Occupancy Testing Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OCCUPANCY dataset score array\n",
    "occupancyDATA = top_dict['Occupancy']\n",
    "\n",
    "# make array for OCCUPANCY dataset BEST PARAM ACCURACY TEST Scores (for all algorithms)\n",
    "occupancy_logreg_acc = []\n",
    "occupancy_knn_acc = []\n",
    "occupancy_rforest_acc = []\n",
    "occupancy_dtree_acc = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    occupancy_logreg_acc.append(occupancyDATA[trial]['LogReg']['acc_dict']['acc_test'])\n",
    "    occupancy_knn_acc.append(occupancyDATA[trial]['KNN']['acc_dict']['acc_test'])\n",
    "    occupancy_rforest_acc.append(occupancyDATA[trial]['Ran_For']['acc_dict']['acc_test'])\n",
    "    occupancy_dtree_acc.append(occupancyDATA[trial]['Dec_Tree']['acc_dict']['acc_test'])\n",
    "\n",
    "\n",
    "# make array for OCCUPANCY dataset BEST PARAM F1 TEST Scores (for all algorithms)\n",
    "occupancy_logreg_f1 = []\n",
    "occupancy_knn_f1 = []\n",
    "occupancy_rforest_f1 = []\n",
    "occupancy_dtree_f1 = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    occupancy_logreg_f1.append(occupancyDATA[trial]['LogReg']['f1_dict']['f1_test'])\n",
    "    occupancy_knn_f1.append(occupancyDATA[trial]['KNN']['f1_dict']['f1_test'])\n",
    "    occupancy_rforest_f1.append(occupancyDATA[trial]['Ran_For']['f1_dict']['f1_test'])\n",
    "    occupancy_dtree_f1.append(occupancyDATA[trial]['Dec_Tree']['f1_dict']['f1_test'])\n",
    "\n",
    "\n",
    "# make array for OCCUPANCY dataset BEST PARAM ROC AUC TEST Scores (for all algorithms)\n",
    "occupancy_logreg_roc_auc = []\n",
    "occupancy_knn_roc_auc = []\n",
    "occupancy_rforest_roc_auc = []\n",
    "occupancy_dtree_roc_auc = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    occupancy_logreg_roc_auc.append(occupancyDATA[trial]['LogReg']['roc_auc_dict']['roc_auc_test'])\n",
    "    occupancy_knn_roc_auc.append(occupancyDATA[trial]['KNN']['roc_auc_dict']['roc_auc_test'])\n",
    "    occupancy_rforest_roc_auc.append(occupancyDATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_test'])\n",
    "    occupancy_dtree_roc_auc.append(occupancyDATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_test'])"
   ]
  },
  {
   "source": [
    "### Get Accuracy Averages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9790510156613429\n0.9777174755776091\n0.979237091021864\n0.9772367808962631\n"
     ]
    }
   ],
   "source": [
    "# get average of logistic regression accuracy average\n",
    "# get average of trials for adult logreg accuracy\n",
    "adult_logreg_acc_avg = sum(adult_logreg_acc)/len(adult_logreg_acc)\n",
    "# get average of trials for grid logreg accuracy\n",
    "grid_logreg_acc_avg = sum(grid_logreg_acc)/len(grid_logreg_acc)\n",
    "# get average of trials for htru2 logreg accuracy\n",
    "htru2_logreg_acc_avg = sum(htru2_logreg_acc)/len(htru2_logreg_acc)\n",
    "# get average of trials for occupancy logreg accuracy\n",
    "occupancy_logreg_acc_avg = sum(occupancy_logreg_acc)/len(occupancy_logreg_acc)\n",
    "# get average of all logreg accuracy average\n",
    "logreg_accuracy_avg = (adult_logreg_acc_avg + grid_logreg_acc_avg + htru2_logreg_acc_avg + \n",
    "                        occupancy_logreg_acc_avg) / 4\n",
    "print(logreg_accuracy_avg)\n",
    "\n",
    "\n",
    "# get average of knn accuracy average\n",
    "# get average of trials for adult knn accuracy\n",
    "adult_knn_acc_avg = sum(adult_knn_acc)/len(adult_knn_acc)\n",
    "# get average of trials for grid knn accuracy\n",
    "grid_knn_acc_avg = sum(grid_knn_acc)/len(grid_knn_acc)\n",
    "# get average of trials for htru2 knn accuracy\n",
    "htru2_knn_acc_avg = sum(htru2_knn_acc)/len(htru2_knn_acc)\n",
    "# get average of trials for occupancy knn accuracy\n",
    "occupancy_knn_acc_avg = sum(occupancy_knn_acc)/len(occupancy_knn_acc)\n",
    "# get average of all knn accuracy average\n",
    "knn_accuracy_avg = (adult_knn_acc_avg + grid_knn_acc_avg + htru2_knn_acc_avg + \n",
    "                        occupancy_knn_acc_avg) / 4\n",
    "print(knn_accuracy_avg)\n",
    "\n",
    "\n",
    "# get average of random forest accuracy average\n",
    "# get average of trials for adult logreg accuracy\n",
    "adult_rforest_acc_avg = sum(adult_rforest_acc)/len(adult_rforest_acc)\n",
    "# get average of trials for grid logreg accuracy\n",
    "grid_rforest_acc_avg = sum(grid_rforest_acc)/len(grid_rforest_acc)\n",
    "# get average of trials for htru2 logreg accuracy\n",
    "htru2_rforest_acc_avg = sum(htru2_rforest_acc)/len(htru2_rforest_acc)\n",
    "# get average of trials for occupancy logreg accuracy\n",
    "occupancy_rforest_acc_avg = sum(occupancy_rforest_acc)/len(occupancy_rforest_acc)\n",
    "# get average of all logreg accuracy average\n",
    "rforest_accuracy_avg = (adult_rforest_acc_avg + grid_rforest_acc_avg + htru2_rforest_acc_avg + \n",
    "                        occupancy_rforest_acc_avg) / 4\n",
    "print(rforest_accuracy_avg)\n",
    "\n",
    "\n",
    "# get average of decision tree accuracy average\n",
    "# get average of trials for adult logreg accuracy\n",
    "adult_dtree_acc_avg = sum(adult_dtree_acc)/len(adult_dtree_acc)\n",
    "# get average of trials for grid logreg accuracy\n",
    "grid_dtree_acc_avg = sum(grid_dtree_acc)/len(grid_dtree_acc)\n",
    "# get average of trials for htru2 logreg accuracy\n",
    "htru2_dtree_acc_avg = sum(htru2_dtree_acc)/len(htru2_dtree_acc)\n",
    "# get average of trials for occupancy logreg accuracy\n",
    "occupancy_dtree_acc_avg = sum(occupancy_dtree_acc)/len(occupancy_dtree_acc)\n",
    "# get average of all logreg accuracy average\n",
    "dtree_accuracy_avg = (adult_dtree_acc_avg + grid_dtree_acc_avg + htru2_dtree_acc_avg + \n",
    "                        occupancy_dtree_acc_avg) / 4\n",
    "print(dtree_accuracy_avg)\n"
   ]
  },
  {
   "source": [
    "### Get F1 Score Averages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.877122479015908\n0.8682236206098961\n0.8790637693565955\n0.8681570890449366\n"
     ]
    }
   ],
   "source": [
    "# get average of logistic regression f1 average\n",
    "# get average of trials for adult logreg f1\n",
    "adult_logreg_f1_avg = sum(adult_logreg_f1)/len(adult_logreg_f1)\n",
    "# get average of trials for grid logreg f1\n",
    "grid_logreg_f1_avg = sum(grid_logreg_f1)/len(grid_logreg_f1)\n",
    "# get average of trials for htru2 logreg f1\n",
    "htru2_logreg_f1_avg = sum(htru2_logreg_f1)/len(htru2_logreg_f1)\n",
    "# get average of trials for occupancy logreg f1\n",
    "occupancy_logreg_f1_avg = sum(occupancy_logreg_f1)/len(occupancy_logreg_f1)\n",
    "# get average of all logreg f1 average\n",
    "logreg_f1_avg = (adult_logreg_f1_avg + grid_logreg_f1_avg + htru2_logreg_f1_avg + \n",
    "                        occupancy_logreg_f1_avg) / 4\n",
    "print(logreg_f1_avg)\n",
    "\n",
    "\n",
    "# get average of knn f1 average\n",
    "# get average of trials for adult knn f1\n",
    "adult_knn_f1_avg = sum(adult_knn_f1)/len(adult_knn_f1)\n",
    "# get average of trials for grid knn f1\n",
    "grid_knn_f1_avg = sum(grid_knn_f1)/len(grid_knn_f1)\n",
    "# get average of trials for htru2 knn f1\n",
    "htru2_knn_f1_avg = sum(htru2_knn_f1)/len(htru2_knn_f1)\n",
    "# get average of trials for occupancy knn f1\n",
    "occupancy_knn_f1_avg = sum(occupancy_knn_f1)/len(occupancy_knn_f1)\n",
    "# get average of all knn f1 average\n",
    "knn_f1_avg = (adult_knn_f1_avg + grid_knn_f1_avg + htru2_knn_f1_avg + \n",
    "                        occupancy_knn_f1_avg) / 4\n",
    "print(knn_f1_avg)\n",
    "\n",
    "\n",
    "# get average of random forest f1 average\n",
    "# get average of trials for adult random forest f1\n",
    "adult_rforest_f1_avg = sum(adult_rforest_f1)/len(adult_rforest_f1)\n",
    "# get average of trials for grid random forest f1\n",
    "grid_rforest_f1_avg = sum(grid_rforest_f1)/len(grid_rforest_f1)\n",
    "# get average of trials for htru2 random forest f1\n",
    "htru2_rforest_f1_avg = sum(htru2_rforest_f1)/len(htru2_rforest_f1)\n",
    "# get average of trials for occupancy random forest f1\n",
    "occupancy_rforest_f1_avg = sum(occupancy_rforest_f1)/len(occupancy_rforest_f1)\n",
    "# get average of all random forest f1 average\n",
    "rforest_f1_avg = (adult_rforest_f1_avg + grid_rforest_f1_avg + htru2_rforest_f1_avg + \n",
    "                        occupancy_rforest_f1_avg) / 4\n",
    "print(rforest_f1_avg)\n",
    "\n",
    "\n",
    "# get average of decision tree f1 average\n",
    "# get average of trials for adult decision tree f1\n",
    "adult_dtree_f1_avg = sum(adult_dtree_f1)/len(adult_dtree_f1)\n",
    "# get average of trials for grid decision tree f1\n",
    "grid_dtree_f1_avg = sum(grid_dtree_f1)/len(grid_dtree_f1)\n",
    "# get average of trials for htru2 decision tree f1\n",
    "htru2_dtree_f1_avg = sum(htru2_dtree_f1)/len(htru2_dtree_f1)\n",
    "# get average of trials for occupancy decision tree f1\n",
    "occupancy_dtree_f1_avg = sum(occupancy_dtree_f1)/len(occupancy_dtree_f1)\n",
    "# get average of all decision tree f1 average\n",
    "dtree_f1_avg = (adult_dtree_f1_avg + grid_dtree_f1_avg + htru2_dtree_f1_avg + \n",
    "                        occupancy_dtree_f1_avg) / 4\n",
    "print(dtree_f1_avg)"
   ]
  },
  {
   "source": [
    "### Get ROC AUC Averages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9046797816283844\n0.8748810870292285\n0.9140686401175266\n0.9079093427076834\n"
     ]
    }
   ],
   "source": [
    "# get average of logistic regression roc_auc average\n",
    "# get average of trials for adult logreg roc_auc\n",
    "adult_logreg_roc_auc_avg = sum(adult_logreg_roc_auc)/len(adult_logreg_roc_auc)\n",
    "# get average of trials for grid logreg roc_auc\n",
    "grid_logreg_roc_auc_avg = sum(grid_logreg_roc_auc)/len(grid_logreg_roc_auc)\n",
    "# get average of trials for htru2 logreg roc_auc\n",
    "htru2_logreg_roc_auc_avg = sum(htru2_logreg_roc_auc)/len(htru2_logreg_roc_auc)\n",
    "# get average of trials for occupancy logreg roc_auc\n",
    "occupancy_logreg_roc_auc_avg = sum(occupancy_logreg_roc_auc)/len(occupancy_logreg_roc_auc)\n",
    "# get average of all logreg roc_auc average\n",
    "logreg_roc_auc_avg = (adult_logreg_roc_auc_avg + grid_logreg_roc_auc_avg + htru2_logreg_roc_auc_avg + \n",
    "                        occupancy_logreg_roc_auc_avg) / 4\n",
    "print(logreg_roc_auc_avg)\n",
    " \n",
    " \n",
    "# get average of knn roc_auc average\n",
    "# get average of trials for adult knn roc_auc\n",
    "adult_knn_roc_auc_avg = sum(adult_knn_roc_auc)/len(adult_knn_roc_auc)\n",
    "# get average of trials for grid knn roc_auc\n",
    "grid_knn_roc_auc_avg = sum(grid_knn_roc_auc)/len(grid_knn_roc_auc)\n",
    "# get average of trials for htru2 knn roc_auc\n",
    "htru2_knn_roc_auc_avg = sum(htru2_knn_roc_auc)/len(htru2_knn_roc_auc)\n",
    "# get average of trials for occupancy knn roc_auc\n",
    "occupancy_knn_roc_auc_avg = sum(occupancy_knn_roc_auc)/len(occupancy_knn_roc_auc)\n",
    "# get average of all knn roc_auc average\n",
    "knn_roc_auc_avg = (adult_knn_roc_auc_avg + grid_knn_roc_auc_avg + htru2_knn_roc_auc_avg + \n",
    "                        occupancy_knn_roc_auc_avg) / 4\n",
    "print(knn_roc_auc_avg)\n",
    " \n",
    " \n",
    "# get average of random forest roc_auc average\n",
    "# get average of trials for adult random forest roc_auc\n",
    "adult_rforest_roc_auc_avg = sum(adult_rforest_roc_auc)/len(adult_rforest_roc_auc)\n",
    "# get average of trials for grid random forest roc_auc\n",
    "grid_rforest_roc_auc_avg = sum(grid_rforest_roc_auc)/len(grid_rforest_roc_auc)\n",
    "# get average of trials for htru2 random forest roc_auc\n",
    "htru2_rforest_roc_auc_avg = sum(htru2_rforest_roc_auc)/len(htru2_rforest_roc_auc)\n",
    "# get average of trials for occupancy random forest roc_auc\n",
    "occupancy_rforest_roc_auc_avg = sum(occupancy_rforest_roc_auc)/len(occupancy_rforest_roc_auc)\n",
    "# get average of all random forest roc_auc average\n",
    "rforest_roc_auc_avg = (adult_rforest_roc_auc_avg + grid_rforest_roc_auc_avg + htru2_rforest_roc_auc_avg + \n",
    "                        occupancy_rforest_roc_auc_avg) / 4\n",
    "print(rforest_roc_auc_avg)\n",
    " \n",
    " \n",
    "# get average of decision tree roc_auc average\n",
    "# get average of trials for adult decision tree roc_auc\n",
    "adult_dtree_roc_auc_avg = sum(adult_dtree_roc_auc)/len(adult_dtree_roc_auc)\n",
    "# get average of trials for grid decision tree roc_auc\n",
    "grid_dtree_roc_auc_avg = sum(grid_dtree_roc_auc)/len(grid_dtree_roc_auc)\n",
    "# get average of trials for htru2 decision tree roc_auc\n",
    "htru2_dtree_roc_auc_avg = sum(htru2_dtree_roc_auc)/len(htru2_dtree_roc_auc)\n",
    "# get average of trials for occupancy decision tree roc_auc\n",
    "occupancy_dtree_roc_auc_avg = sum(occupancy_dtree_roc_auc)/len(occupancy_dtree_roc_auc)\n",
    "# get average of all decision tree roc_auc average\n",
    "dtree_roc_auc_avg = (adult_dtree_roc_auc_avg + grid_dtree_roc_auc_avg + htru2_dtree_roc_auc_avg + \n",
    "                        occupancy_dtree_roc_auc_avg) / 4\n",
    "print(dtree_roc_auc_avg)"
   ]
  },
  {
   "source": [
    "## Analyze Score Data by SCORES and MODEL (Table 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    MODEL       ACC        F1   ROC AUC      MEAN\n",
       "2      RF  0.979237  0.879064  0.914069  0.924123\n",
       "0  LogReg  0.979051  0.877122  0.904680  0.920284\n",
       "3      DT  0.977237  0.868157  0.907909  0.917768\n",
       "1     KNN  0.977717  0.868224  0.874881  0.906941"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MODEL</th>\n      <th>ACC</th>\n      <th>F1</th>\n      <th>ROC AUC</th>\n      <th>MEAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>RF</td>\n      <td>0.979237</td>\n      <td>0.879064</td>\n      <td>0.914069</td>\n      <td>0.924123</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LogReg</td>\n      <td>0.979051</td>\n      <td>0.877122</td>\n      <td>0.904680</td>\n      <td>0.920284</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DT</td>\n      <td>0.977237</td>\n      <td>0.868157</td>\n      <td>0.907909</td>\n      <td>0.917768</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KNN</td>\n      <td>0.977717</td>\n      <td>0.868224</td>\n      <td>0.874881</td>\n      <td>0.906941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# row labels\n",
    "models = ['LogReg', 'KNN', 'RF', 'DT']\n",
    "# accuracy column\n",
    "t2_acc = [logreg_accuracy_avg, knn_accuracy_avg, rforest_accuracy_avg, dtree_accuracy_avg]\n",
    "# f1 column\n",
    "t2_f1 = [logreg_f1_avg, knn_f1_avg, rforest_f1_avg, dtree_f1_avg]\n",
    "# roc auc column\n",
    "t2_roc_auc = [logreg_roc_auc_avg, knn_roc_auc_avg, rforest_roc_auc_avg, dtree_roc_auc_avg]\n",
    "# get average of rows\n",
    "t2_logreg_mean = (logreg_accuracy_avg + logreg_f1_avg + logreg_roc_auc_avg)/3\n",
    "t2_knn_mean = (knn_accuracy_avg + knn_f1_avg + knn_roc_auc_avg)/3\n",
    "t2_rforest_mean = (rforest_accuracy_avg + rforest_f1_avg + rforest_roc_auc_avg)/3\n",
    "t2_dtree_mean = (dtree_accuracy_avg + dtree_f1_avg + dtree_roc_auc_avg)/3\n",
    "# mean column\n",
    "t2_mean = [t2_logreg_mean, t2_knn_mean, t2_rforest_mean, t2_dtree_mean]\n",
    "# make dictionary and dataframe\n",
    "t2 = {'MODEL': models, 'ACC': t2_acc, 'F1': t2_f1, 'ROC AUC': t2_roc_auc, 'MEAN': t2_mean}\n",
    "t2 = pd.DataFrame.from_dict(t2)\n",
    "t2.sort_values(by='MEAN', ascending=False, inplace=True)\n",
    "t2\n"
   ]
  },
  {
   "source": [
    "## Analyze Score Data by DATASET and MODEL (Table 3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    MODEL     ADULT      GRID     HTRU2  OCCUPANCY      MEAN\n",
       "2      RF  0.924123  0.924123  0.924123   0.924123  0.924123\n",
       "0  LogReg  0.920284  0.920284  0.920284   0.920284  0.920284\n",
       "3      DT  0.917768  0.917768  0.917768   0.917768  0.917768\n",
       "1     KNN  0.906941  0.906941  0.906941   0.906941  0.906941"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MODEL</th>\n      <th>ADULT</th>\n      <th>GRID</th>\n      <th>HTRU2</th>\n      <th>OCCUPANCY</th>\n      <th>MEAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>RF</td>\n      <td>0.924123</td>\n      <td>0.924123</td>\n      <td>0.924123</td>\n      <td>0.924123</td>\n      <td>0.924123</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LogReg</td>\n      <td>0.920284</td>\n      <td>0.920284</td>\n      <td>0.920284</td>\n      <td>0.920284</td>\n      <td>0.920284</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DT</td>\n      <td>0.917768</td>\n      <td>0.917768</td>\n      <td>0.917768</td>\n      <td>0.917768</td>\n      <td>0.917768</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KNN</td>\n      <td>0.906941</td>\n      <td>0.906941</td>\n      <td>0.906941</td>\n      <td>0.906941</td>\n      <td>0.906941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# adult column\n",
    "adult_logreg_avg = (adult_logreg_acc_avg + adult_logreg_f1_avg + adult_logreg_roc_auc_avg) / 3\n",
    "adult_knn_avg = (adult_knn_acc_avg + adult_knn_f1_avg + adult_knn_roc_auc_avg) / 3\n",
    "adult_rforest_avg = (adult_rforest_acc_avg + adult_rforest_f1_avg + adult_rforest_roc_auc_avg) / 3\n",
    "adult_dtree_avg = (adult_dtree_acc_avg + adult_dtree_f1_avg + adult_dtree_roc_auc_avg) / 3\n",
    "t3_adult = [adult_logreg_avg, adult_knn_avg, adult_rforest_avg, adult_dtree_avg]\n",
    "\n",
    "# grid column\n",
    "grid_logreg_avg = (grid_logreg_acc_avg + grid_logreg_f1_avg + grid_logreg_roc_auc_avg) / 3\n",
    "grid_knn_avg = (grid_knn_acc_avg + grid_knn_f1_avg + grid_knn_roc_auc_avg) / 3\n",
    "grid_rforest_avg = (grid_rforest_acc_avg + grid_rforest_f1_avg + grid_rforest_roc_auc_avg) / 3\n",
    "grid_dtree_avg = (grid_dtree_acc_avg + grid_dtree_f1_avg + grid_dtree_roc_auc_avg) / 3\n",
    "t3_grid = [grid_logreg_avg, grid_knn_avg, grid_rforest_avg, grid_dtree_avg]\n",
    "\n",
    "# htru2 column\n",
    "htru2_logreg_avg = (htru2_logreg_acc_avg + htru2_logreg_f1_avg + htru2_logreg_roc_auc_avg) / 3\n",
    "htru2_knn_avg = (htru2_knn_acc_avg + htru2_knn_f1_avg + htru2_knn_roc_auc_avg) / 3\n",
    "htru2_rforest_avg = (htru2_rforest_acc_avg + htru2_rforest_f1_avg + htru2_rforest_roc_auc_avg) / 3\n",
    "htru2_dtree_avg = (htru2_dtree_acc_avg + htru2_dtree_f1_avg + htru2_dtree_roc_auc_avg) / 3\n",
    "t3_htru2 = [htru2_logreg_avg, htru2_knn_avg, htru2_rforest_avg, htru2_dtree_avg]\n",
    "\n",
    "# occupancy column\n",
    "occupancy_logreg_avg = (occupancy_logreg_acc_avg + occupancy_logreg_f1_avg + occupancy_logreg_roc_auc_avg) / 3\n",
    "occupancy_knn_avg = (occupancy_knn_acc_avg + occupancy_knn_f1_avg + occupancy_knn_roc_auc_avg) / 3\n",
    "occupancy_rforest_avg = (occupancy_rforest_acc_avg + occupancy_rforest_f1_avg + occupancy_rforest_roc_auc_avg) / 3\n",
    "occupancy_dtree_avg = (occupancy_dtree_acc_avg + occupancy_dtree_f1_avg + occupancy_dtree_roc_auc_avg) / 3\n",
    "t3_occupancy = [occupancy_logreg_avg, occupancy_knn_avg, occupancy_rforest_avg, occupancy_dtree_avg]\n",
    "\n",
    "# mean column\n",
    "# get average of rows\n",
    "t3_logreg_mean = (adult_logreg_avg + grid_logreg_avg + htru2_logreg_avg + occupancy_logreg_avg)/4\n",
    "t3_knn_mean = (adult_knn_avg + grid_knn_avg + htru2_knn_avg + occupancy_knn_avg)/4\n",
    "t3_rforest_mean = (adult_rforest_avg + grid_rforest_avg + htru2_rforest_avg + occupancy_rforest_avg)/4\n",
    "t3_dtree_mean = (adult_dtree_avg + grid_dtree_avg + htru2_dtree_avg + occupancy_dtree_avg)/4\n",
    "# mean column\n",
    "t3_mean = [t3_logreg_mean, t3_knn_mean, t3_rforest_mean, t3_dtree_mean]\n",
    "\n",
    "# make dictionary and dataframe\n",
    "t3 = {'MODEL': models, 'ADULT': t3_adult, 'GRID': t3_grid, 'HTRU2': t3_htru2, 'OCCUPANCY': t3_occupancy, 'MEAN': t3_mean}\n",
    "t3 = pd.DataFrame.from_dict(t3)\n",
    "t3.sort_values(by='MEAN', ascending=False, inplace=True)\n",
    "t3"
   ]
  },
  {
   "source": [
    "## Secondary Results Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Break Down Adult Training Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adult dataset score array\n",
    "adultDATA = top_dict['Adult']\n",
    " \n",
    "# make array for ADULT dataset BEST PARAM ACCURACY train Scores (for all algorithms)\n",
    "adult_logreg_acc_train = []\n",
    "adult_knn_acc_train = []\n",
    "adult_rforest_acc_train = []\n",
    "adult_dtree_acc_train = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    adult_logreg_acc_train.append(adultDATA[trial]['LogReg']['acc_dict']['acc_train'])\n",
    "    adult_knn_acc_train.append(adultDATA[trial]['KNN']['acc_dict']['acc_train'])\n",
    "    adult_rforest_acc_train.append(adultDATA[trial]['Ran_For']['acc_dict']['acc_train'])\n",
    "    adult_dtree_acc_train.append(adultDATA[trial]['Dec_Tree']['acc_dict']['acc_train'])\n",
    " \n",
    " \n",
    "# make array for ADULT dataset BEST PARAM F1 train Scores (for all algorithms)\n",
    "adult_logreg_f1_train = []\n",
    "adult_knn_f1_train = []\n",
    "adult_rforest_f1_train = []\n",
    "adult_dtree_f1_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    adult_logreg_f1_train.append(adultDATA[trial]['LogReg']['f1_dict']['f1_train'])\n",
    "    adult_knn_f1_train.append(adultDATA[trial]['KNN']['f1_dict']['f1_train'])\n",
    "    adult_rforest_f1_train.append(adultDATA[trial]['Ran_For']['f1_dict']['f1_train'])\n",
    "    adult_dtree_f1_train.append(adultDATA[trial]['Dec_Tree']['f1_dict']['f1_train'])\n",
    " \n",
    " \n",
    "# make array for ADULT dataset BEST PARAM ROC AUC train Scores (for all algorithms)\n",
    "adult_logreg_roc_auc_train = []\n",
    "adult_knn_roc_auc_train = []\n",
    "adult_rforest_roc_auc_train = []\n",
    "adult_dtree_roc_auc_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    adult_logreg_roc_auc_train.append(adultDATA[trial]['LogReg']['roc_auc_dict']['roc_auc_train'])\n",
    "    adult_knn_roc_auc_train.append(adultDATA[trial]['KNN']['roc_auc_dict']['roc_auc_train'])\n",
    "    adult_rforest_roc_auc_train.append(adultDATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_train'])\n",
    "    adult_dtree_roc_auc_train.append(adultDATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_train'])"
   ]
  },
  {
   "source": [
    "### Break Down Grid Training Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get grid dataset score array\n",
    "gridDATA = top_dict['Grid']\n",
    " \n",
    "# make array for grid dataset BEST PARAM ACCURACY train Scores (for all algorithms)\n",
    "grid_logreg_acc_train = []\n",
    "grid_knn_acc_train = []\n",
    "grid_rforest_acc_train = []\n",
    "grid_dtree_acc_train = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    grid_logreg_acc_train.append(gridDATA[trial]['LogReg']['acc_dict']['acc_train'])\n",
    "    grid_knn_acc_train.append(gridDATA[trial]['KNN']['acc_dict']['acc_train'])\n",
    "    grid_rforest_acc_train.append(gridDATA[trial]['Ran_For']['acc_dict']['acc_train'])\n",
    "    grid_dtree_acc_train.append(gridDATA[trial]['Dec_Tree']['acc_dict']['acc_train'])\n",
    " \n",
    " \n",
    "# make array for grid dataset BEST PARAM F1 train Scores (for all algorithms)\n",
    "grid_logreg_f1_train = []\n",
    "grid_knn_f1_train = []\n",
    "grid_rforest_f1_train = []\n",
    "grid_dtree_f1_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    grid_logreg_f1_train.append(gridDATA[trial]['LogReg']['f1_dict']['f1_train'])\n",
    "    grid_knn_f1_train.append(gridDATA[trial]['KNN']['f1_dict']['f1_train'])\n",
    "    grid_rforest_f1_train.append(gridDATA[trial]['Ran_For']['f1_dict']['f1_train'])\n",
    "    grid_dtree_f1_train.append(gridDATA[trial]['Dec_Tree']['f1_dict']['f1_train'])\n",
    " \n",
    " \n",
    "# make array for grid dataset BEST PARAM ROC AUC train Scores (for all algorithms)\n",
    "grid_logreg_roc_auc_train = []\n",
    "grid_knn_roc_auc_train = []\n",
    "grid_rforest_roc_auc_train = []\n",
    "grid_dtree_roc_auc_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    grid_logreg_roc_auc_train.append(gridDATA[trial]['LogReg']['roc_auc_dict']['roc_auc_train'])\n",
    "    grid_knn_roc_auc_train.append(gridDATA[trial]['KNN']['roc_auc_dict']['roc_auc_train'])\n",
    "    grid_rforest_roc_auc_train.append(gridDATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_train'])\n",
    "    grid_dtree_roc_auc_train.append(gridDATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_train'])"
   ]
  },
  {
   "source": [
    "### Break Down HTRU2 Training Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get htru2 dataset score array\n",
    "htru2DATA = top_dict['HTRU2']\n",
    " \n",
    "# make array for htru2 dataset BEST PARAM ACCURACY train Scores (for all algorithms)\n",
    "htru2_logreg_acc_train = []\n",
    "htru2_knn_acc_train = []\n",
    "htru2_rforest_acc_train = []\n",
    "htru2_dtree_acc_train = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    htru2_logreg_acc_train.append(htru2DATA[trial]['LogReg']['acc_dict']['acc_train'])\n",
    "    htru2_knn_acc_train.append(htru2DATA[trial]['KNN']['acc_dict']['acc_train'])\n",
    "    htru2_rforest_acc_train.append(htru2DATA[trial]['Ran_For']['acc_dict']['acc_train'])\n",
    "    htru2_dtree_acc_train.append(htru2DATA[trial]['Dec_Tree']['acc_dict']['acc_train'])\n",
    " \n",
    " \n",
    "# make array for htru2 dataset BEST PARAM F1 train Scores (for all algorithms)\n",
    "htru2_logreg_f1_train = []\n",
    "htru2_knn_f1_train = []\n",
    "htru2_rforest_f1_train = []\n",
    "htru2_dtree_f1_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    htru2_logreg_f1_train.append(htru2DATA[trial]['LogReg']['f1_dict']['f1_train'])\n",
    "    htru2_knn_f1_train.append(htru2DATA[trial]['KNN']['f1_dict']['f1_train'])\n",
    "    htru2_rforest_f1_train.append(htru2DATA[trial]['Ran_For']['f1_dict']['f1_train'])\n",
    "    htru2_dtree_f1_train.append(htru2DATA[trial]['Dec_Tree']['f1_dict']['f1_train'])\n",
    " \n",
    " \n",
    "# make array for htru2 dataset BEST PARAM ROC AUC train Scores (for all algorithms)\n",
    "htru2_logreg_roc_auc_train = []\n",
    "htru2_knn_roc_auc_train = []\n",
    "htru2_rforest_roc_auc_train = []\n",
    "htru2_dtree_roc_auc_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    htru2_logreg_roc_auc_train.append(htru2DATA[trial]['LogReg']['roc_auc_dict']['roc_auc_train'])\n",
    "    htru2_knn_roc_auc_train.append(htru2DATA[trial]['KNN']['roc_auc_dict']['roc_auc_train'])\n",
    "    htru2_rforest_roc_auc_train.append(htru2DATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_train'])\n",
    "    htru2_dtree_roc_auc_train.append(htru2DATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_train'])"
   ]
  },
  {
   "source": [
    "### Break Down Occupancy Training Data Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get occupancy dataset score array\n",
    "occupancyDATA = top_dict['Occupancy']\n",
    " \n",
    "# make array for occupancy dataset BEST PARAM ACCURACY train Scores (for all algorithms)\n",
    "occupancy_logreg_acc_train = []\n",
    "occupancy_knn_acc_train = []\n",
    "occupancy_rforest_acc_train = []\n",
    "occupancy_dtree_acc_train = []\n",
    "# get best ACCURACY scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    occupancy_logreg_acc_train.append(occupancyDATA[trial]['LogReg']['acc_dict']['acc_train'])\n",
    "    occupancy_knn_acc_train.append(occupancyDATA[trial]['KNN']['acc_dict']['acc_train'])\n",
    "    occupancy_rforest_acc_train.append(occupancyDATA[trial]['Ran_For']['acc_dict']['acc_train'])\n",
    "    occupancy_dtree_acc_train.append(occupancyDATA[trial]['Dec_Tree']['acc_dict']['acc_train'])\n",
    " \n",
    " \n",
    "# make array for occupancy dataset BEST PARAM F1 train Scores (for all algorithms)\n",
    "occupancy_logreg_f1_train = []\n",
    "occupancy_knn_f1_train = []\n",
    "occupancy_rforest_f1_train = []\n",
    "occupancy_dtree_f1_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    occupancy_logreg_f1_train.append(occupancyDATA[trial]['LogReg']['f1_dict']['f1_train'])\n",
    "    occupancy_knn_f1_train.append(occupancyDATA[trial]['KNN']['f1_dict']['f1_train'])\n",
    "    occupancy_rforest_f1_train.append(occupancyDATA[trial]['Ran_For']['f1_dict']['f1_train'])\n",
    "    occupancy_dtree_f1_train.append(occupancyDATA[trial]['Dec_Tree']['f1_dict']['f1_train'])\n",
    " \n",
    " \n",
    "# make array for occupancy dataset BEST PARAM ROC AUC train Scores (for all algorithms)\n",
    "occupancy_logreg_roc_auc_train = []\n",
    "occupancy_knn_roc_auc_train = []\n",
    "occupancy_rforest_roc_auc_train = []\n",
    "occupancy_dtree_roc_auc_train = []\n",
    "# get best F1 scores (for all algorithms)\n",
    "for trial in range(5):\n",
    "    occupancy_logreg_roc_auc_train.append(occupancyDATA[trial]['LogReg']['roc_auc_dict']['roc_auc_train'])\n",
    "    occupancy_knn_roc_auc_train.append(occupancyDATA[trial]['KNN']['roc_auc_dict']['roc_auc_train'])\n",
    "    occupancy_rforest_roc_auc_train.append(occupancyDATA[trial]['Ran_For']['roc_auc_dict']['roc_auc_train'])\n",
    "    occupancy_dtree_roc_auc_train.append(occupancyDATA[trial]['Dec_Tree']['roc_auc_dict']['roc_auc_train'])"
   ]
  },
  {
   "source": [
    "### Get Accuracy Averages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.97988\n1.0\n1.0\n0.98188\n"
     ]
    }
   ],
   "source": [
    "# get average of logistic regression accuracy average\n",
    "# get average of trials for adult logreg accuracy\n",
    "adult_logreg_acc_train_avg = sum(adult_logreg_acc_train)/len(adult_logreg_acc_train)\n",
    "# get average of trials for grid logreg accuracy\n",
    "grid_logreg_acc_train_avg = sum(grid_logreg_acc_train)/len(grid_logreg_acc_train)\n",
    "# get average of trials for htru2 logreg accuracy\n",
    "htru2_logreg_acc_train_avg = sum(htru2_logreg_acc_train)/len(htru2_logreg_acc_train)\n",
    "# get average of trials for occupancy logreg accuracy\n",
    "occupancy_logreg_acc_train_avg = sum(occupancy_logreg_acc_train)/len(occupancy_logreg_acc_train)\n",
    "# get average of all logreg accuracy average\n",
    "logreg_accuracy_train_avg = (adult_logreg_acc_train_avg + grid_logreg_acc_train_avg + htru2_logreg_acc_train_avg + \n",
    "                        occupancy_logreg_acc_train_avg) / 4\n",
    "print(logreg_accuracy_train_avg)\n",
    "\n",
    "\n",
    "# get average of knn accuracy average\n",
    "# get average of trials for adult knn accuracy\n",
    "adult_knn_acc_train_avg = sum(adult_knn_acc_train)/len(adult_knn_acc_train)\n",
    "# get average of trials for grid knn accuracy\n",
    "grid_knn_acc_train_avg = sum(grid_knn_acc_train)/len(grid_knn_acc_train)\n",
    "# get average of trials for htru2 knn accuracy\n",
    "htru2_knn_acc_train_avg = sum(htru2_knn_acc_train)/len(htru2_knn_acc_train)\n",
    "# get average of trials for occupancy knn accuracy\n",
    "occupancy_knn_acc_train_avg = sum(occupancy_knn_acc_train)/len(occupancy_knn_acc_train)\n",
    "# get average of all knn accuracy average\n",
    "knn_accuracy_train_avg = (adult_knn_acc_train_avg + grid_knn_acc_train_avg + htru2_knn_acc_train_avg + \n",
    "                        occupancy_knn_acc_train_avg) / 4\n",
    "print(knn_accuracy_train_avg)\n",
    "\n",
    "\n",
    "# get average of random forest accuracy average\n",
    "# get average of trials for adult random forest accuracy\n",
    "adult_rforest_acc_train_avg = sum(adult_rforest_acc_train)/len(adult_rforest_acc_train)\n",
    "# get average of trials for grid random forest accuracy\n",
    "grid_rforest_acc_train_avg = sum(grid_rforest_acc_train)/len(grid_rforest_acc_train)\n",
    "# get average of trials for htru2 random forest accuracy\n",
    "htru2_rforest_acc_train_avg = sum(htru2_rforest_acc_train)/len(htru2_rforest_acc_train)\n",
    "# get average of trials for occupancy random forest accuracy\n",
    "occupancy_rforest_acc_train_avg = sum(occupancy_rforest_acc_train)/len(occupancy_rforest_acc_train)\n",
    "# get average of all random forest accuracy average\n",
    "rforest_accuracy_train_avg = (adult_rforest_acc_train_avg + grid_rforest_acc_train_avg + htru2_rforest_acc_train_avg + \n",
    "                        occupancy_rforest_acc_train_avg) / 4\n",
    "print(rforest_accuracy_train_avg)\n",
    "\n",
    "\n",
    "# get average of decision tree accuracy average\n",
    "# get average of trials for adult decision tree accuracy\n",
    "adult_dtree_acc_train_avg = sum(adult_dtree_acc_train)/len(adult_dtree_acc_train)\n",
    "# get average of trials for grid decision tree accuracy\n",
    "grid_dtree_acc_train_avg = sum(grid_dtree_acc_train)/len(grid_dtree_acc_train)\n",
    "# get average of trials for htru2 decision tree accuracy\n",
    "htru2_dtree_acc_train_avg = sum(htru2_dtree_acc_train)/len(htru2_dtree_acc_train)\n",
    "# get average of trials for occupancy decision tree accuracy\n",
    "occupancy_dtree_acc_train_avg = sum(occupancy_dtree_acc_train)/len(occupancy_dtree_acc_train)\n",
    "# get average of all decision tree accuracy average\n",
    "dtree_accuracy_train_avg = (adult_dtree_acc_train_avg + grid_dtree_acc_train_avg + htru2_dtree_acc_train_avg + \n",
    "                        occupancy_dtree_acc_train_avg) / 4\n",
    "print(dtree_accuracy_train_avg)"
   ]
  },
  {
   "source": [
    "### Get F1 Averages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8855140292812305\n1.0\n1.0\n0.8979720204019159\n"
     ]
    }
   ],
   "source": [
    "# get average of logistic regression f1 average\n",
    "# get average of trials for adult logreg f1\n",
    "adult_logreg_f1_train_avg = sum(adult_logreg_f1_train)/len(adult_logreg_f1_train)\n",
    "# get average of trials for grid logreg f1\n",
    "grid_logreg_f1_train_avg = sum(grid_logreg_f1_train)/len(grid_logreg_f1_train)\n",
    "# get average of trials for htru2 logreg f1\n",
    "htru2_logreg_f1_train_avg = sum(htru2_logreg_f1_train)/len(htru2_logreg_f1_train)\n",
    "# get average of trials for occupancy logreg f1\n",
    "occupancy_logreg_f1_train_avg = sum(occupancy_logreg_f1_train)/len(occupancy_logreg_f1_train)\n",
    "# get average of all logreg f1 average\n",
    "logreg_f1_train_avg = (adult_logreg_f1_train_avg + grid_logreg_f1_train_avg + htru2_logreg_f1_train_avg + \n",
    "                        occupancy_logreg_f1_train_avg) / 4\n",
    "print(logreg_f1_train_avg)\n",
    " \n",
    " \n",
    "# get average of knn f1 average\n",
    "# get average of trials for adult knn f1\n",
    "adult_knn_f1_train_avg = sum(adult_knn_f1_train)/len(adult_knn_f1_train)\n",
    "# get average of trials for grid knn f1\n",
    "grid_knn_f1_train_avg = sum(grid_knn_f1_train)/len(grid_knn_f1_train)\n",
    "# get average of trials for htru2 knn f1\n",
    "htru2_knn_f1_train_avg = sum(htru2_knn_f1_train)/len(htru2_knn_f1_train)\n",
    "# get average of trials for occupancy knn f1\n",
    "occupancy_knn_f1_train_avg = sum(occupancy_knn_f1_train)/len(occupancy_knn_f1_train)\n",
    "# get average of all knn f1 average\n",
    "knn_f1_train_avg = (adult_knn_f1_train_avg + grid_knn_f1_train_avg + htru2_knn_f1_train_avg + \n",
    "                        occupancy_knn_f1_train_avg) / 4\n",
    "print(knn_f1_train_avg)\n",
    " \n",
    " \n",
    "# get average of random forest f1 average\n",
    "# get average of trials for adult random forest f1\n",
    "adult_rforest_f1_train_avg = sum(adult_rforest_f1_train)/len(adult_rforest_f1_train)\n",
    "# get average of trials for grid random forest f1\n",
    "grid_rforest_f1_train_avg = sum(grid_rforest_f1_train)/len(grid_rforest_f1_train)\n",
    "# get average of trials for htru2 random forest f1\n",
    "htru2_rforest_f1_train_avg = sum(htru2_rforest_f1_train)/len(htru2_rforest_f1_train)\n",
    "# get average of trials for occupancy random forest f1\n",
    "occupancy_rforest_f1_train_avg = sum(occupancy_rforest_f1_train)/len(occupancy_rforest_f1_train)\n",
    "# get average of all random forest f1 average\n",
    "rforest_f1_train_avg = (adult_rforest_f1_train_avg + grid_rforest_f1_train_avg + htru2_rforest_f1_train_avg + \n",
    "                        occupancy_rforest_f1_train_avg) / 4\n",
    "print(rforest_f1_train_avg)\n",
    " \n",
    " \n",
    "# get average of decision tree f1 average\n",
    "# get average of trials for adult decision tree f1\n",
    "adult_dtree_f1_train_avg = sum(adult_dtree_f1_train)/len(adult_dtree_f1_train)\n",
    "# get average of trials for grid decision tree f1\n",
    "grid_dtree_f1_train_avg = sum(grid_dtree_f1_train)/len(grid_dtree_f1_train)\n",
    "# get average of trials for htru2 decision tree f1\n",
    "htru2_dtree_f1_train_avg = sum(htru2_dtree_f1_train)/len(htru2_dtree_f1_train)\n",
    "# get average of trials for occupancy decision tree f1\n",
    "occupancy_dtree_f1_train_avg = sum(occupancy_dtree_f1_train)/len(occupancy_dtree_f1_train)\n",
    "# get average of all decision tree f1 average\n",
    "dtree_f1_train_avg = (adult_dtree_f1_train_avg + grid_dtree_f1_train_avg + htru2_dtree_f1_train_avg + \n",
    "                        occupancy_dtree_f1_train_avg) / 4\n",
    "print(dtree_f1_train_avg)"
   ]
  },
  {
   "source": [
    "### Get ROC AUC Averages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9107285382054966\n1.0\n1.0\n0.927505804326594\n"
     ]
    }
   ],
   "source": [
    "# get average of logistic regression roc_auc average\n",
    "# get average of trials for adult logreg roc_auc\n",
    "adult_logreg_roc_auc_train_avg = sum(adult_logreg_roc_auc_train)/len(adult_logreg_roc_auc_train)\n",
    "# get average of trials for grid logreg roc_auc\n",
    "grid_logreg_roc_auc_train_avg = sum(grid_logreg_roc_auc_train)/len(grid_logreg_roc_auc_train)\n",
    "# get average of trials for htru2 logreg roc_auc\n",
    "htru2_logreg_roc_auc_train_avg = sum(htru2_logreg_roc_auc_train)/len(htru2_logreg_roc_auc_train)\n",
    "# get average of trials for occupancy logreg roc_auc\n",
    "occupancy_logreg_roc_auc_train_avg = sum(occupancy_logreg_roc_auc_train)/len(occupancy_logreg_roc_auc_train)\n",
    "# get average of all logreg roc_auc average\n",
    "logreg_roc_auc_train_avg = (adult_logreg_roc_auc_train_avg + grid_logreg_roc_auc_train_avg + htru2_logreg_roc_auc_train_avg + \n",
    "                        occupancy_logreg_roc_auc_train_avg) / 4\n",
    "print(logreg_roc_auc_train_avg)\n",
    " \n",
    " \n",
    "# get average of knn roc_auc average\n",
    "# get average of trials for adult knn roc_auc\n",
    "adult_knn_roc_auc_train_avg = sum(adult_knn_roc_auc_train)/len(adult_knn_roc_auc_train)\n",
    "# get average of trials for grid knn roc_auc\n",
    "grid_knn_roc_auc_train_avg = sum(grid_knn_roc_auc_train)/len(grid_knn_roc_auc_train)\n",
    "# get average of trials for htru2 knn roc_auc\n",
    "htru2_knn_roc_auc_train_avg = sum(htru2_knn_roc_auc_train)/len(htru2_knn_roc_auc_train)\n",
    "# get average of trials for occupancy knn roc_auc\n",
    "occupancy_knn_roc_auc_train_avg = sum(occupancy_knn_roc_auc_train)/len(occupancy_knn_roc_auc_train)\n",
    "# get average of all knn roc_auc average\n",
    "knn_roc_auc_train_avg = (adult_knn_roc_auc_train_avg + grid_knn_roc_auc_train_avg + htru2_knn_roc_auc_train_avg + \n",
    "                        occupancy_knn_roc_auc_train_avg) / 4\n",
    "print(knn_roc_auc_train_avg)\n",
    " \n",
    " \n",
    "# get average of random forest roc_auc average\n",
    "# get average of trials for adult random forest roc_auc\n",
    "adult_rforest_roc_auc_train_avg = sum(adult_rforest_roc_auc_train)/len(adult_rforest_roc_auc_train)\n",
    "# get average of trials for grid random forest roc_auc\n",
    "grid_rforest_roc_auc_train_avg = sum(grid_rforest_roc_auc_train)/len(grid_rforest_roc_auc_train)\n",
    "# get average of trials for htru2 random forest roc_auc\n",
    "htru2_rforest_roc_auc_train_avg = sum(htru2_rforest_roc_auc_train)/len(htru2_rforest_roc_auc_train)\n",
    "# get average of trials for occupancy random forest roc_auc\n",
    "occupancy_rforest_roc_auc_train_avg = sum(occupancy_rforest_roc_auc_train)/len(occupancy_rforest_roc_auc_train)\n",
    "# get average of all random forest roc_auc average\n",
    "rforest_roc_auc_train_avg = (adult_rforest_roc_auc_train_avg + grid_rforest_roc_auc_train_avg + htru2_rforest_roc_auc_train_avg + \n",
    "                        occupancy_rforest_roc_auc_train_avg) / 4\n",
    "print(rforest_roc_auc_train_avg)\n",
    " \n",
    " \n",
    "# get average of decision tree roc_auc average\n",
    "# get average of trials for adult decision tree roc_auc\n",
    "adult_dtree_roc_auc_train_avg = sum(adult_dtree_roc_auc_train)/len(adult_dtree_roc_auc_train)\n",
    "# get average of trials for grid decision tree roc_auc\n",
    "grid_dtree_roc_auc_train_avg = sum(grid_dtree_roc_auc_train)/len(grid_dtree_roc_auc_train)\n",
    "# get average of trials for htru2 decision tree roc_auc\n",
    "htru2_dtree_roc_auc_train_avg = sum(htru2_dtree_roc_auc_train)/len(htru2_dtree_roc_auc_train)\n",
    "# get average of trials for occupancy decision tree roc_auc\n",
    "occupancy_dtree_roc_auc_train_avg = sum(occupancy_dtree_roc_auc_train)/len(occupancy_dtree_roc_auc_train)\n",
    "# get average of all decision tree roc_auc average\n",
    "dtree_roc_auc_train_avg = (adult_dtree_roc_auc_train_avg + grid_dtree_roc_auc_train_avg + htru2_dtree_roc_auc_train_avg + \n",
    "                        occupancy_dtree_roc_auc_train_avg) / 4\n",
    "print(dtree_roc_auc_train_avg)"
   ]
  },
  {
   "source": [
    "## Analyze Train Score Data by SCORES and MODEL (Secondary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    MODEL      ACC        F1   ROC AUC      MEAN\n",
       "1     KNN  1.00000  1.000000  1.000000  1.000000\n",
       "2      RF  1.00000  1.000000  1.000000  1.000000\n",
       "3      DT  0.98188  0.897972  0.927506  0.935786\n",
       "0  LogReg  0.97988  0.885514  0.910729  0.925374"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MODEL</th>\n      <th>ACC</th>\n      <th>F1</th>\n      <th>ROC AUC</th>\n      <th>MEAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>KNN</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RF</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DT</td>\n      <td>0.98188</td>\n      <td>0.897972</td>\n      <td>0.927506</td>\n      <td>0.935786</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LogReg</td>\n      <td>0.97988</td>\n      <td>0.885514</td>\n      <td>0.910729</td>\n      <td>0.925374</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# accuracy column\n",
    "t4_acc = [logreg_accuracy_train_avg, knn_accuracy_train_avg, rforest_accuracy_train_avg, dtree_accuracy_train_avg]\n",
    "# f1 column\n",
    "t4_f1 = [logreg_f1_train_avg, knn_f1_train_avg, rforest_f1_train_avg, dtree_f1_train_avg]\n",
    "# roc auc column\n",
    "t4_roc_auc = [logreg_roc_auc_train_avg, knn_roc_auc_train_avg, rforest_roc_auc_train_avg, dtree_roc_auc_train_avg]\n",
    "# get average of rows\n",
    "t4_logreg_mean = (logreg_accuracy_train_avg + logreg_f1_train_avg + logreg_roc_auc_train_avg)/3\n",
    "t4_knn_mean = (knn_accuracy_train_avg + knn_f1_train_avg + knn_roc_auc_train_avg)/3\n",
    "t4_rforest_mean = (rforest_accuracy_train_avg + rforest_f1_train_avg + rforest_roc_auc_train_avg)/3\n",
    "t4_dtree_mean = (dtree_accuracy_train_avg + dtree_f1_train_avg + dtree_roc_auc_train_avg)/3\n",
    "# mean column\n",
    "t4_mean = [t4_logreg_mean, t4_knn_mean, t4_rforest_mean, t4_dtree_mean]\n",
    "# make dictionary and dataframe\n",
    "t4 = {'MODEL': models, 'ACC': t4_acc, 'F1': t4_f1, 'ROC AUC': t4_roc_auc, 'MEAN': t4_mean}\n",
    "t4 = pd.DataFrame.from_dict(t4)\n",
    "t4.sort_values(by='MEAN', ascending=False, inplace=True)\n",
    "t4"
   ]
  },
  {
   "source": [
    "## Analyze Train Score Data by DATASET and MODEL (Secondary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    MODEL     ADULT      GRID     HTRU2  OCCUPANCY      MEAN\n",
       "1     KNN  1.000000  1.000000  1.000000   1.000000  1.000000\n",
       "2      RF  1.000000  1.000000  1.000000   1.000000  1.000000\n",
       "3      DT  0.935786  0.935786  0.935786   0.935786  0.935786\n",
       "0  LogReg  0.925374  0.925374  0.925374   0.925374  0.925374"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MODEL</th>\n      <th>ADULT</th>\n      <th>GRID</th>\n      <th>HTRU2</th>\n      <th>OCCUPANCY</th>\n      <th>MEAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>KNN</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RF</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DT</td>\n      <td>0.935786</td>\n      <td>0.935786</td>\n      <td>0.935786</td>\n      <td>0.935786</td>\n      <td>0.935786</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LogReg</td>\n      <td>0.925374</td>\n      <td>0.925374</td>\n      <td>0.925374</td>\n      <td>0.925374</td>\n      <td>0.925374</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# adult column\n",
    "adult_logreg_train_avg = (adult_logreg_acc_train_avg + adult_logreg_f1_train_avg + adult_logreg_roc_auc_train_avg) / 3\n",
    "adult_knn_train_avg = (adult_knn_acc_train_avg + adult_knn_f1_train_avg + adult_knn_roc_auc_train_avg) / 3\n",
    "adult_rforest_train_avg = (adult_rforest_acc_train_avg + adult_rforest_f1_train_avg + adult_rforest_roc_auc_train_avg) / 3\n",
    "adult_dtree_train_avg = (adult_dtree_acc_train_avg + adult_dtree_f1_train_avg + adult_dtree_roc_auc_train_avg) / 3\n",
    "t5_adult = [adult_logreg_train_avg, adult_knn_train_avg, adult_rforest_train_avg, adult_dtree_train_avg]\n",
    "\n",
    "# grid column\n",
    "grid_logreg_train_avg = (grid_logreg_acc_train_avg + grid_logreg_f1_train_avg + grid_logreg_roc_auc_train_avg) / 3\n",
    "grid_knn_train_avg = (grid_knn_acc_train_avg + grid_knn_f1_train_avg + grid_knn_roc_auc_train_avg) / 3\n",
    "grid_rforest_train_avg = (grid_rforest_acc_train_avg + grid_rforest_f1_train_avg + grid_rforest_roc_auc_train_avg) / 3\n",
    "grid_dtree_train_avg = (grid_dtree_acc_train_avg + grid_dtree_f1_train_avg + grid_dtree_roc_auc_train_avg) / 3\n",
    "t5_grid = [grid_logreg_train_avg, grid_knn_train_avg, grid_rforest_train_avg, grid_dtree_train_avg]\n",
    "\n",
    "# htru2 column\n",
    "htru2_logreg_train_avg = (htru2_logreg_acc_train_avg + htru2_logreg_f1_train_avg + htru2_logreg_roc_auc_train_avg) / 3\n",
    "htru2_knn_train_avg = (htru2_knn_acc_train_avg + htru2_knn_f1_train_avg + htru2_knn_roc_auc_train_avg) / 3\n",
    "htru2_rforest_train_avg = (htru2_rforest_acc_train_avg + htru2_rforest_f1_train_avg + htru2_rforest_roc_auc_train_avg) / 3\n",
    "htru2_dtree_train_avg = (htru2_dtree_acc_train_avg + htru2_dtree_f1_train_avg + htru2_dtree_roc_auc_train_avg) / 3\n",
    "t5_htru2 = [htru2_logreg_train_avg, htru2_knn_train_avg, htru2_rforest_train_avg, htru2_dtree_train_avg]\n",
    "\n",
    "# occupancy column\n",
    "occupancy_logreg_train_avg = (occupancy_logreg_acc_train_avg + occupancy_logreg_f1_train_avg + occupancy_logreg_roc_auc_train_avg) / 3\n",
    "occupancy_knn_train_avg = (occupancy_knn_acc_train_avg + occupancy_knn_f1_train_avg + occupancy_knn_roc_auc_train_avg) / 3\n",
    "occupancy_rforest_train_avg = (occupancy_rforest_acc_train_avg + occupancy_rforest_f1_train_avg + occupancy_rforest_roc_auc_train_avg) / 3\n",
    "occupancy_dtree_train_avg = (occupancy_dtree_acc_train_avg + occupancy_dtree_f1_train_avg + occupancy_dtree_roc_auc_train_avg) / 3\n",
    "t5_occupancy = [occupancy_logreg_train_avg, occupancy_knn_train_avg, occupancy_rforest_train_avg, occupancy_dtree_train_avg]\n",
    "\n",
    "# mean column\n",
    "# get average of rows\n",
    "t5_logreg_mean = (adult_logreg_train_avg + grid_logreg_train_avg + htru2_logreg_train_avg + occupancy_logreg_train_avg)/4\n",
    "t5_knn_mean = (adult_knn_train_avg + grid_knn_train_avg + htru2_knn_train_avg + occupancy_knn_train_avg)/4\n",
    "t5_rforest_mean = (adult_rforest_train_avg + grid_rforest_train_avg + htru2_rforest_train_avg + occupancy_rforest_train_avg)/4\n",
    "t5_dtree_mean = (adult_dtree_train_avg + grid_dtree_train_avg + htru2_dtree_train_avg + occupancy_dtree_train_avg)/4\n",
    "# mean column\n",
    "t5_mean = [t5_logreg_mean, t5_knn_mean, t5_rforest_mean, t5_dtree_mean]\n",
    "\n",
    "# make dictionary and dataframe\n",
    "t5 = {'MODEL': models, 'ADULT': t5_adult, 'GRID': t5_grid, 'HTRU2': t5_htru2, 'OCCUPANCY': t5_occupancy, 'MEAN': t5_mean}\n",
    "t5 = pd.DataFrame.from_dict(t5)\n",
    "t5.sort_values(by='MEAN', ascending=False, inplace=True)\n",
    "t5"
   ]
  },
  {
   "source": [
    "## Raw Test Data Scores (Secondary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.9793766475422546, 0.9787563963405179, 0.9787563963405179, 0.9792215847418204, 0.9791440533416034, 0.9782136765389983, 0.9766630485346566, 0.9778260195379128, 0.97790355093813, 0.977981082338347, 0.978833927740735, 0.9785238021398667, 0.9797643045433401, 0.9796092417429059, 0.9794541789424717, 0.9774383625368274, 0.9762753915335711, 0.9772057683361761, 0.9768956427353078, 0.9783687393394325, 0.8788706739526412, 0.877019748653501, 0.8741965105601468, 0.8815207780725022, 0.8740046838407494, 0.8700878409616275, 0.8655649843680215, 0.8654750705550328, 0.8728246318607765, 0.8671655753040224, 0.8761384335154827, 0.8769298632554036, 0.8824593128390597, 0.8832309043020192, 0.8765603328710125, 0.8675466545289031, 0.8657894736842107, 0.8656579550664832, 0.8706597222222223, 0.8711316397228636, 0.91139312507293, 0.9087629957619124, 0.898173984280681, 0.9025418974291459, 0.9025269055972525, 0.8792408265597675, 0.8810083019183776, 0.8645984023737369, 0.8754379060706958, 0.8741199982235643, 0.9151309775553271, 0.9139972703352985, 0.9187513272488141, 0.9141902468157529, 0.9082733786324406, 0.9163393736678289, 0.9057107572817216, 0.8839700754522619, 0.9210905698447808, 0.9124359372918239, 0.9793766475422546, 0.9787563963405179, 0.9787563963405179, 0.9792215847418204, 0.9791440533416034, 0.9782136765389983, 0.9766630485346566, 0.9778260195379128, 0.97790355093813, 0.977981082338347, 0.978833927740735, 0.9785238021398667, 0.9797643045433401, 0.9796092417429059, 0.9794541789424717, 0.9774383625368274, 0.9762753915335711, 0.9772057683361761, 0.9768956427353078, 0.9783687393394325, 0.8788706739526412, 0.877019748653501, 0.8741965105601468, 0.8815207780725022, 0.8740046838407494, 0.8700878409616275, 0.8655649843680215, 0.8654750705550328, 0.8728246318607765, 0.8671655753040224, 0.8761384335154827, 0.8769298632554036, 0.8824593128390597, 0.8832309043020192, 0.8765603328710125, 0.8675466545289031, 0.8657894736842107, 0.8656579550664832, 0.8706597222222223, 0.8711316397228636, 0.91139312507293, 0.9087629957619124, 0.898173984280681, 0.9025418974291459, 0.9025269055972525, 0.8792408265597675, 0.8810083019183776, 0.8645984023737369, 0.8754379060706958, 0.8741199982235643, 0.9151309775553271, 0.9139972703352985, 0.9187513272488141, 0.9141902468157529, 0.9082733786324406, 0.9163393736678289, 0.9057107572817216, 0.8839700754522619, 0.9210905698447808, 0.9124359372918239, 0.9793766475422546, 0.9787563963405179, 0.9787563963405179, 0.9792215847418204, 0.9791440533416034, 0.9782136765389983, 0.9766630485346566, 0.9778260195379128, 0.97790355093813, 0.977981082338347, 0.978833927740735, 0.9785238021398667, 0.9797643045433401, 0.9796092417429059, 0.9794541789424717, 0.9774383625368274, 0.9762753915335711, 0.9772057683361761, 0.9768956427353078, 0.9783687393394325, 0.8788706739526412, 0.877019748653501, 0.8741965105601468, 0.8815207780725022, 0.8740046838407494, 0.8700878409616275, 0.8655649843680215, 0.8654750705550328, 0.8728246318607765, 0.8671655753040224, 0.8761384335154827, 0.8769298632554036, 0.8824593128390597, 0.8832309043020192, 0.8765603328710125, 0.8675466545289031, 0.8657894736842107, 0.8656579550664832, 0.8706597222222223, 0.8711316397228636, 0.91139312507293, 0.9087629957619124, 0.898173984280681, 0.9025418974291459, 0.9025269055972525, 0.8792408265597675, 0.8810083019183776, 0.8645984023737369, 0.8754379060706958, 0.8741199982235643, 0.9151309775553271, 0.9139972703352985, 0.9187513272488141, 0.9141902468157529, 0.9082733786324406, 0.9163393736678289, 0.9057107572817216, 0.8839700754522619, 0.9210905698447808, 0.9124359372918239, 0.9793766475422546, 0.9787563963405179, 0.9787563963405179, 0.9792215847418204, 0.9791440533416034, 0.9782136765389983, 0.9766630485346566, 0.9778260195379128, 0.97790355093813, 0.977981082338347, 0.978833927740735, 0.9785238021398667, 0.9797643045433401, 0.9796092417429059, 0.9794541789424717, 0.9774383625368274, 0.9762753915335711, 0.9772057683361761, 0.9768956427353078, 0.9783687393394325, 0.8788706739526412, 0.877019748653501, 0.8741965105601468, 0.8815207780725022, 0.8740046838407494, 0.8700878409616275, 0.8655649843680215, 0.8654750705550328, 0.8728246318607765, 0.8671655753040224, 0.8761384335154827, 0.8769298632554036, 0.8824593128390597, 0.8832309043020192, 0.8765603328710125, 0.8675466545289031, 0.8657894736842107, 0.8656579550664832, 0.8706597222222223, 0.8711316397228636, 0.91139312507293, 0.9087629957619124, 0.898173984280681, 0.9025418974291459, 0.9025269055972525, 0.8792408265597675, 0.8810083019183776, 0.8645984023737369, 0.8754379060706958, 0.8741199982235643, 0.9151309775553271, 0.9139972703352985, 0.9187513272488141, 0.9141902468157529, 0.9082733786324406, 0.9163393736678289, 0.9057107572817216, 0.8839700754522619, 0.9210905698447808, 0.9124359372918239]\n\n3 scoring metrics X 4 datasets X 4 algorithms X 5 trials = 240\n"
     ]
    }
   ],
   "source": [
    "                    # array of (5 trial) accuracy scores from adult dataset\n",
    "raw_test_scores =   (adult_logreg_acc +\n",
    "                    adult_knn_acc +\n",
    "                    adult_rforest_acc +\n",
    "                    adult_dtree_acc +\n",
    "\n",
    "                    # array of (5 trial) f1 scores from adult dataset\n",
    "                    adult_logreg_f1 +\n",
    "                    adult_knn_f1 +\n",
    "                    adult_rforest_f1 +\n",
    "                    adult_dtree_f1 +\n",
    "\n",
    "                    # array of (5 trial) roc auc scores from adult dataset\n",
    "                    adult_logreg_roc_auc +\n",
    "                    adult_knn_roc_auc +\n",
    "                    adult_rforest_roc_auc +\n",
    "                    adult_dtree_roc_auc +\n",
    "\n",
    "\n",
    "                    # array of (5 trial) accuracy scores from grid dataset\n",
    "                    grid_logreg_acc +\n",
    "                    grid_knn_acc +\n",
    "                    grid_rforest_acc +\n",
    "                    grid_dtree_acc +\n",
    "                    \n",
    "                    # array of (5 trial) f1 scores from grid dataset\n",
    "                    grid_logreg_f1 +\n",
    "                    grid_knn_f1 +\n",
    "                    grid_rforest_f1 +\n",
    "                    grid_dtree_f1 +\n",
    "                    \n",
    "                    # array of (5 trial) roc auc scores from grid dataset\n",
    "                    grid_logreg_roc_auc +\n",
    "                    grid_knn_roc_auc +\n",
    "                    grid_rforest_roc_auc +\n",
    "                    grid_dtree_roc_auc +\n",
    "\n",
    "\n",
    "                    # array of (5 trial) accuracy scores from htru2 dataset\n",
    "                    htru2_logreg_acc +\n",
    "                    htru2_knn_acc +\n",
    "                    htru2_rforest_acc +\n",
    "                    htru2_dtree_acc +\n",
    "                    \n",
    "                    # array of (5 trial) f1 scores from htru2 dataset\n",
    "                    htru2_logreg_f1 +\n",
    "                    htru2_knn_f1 +\n",
    "                    htru2_rforest_f1 +\n",
    "                    htru2_dtree_f1 +\n",
    "                    \n",
    "                    # array of (5 trial) roc auc scores from htru2 dataset\n",
    "                    htru2_logreg_roc_auc +\n",
    "                    htru2_knn_roc_auc +\n",
    "                    htru2_rforest_roc_auc +\n",
    "                    htru2_dtree_roc_auc +\n",
    "\n",
    "\n",
    "                    # array of (5 trial) accuracy scores from occupancy dataset\n",
    "                    occupancy_logreg_acc +\n",
    "                    occupancy_knn_acc +\n",
    "                    occupancy_rforest_acc +\n",
    "                    occupancy_dtree_acc +\n",
    "                    \n",
    "                    # array of (5 trial) f1 scores from occupancy dataset\n",
    "                    occupancy_logreg_f1 +\n",
    "                    occupancy_knn_f1 +\n",
    "                    occupancy_rforest_f1 +\n",
    "                    occupancy_dtree_f1 +\n",
    "                    \n",
    "                    # array of (5 trial) roc auc scores from occupancy dataset\n",
    "                    occupancy_logreg_roc_auc +\n",
    "                    occupancy_knn_roc_auc +\n",
    "                    occupancy_rforest_roc_auc +\n",
    "                    occupancy_dtree_roc_auc)\n",
    "\n",
    "print(raw_test_scores)\n",
    "print(\"\\n3 scoring metrics X 4 datasets X 4 algorithms X 5 trials = \" + str(len(raw_test_scores)))"
   ]
  },
  {
   "source": [
    "## P-Value Tables for Table 2 (Secondary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipay stats to get t-test function\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  MODEL           ACC            F1       ROC AUC      MEAN\n",
       "0    RF           NaN           NaN           NaN       NaN\n",
       "1    DT  3.443900e-11  1.393094e-10  9.339767e-02  0.132168\n",
       "2    LR  1.468651e-01  3.145009e-02  2.616912e-06  0.306631\n",
       "3   KNN  2.114964e-11  4.557926e-11  5.161585e-15  0.268552"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MODEL</th>\n      <th>ACC</th>\n      <th>F1</th>\n      <th>ROC AUC</th>\n      <th>MEAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>3.443900e-11</td>\n      <td>1.393094e-10</td>\n      <td>9.339767e-02</td>\n      <td>0.132168</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LR</td>\n      <td>1.468651e-01</td>\n      <td>3.145009e-02</td>\n      <td>2.616912e-06</td>\n      <td>0.306631</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>KNN</td>\n      <td>2.114964e-11</td>\n      <td>4.557926e-11</td>\n      <td>5.161585e-15</td>\n      <td>0.268552</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# best performing in ACC column (Random Tree)\n",
    "# get array of values (5 trials X 4 datasets = 20 values)\n",
    "best_acc = (adult_rforest_acc + grid_rforest_acc + htru2_rforest_acc + occupancy_rforest_acc)\n",
    "# best performing in F1 column (Random Tree)\n",
    "# get array of values (5 trials X 4 datasets = 20 values)\n",
    "best_f1 = (adult_rforest_f1 + grid_rforest_f1 + htru2_rforest_f1 + occupancy_rforest_f1)\n",
    "# best performing in ROC AUC column (Random Tree)\n",
    "# get array of values (5 trials X 4 datasets = 20 values)\n",
    "best_roc_auc = (adult_rforest_roc_auc + grid_rforest_roc_auc + htru2_rforest_roc_auc + occupancy_rforest_roc_auc)\n",
    "# best performing in MEAN column (Random Tree)\n",
    "best_mean = [rforest_accuracy_avg, rforest_f1_avg, rforest_roc_auc_avg]\n",
    "\n",
    "# get array of values for other rows\n",
    "# get decision tree arrays\n",
    "dtree_acc = (adult_dtree_acc + grid_dtree_acc + htru2_dtree_acc + occupancy_dtree_acc)\n",
    "dtree_f1 = (adult_dtree_f1 + grid_dtree_f1 + htru2_dtree_f1 + occupancy_dtree_f1)\n",
    "dtree_roc_auc = (adult_dtree_roc_auc + grid_dtree_roc_auc + htru2_dtree_roc_auc + occupancy_dtree_roc_auc)\n",
    "dtree_mean = [dtree_accuracy_avg, dtree_f1_avg, dtree_roc_auc_avg]\n",
    "# get logistic regression arrays\n",
    "logreg_acc = (adult_logreg_acc + grid_logreg_acc + htru2_logreg_acc + occupancy_logreg_acc)\n",
    "logreg_f1 = (adult_logreg_f1 + grid_logreg_f1 + htru2_logreg_f1 + occupancy_logreg_f1)\n",
    "logreg_roc_auc = (adult_logreg_roc_auc + grid_logreg_roc_auc + htru2_logreg_roc_auc + occupancy_logreg_roc_auc)\n",
    "logreg_mean = [logreg_accuracy_avg, logreg_f1_avg, logreg_roc_auc_avg]\n",
    "# get logistic regression arrays\n",
    "knn_acc = (adult_knn_acc + grid_knn_acc + htru2_knn_acc + occupancy_knn_acc)\n",
    "knn_f1 = (adult_knn_f1 + grid_knn_f1 + htru2_knn_f1 + occupancy_knn_f1)\n",
    "knn_roc_auc = (adult_knn_roc_auc + grid_knn_roc_auc + htru2_knn_roc_auc + occupancy_knn_roc_auc)\n",
    "knn_mean = [knn_accuracy_avg, knn_f1_avg, knn_roc_auc_avg]\n",
    "\n",
    "\n",
    "# make arrays for p-values\n",
    "# array for ACC column\n",
    "acc_pvalue = [None] * 4\n",
    "# array for F1 column\n",
    "f1_pvalue = [None] * 4\n",
    "# array for ROC AUC column\n",
    "roc_auc_pvalue = [None] * 4\n",
    "# array for ROC AUC column\n",
    "mean_pvalue = [None] * 4\n",
    "\n",
    "\n",
    "# fill the pvalue array for accuracy\n",
    "for i, acc, f1, roc_auc, mean in zip(range(4), [best_acc, dtree_acc, logreg_acc, knn_acc], \n",
    "                        [best_f1, dtree_f1, logreg_f1, knn_f1], [best_roc_auc, dtree_roc_auc, logreg_roc_auc, knn_roc_auc],\n",
    "                        [best_mean, dtree_mean, logreg_mean, knn_mean]):\n",
    "    acc_pvalue[i] = stats.ttest_rel(best_acc, acc)[1]\n",
    "    f1_pvalue[i] = stats.ttest_rel(best_f1, f1)[1]\n",
    "    roc_auc_pvalue[i] = stats.ttest_rel(best_roc_auc, roc_auc)[1]\n",
    "    mean_pvalue[i] = stats.ttest_rel(best_mean, mean)[1]\n",
    "\n",
    "t6 = {'MODEL': ['RF', 'DT', 'LR', 'KNN'], 'ACC': acc_pvalue, 'F1': f1_pvalue, 'ROC AUC': roc_auc_pvalue, 'MEAN': mean_pvalue}\n",
    "t6 = pd.DataFrame.from_dict(t6)\n",
    "t6\n"
   ]
  },
  {
   "source": [
    "## P-Value Tables for Table 3 (Secondary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.9241231668319955, 0.9241231668319955, 0.9241231668319955, 0.9241231668319955]\n",
      "Ttest_relResult(statistic=nan, pvalue=nan)\n",
      "[0.9177677375496277, 0.9177677375496277, 0.9177677375496277, 0.9177677375496277]\n",
      "Ttest_relResult(statistic=inf, pvalue=0.0)\n",
      "[0.9202844254352117, 0.9202844254352117, 0.9202844254352117, 0.9202844254352117]\n",
      "Ttest_relResult(statistic=inf, pvalue=0.0)\n",
      "[0.9069407277389113, 0.9069407277389113, 0.9069407277389113, 0.9069407277389113]\n",
      "Ttest_relResult(statistic=inf, pvalue=0.0)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  MODEL     ADULT      GRID     HTRU2  OCCUPANCY  MEAN\n",
       "0    RF       NaN       NaN       NaN        NaN   NaN\n",
       "1    DT  0.028778  0.028778  0.028778   0.028778   0.0\n",
       "2    LR  0.025984  0.025984  0.025984   0.025984   0.0\n",
       "3   KNN  0.001815  0.001815  0.001815   0.001815   0.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MODEL</th>\n      <th>ADULT</th>\n      <th>GRID</th>\n      <th>HTRU2</th>\n      <th>OCCUPANCY</th>\n      <th>MEAN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>0.028778</td>\n      <td>0.028778</td>\n      <td>0.028778</td>\n      <td>0.028778</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LR</td>\n      <td>0.025984</td>\n      <td>0.025984</td>\n      <td>0.025984</td>\n      <td>0.025984</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>KNN</td>\n      <td>0.001815</td>\n      <td>0.001815</td>\n      <td>0.001815</td>\n      <td>0.001815</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# best performing in ADULT column (Random Tree)\n",
    "# get array of values (5 trials X 3 scores = 15 values)\n",
    "best_adult = (adult_rforest_acc + adult_rforest_f1 + adult_rforest_roc_auc)\n",
    "# best performing in GRID column (Random Tree)\n",
    "# get array of values (5 trials X 3 scores = 15 values)\n",
    "best_grid = (grid_rforest_acc + grid_rforest_f1 + grid_rforest_roc_auc)\n",
    "# best performing in HTRU2 column (Random Tree)\n",
    "# get array of values (5 trials X 3 scores = 15 values)\n",
    "best_htru2 = (htru2_rforest_acc + htru2_rforest_f1 + htru2_rforest_roc_auc)\n",
    "# best performing in Occupancy column (Random Tree)\n",
    "# get array of values (5 trials X 3 scores = 15 values)\n",
    "best_occupancy = (occupancy_rforest_acc + occupancy_rforest_f1 + occupancy_rforest_roc_auc)\n",
    "# best performing in MEAN column (Random Tree)\n",
    "best_mean = [adult_rforest_avg, grid_rforest_avg, htru2_rforest_avg, occupancy_rforest_avg]\n",
    "\n",
    "# get array of values for other rows\n",
    "# get decision tree arrays\n",
    "dtree_adult = (adult_dtree_acc + adult_dtree_f1 + adult_dtree_roc_auc)\n",
    "dtree_grid = (grid_dtree_acc + grid_dtree_f1 + grid_dtree_roc_auc)\n",
    "dtree_htru2 = (htru2_dtree_acc + htru2_dtree_f1 + htru2_dtree_roc_auc)\n",
    "dtree_occupancy = (occupancy_dtree_acc + occupancy_dtree_f1 + occupancy_dtree_roc_auc)\n",
    "dtree_mean = [adult_dtree_avg, grid_dtree_avg, htru2_dtree_avg, occupancy_dtree_avg]\n",
    "# get logistic regression arrays\n",
    "logreg_adult = (adult_logreg_acc + adult_logreg_f1 + adult_logreg_roc_auc)\n",
    "logreg_grid = (grid_logreg_acc + grid_logreg_f1 + grid_logreg_roc_auc)\n",
    "logreg_htru2 = (htru2_logreg_acc + htru2_logreg_f1 + htru2_logreg_roc_auc)\n",
    "logreg_occupancy = (occupancy_logreg_acc + occupancy_logreg_f1 + occupancy_logreg_roc_auc)\n",
    "logreg_mean = [adult_logreg_avg, grid_logreg_avg, htru2_logreg_avg, occupancy_logreg_avg]\n",
    "# get logistic regression arrays\n",
    "knn_adult = (adult_knn_acc + adult_knn_f1 + adult_knn_roc_auc)\n",
    "knn_grid = (grid_knn_acc + grid_knn_f1 + grid_knn_roc_auc)\n",
    "knn_htru2 = (htru2_knn_acc + htru2_knn_f1 + htru2_knn_roc_auc)\n",
    "knn_occupancy = (occupancy_knn_acc + occupancy_knn_f1 + occupancy_knn_roc_auc)\n",
    "knn_mean = [adult_knn_avg, grid_knn_avg, htru2_knn_avg, occupancy_knn_avg]\n",
    "\n",
    "\n",
    "# make arrays for p-values\n",
    "# array for ADULT column\n",
    "adult_pvalue = [None] * 4\n",
    "# array for GRID column\n",
    "grid_pvalue = [None] * 4\n",
    "# array for HTRU2 column\n",
    "htru2_pvalue = [None] * 4\n",
    "# array for Occupancy column\n",
    "occupancy_pvalue = [None] * 4\n",
    "# array for Mean column\n",
    "mean_pvalue1 = [None] * 4\n",
    "\n",
    "\n",
    "# fill the pvalue array for accuracy\n",
    "for i, adult, grid, htru2, occupancy, mean in zip(range(4), [best_adult, dtree_adult, logreg_adult, knn_adult], \n",
    "                        [best_grid, dtree_grid, logreg_grid, knn_grid], [best_htru2, dtree_htru2, logreg_htru2, knn_htru2],\n",
    "                        [best_occupancy, dtree_occupancy, logreg_occupancy, knn_occupancy], [best_mean, dtree_mean, logreg_mean, knn_mean]):\n",
    "    adult_pvalue[i] = stats.ttest_rel(best_adult, adult)[1]\n",
    "    grid_pvalue[i] = stats.ttest_rel(best_grid, grid)[1]\n",
    "    htru2_pvalue[i] = stats.ttest_rel(best_htru2, htru2)[1]\n",
    "    occupancy_pvalue[i] = stats.ttest_rel(best_occupancy, occupancy)[1]\n",
    "    mean_pvalue1[i] = stats.ttest_rel(best_mean, mean)[1]\n",
    "    print(mean)\n",
    "    print(stats.ttest_rel(best_mean, mean))\n",
    "\n",
    "t7 = {'MODEL': ['RF', 'DT', 'LR', 'KNN'], 'ADULT': adult_pvalue, 'GRID': grid_pvalue, 'HTRU2': htru2_pvalue, 'OCCUPANCY': occupancy_pvalue, 'MEAN': mean_pvalue1}\n",
    "t7 = pd.DataFrame.from_dict(t7)\n",
    "t7"
   ]
  }
 ]
}