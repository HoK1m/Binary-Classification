{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9032458e503ab28519db53568226f597adad35e1b11ccc360aee2243f83ff687"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessarily libraries for the binary classification task\n",
    "\n",
    "# libraries imported for data processing and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# libraries imported for learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import pipeline\n",
    "\n",
    "# libraries imported for performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         mean_int  stddev_int  excess_int  skew_int  mean_dmsnr  stddev_dmsnr  \\\n",
       "0      140.562500   55.683782   -0.234571 -0.699648    3.199833     19.110426   \n",
       "1      102.507812   58.882430    0.465318 -0.515088    1.677258     14.860146   \n",
       "2      103.015625   39.341649    0.323328  1.051164    3.121237     21.744669   \n",
       "3      136.750000   57.178449   -0.068415 -0.636238    3.642977     20.959280   \n",
       "4       88.726562   40.672225    0.600866  1.123492    1.178930     11.468720   \n",
       "...           ...         ...         ...       ...         ...           ...   \n",
       "17893  136.429688   59.847421   -0.187846 -0.738123    1.296823     12.166062   \n",
       "17894  122.554688   49.485605    0.127978  0.323061   16.409699     44.626893   \n",
       "17895  119.335938   59.935939    0.159363 -0.743025   21.430602     58.872000   \n",
       "17896  114.507812   53.902400    0.201161 -0.024789    1.946488     13.381731   \n",
       "17897   57.062500   85.797340    1.406391  0.089520  188.306020     64.712562   \n",
       "\n",
       "       excess_dmsnr  skew_dmsnr  class  \n",
       "0          7.975532   74.242225      0  \n",
       "1         10.576487  127.393580      0  \n",
       "2          7.735822   63.171909      0  \n",
       "3          6.896499   53.593661      0  \n",
       "4         14.269573  252.567306      0  \n",
       "...             ...         ...    ...  \n",
       "17893     15.450260  285.931022      0  \n",
       "17894      2.945244    8.297092      0  \n",
       "17895      2.499517    4.595173      0  \n",
       "17896     10.007967  134.238910      0  \n",
       "17897     -1.597527    1.429475      0  \n",
       "\n",
       "[17898 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_int</th>\n      <th>stddev_int</th>\n      <th>excess_int</th>\n      <th>skew_int</th>\n      <th>mean_dmsnr</th>\n      <th>stddev_dmsnr</th>\n      <th>excess_dmsnr</th>\n      <th>skew_dmsnr</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>140.562500</td>\n      <td>55.683782</td>\n      <td>-0.234571</td>\n      <td>-0.699648</td>\n      <td>3.199833</td>\n      <td>19.110426</td>\n      <td>7.975532</td>\n      <td>74.242225</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>102.507812</td>\n      <td>58.882430</td>\n      <td>0.465318</td>\n      <td>-0.515088</td>\n      <td>1.677258</td>\n      <td>14.860146</td>\n      <td>10.576487</td>\n      <td>127.393580</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>103.015625</td>\n      <td>39.341649</td>\n      <td>0.323328</td>\n      <td>1.051164</td>\n      <td>3.121237</td>\n      <td>21.744669</td>\n      <td>7.735822</td>\n      <td>63.171909</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>136.750000</td>\n      <td>57.178449</td>\n      <td>-0.068415</td>\n      <td>-0.636238</td>\n      <td>3.642977</td>\n      <td>20.959280</td>\n      <td>6.896499</td>\n      <td>53.593661</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>88.726562</td>\n      <td>40.672225</td>\n      <td>0.600866</td>\n      <td>1.123492</td>\n      <td>1.178930</td>\n      <td>11.468720</td>\n      <td>14.269573</td>\n      <td>252.567306</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17893</th>\n      <td>136.429688</td>\n      <td>59.847421</td>\n      <td>-0.187846</td>\n      <td>-0.738123</td>\n      <td>1.296823</td>\n      <td>12.166062</td>\n      <td>15.450260</td>\n      <td>285.931022</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17894</th>\n      <td>122.554688</td>\n      <td>49.485605</td>\n      <td>0.127978</td>\n      <td>0.323061</td>\n      <td>16.409699</td>\n      <td>44.626893</td>\n      <td>2.945244</td>\n      <td>8.297092</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17895</th>\n      <td>119.335938</td>\n      <td>59.935939</td>\n      <td>0.159363</td>\n      <td>-0.743025</td>\n      <td>21.430602</td>\n      <td>58.872000</td>\n      <td>2.499517</td>\n      <td>4.595173</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17896</th>\n      <td>114.507812</td>\n      <td>53.902400</td>\n      <td>0.201161</td>\n      <td>-0.024789</td>\n      <td>1.946488</td>\n      <td>13.381731</td>\n      <td>10.007967</td>\n      <td>134.238910</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17897</th>\n      <td>57.062500</td>\n      <td>85.797340</td>\n      <td>1.406391</td>\n      <td>0.089520</td>\n      <td>188.306020</td>\n      <td>64.712562</td>\n      <td>-1.597527</td>\n      <td>1.429475</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17898 rows Ã— 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# load 'Electrical Grid Stability' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/HTRU2/HTRU_2.csv\")\n",
    "\n",
    "# clean data\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   35.8s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.1s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   37.8s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   37.3s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.0s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   36.2s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.1s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   35.4s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# pre-declared values/arrays/functions to be used once inside the trial loop\n",
    "# C values for logistic regression regularization in range of 10(-8) to 10(4)\n",
    "Cvals = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# K values for k-nearest neighbors in range of 1 to 105 in steps of 4\n",
    "Kvals = np.linspace(1, 105, num=26, dtype=int).tolist()\n",
    "# max feature values for random forest similar to CNM06\n",
    "max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "# max depth values for decision trees (shallower = better)\n",
    "max_depths = np.linspace(1, 5, num=5, dtype=int).tolist()\n",
    "# array of performance metrics\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "# build parameter grids to be passed into GridSearchCV\n",
    "logreg_pgrid = {'classifier__penalty': ['l1','l2'], 'classifier__C': Cvals, 'classifier__max_iter': [5000]}\n",
    "knn_pgrid = {'classifier__weights': ['distance'], 'classifier__n_neighbors': Kvals}\n",
    "rforest_pgrid = {'classifier__n_estimators': [1024], 'classifier__max_features': max_features}\n",
    "dtree_pgrid = {'classifier__max_depth': max_depths}\n",
    "\n",
    "# arrays + dictionaries to store scores\n",
    "score_dict = [{}, {}, {}, {}, {}]\n",
    "\n",
    "# loop through this entire trial FIVE (5) times\n",
    "for i in range(5):\n",
    "    # slice the dataframe to not include the binary classifier (label)\n",
    "    # last column is the label (income>50K)\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "    # randomly pick 5000 samples with replacement for training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "    # make pipeline for each algorithms to condense model call\n",
    "    logreg = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', LogisticRegression(n_jobs=-1))])\n",
    "    knn = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "    rforest = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "    dtree = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    # 5-fold cross validation using Stratified KFold\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "    # GridSearchCV classifier for each algorithm\n",
    "    logreg_clf = GridSearchCV(estimator=logreg, param_grid=logreg_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    knn_clf = GridSearchCV(estimator=knn, param_grid=knn_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    rforest_clf = GridSearchCV(estimator=rforest, param_grid=rforest_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    dtree_clf = GridSearchCV(estimator=dtree, param_grid=dtree_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "\n",
    "\n",
    "\n",
    "    # for each classifier\n",
    "    for clf, clf_name in zip([logreg_clf, knn_clf, rforest_clf, dtree_clf], \n",
    "                ['LogReg', 'KNN', 'Ran_For', 'Dec_Tree']):\n",
    "        # fit to training data of 5000 samples\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # get parameters for each scoring metric's best\n",
    "        best_acc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_accuracy']) ]\n",
    "        best_f1_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_f1_micro']) ]\n",
    "        best_roc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_roc_auc_ovr']) ]\n",
    "\n",
    "        # get pipeline based on current classifier\n",
    "        if (clf_name == 'LogReg'):\n",
    "            pipe = logreg\n",
    "        elif (clf_name == 'KNN'):\n",
    "            pipe = knn\n",
    "        elif (clf_name == 'Ran_For'):\n",
    "            pipe = rforest\n",
    "        elif (clf_name == 'Dec_Tree'):\n",
    "            pipe = dtree\n",
    "\n",
    "        # set pipeline parameters to the parameters for best accuracy\n",
    "        pipe.set_params(**best_acc_param)\n",
    "        # fit classifier with training data and new parameters for scoring metric\n",
    "        pipe.fit(X_train, y_train)\n",
    "        # get predictions for both training and testing data\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        # get scores for all metrics from both training and testing data\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        # store all scores into a dictionary for accuracy metric\n",
    "        acc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        \n",
    "        # do ^^^^^ all that for f1 score\n",
    "        pipe.set_params(**best_f1_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        f1_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "\n",
    "        # do ^^^^^ all that for roc_auc score\n",
    "        pipe.set_params(**best_roc_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        roc_auc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        # build final dictionary to store all scores from all three models and their best parameters\n",
    "        score_dict[i][clf_name] = {'acc_dict': acc_dict, 'f1_dict': f1_dict, 'roc_auc_dict': roc_auc_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'LogReg': {'acc_dict': {'acc_train': 0.9826, 'f1_train': 0.8937728937728937, 'roc_auc_train': 0.9195783904677127, 'acc_test': 0.9776709567374787, 'f1_test': 0.8726790450928382, 'roc_auc_test': 0.9065503246436406}, 'f1_dict': {'acc_train': 0.9826, 'f1_train': 0.8937728937728937, 'roc_auc_train': 0.9195783904677127, 'acc_test': 0.9776709567374787, 'f1_test': 0.8726790450928382, 'roc_auc_test': 0.9065503246436406}, 'roc_auc_dict': {'acc_train': 0.9816, 'f1_train': 0.8869778869778869, 'roc_auc_train': 0.9138180218041181, 'acc_test': 0.9766630485346566, 'f1_test': 0.8658047258136424, 'roc_auc_test': 0.9000396023723098}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9761203287331369, 'f1_test': 0.8628673196794302, 'roc_auc_test': 0.89899592372356}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9761203287331369, 'f1_test': 0.8628673196794302, 'roc_auc_test': 0.89899592372356}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9730966041246705, 'f1_test': 0.8370126820103334, 'roc_auc_test': 0.8682984429099512}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9775934253372616, 'f1_test': 0.8737439930100481, 'roc_auc_test': 0.9113458667507922}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9776709567374787, 'f1_test': 0.8743455497382199, 'roc_auc_test': 0.9121329816434489}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977050705535742, 'f1_test': 0.870967741935484, 'roc_auc_test': 0.9106743652353627}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9826, 'f1_train': 0.8945454545454546, 'roc_auc_train': 0.9227060965541742, 'acc_test': 0.9755776089316173, 'f1_test': 0.8617814831066257, 'roc_auc_test': 0.9035349020746178}, 'f1_dict': {'acc_train': 0.9826, 'f1_train': 0.8945454545454546, 'roc_auc_train': 0.9227060965541742, 'acc_test': 0.9755776089316173, 'f1_test': 0.8617814831066257, 'roc_auc_test': 0.9035349020746178}, 'roc_auc_dict': {'acc_train': 0.9854, 'f1_train': 0.9144196951934349, 'roc_auc_train': 0.946133109680649, 'acc_test': 0.9768181113350907, 'f1_test': 0.8735729386892176, 'roc_auc_test': 0.9232001058902142}}}, {'LogReg': {'acc_dict': {'acc_train': 0.978, 'f1_train': 0.86810551558753, 'roc_auc_train': 0.9013816532764249, 'acc_test': 0.9765079857342224, 'f1_test': 0.8624602814344076, 'roc_auc_test': 0.8961765288294078}, 'f1_dict': {'acc_train': 0.978, 'f1_train': 0.86810551558753, 'roc_auc_train': 0.9013816532764249, 'acc_test': 0.9765079857342224, 'f1_test': 0.8624602814344076, 'roc_auc_test': 0.8961765288294078}, 'roc_auc_dict': {'acc_train': 0.978, 'f1_train': 0.86810551558753, 'roc_auc_train': 0.9013816532764249, 'acc_test': 0.9765079857342224, 'f1_test': 0.8624602814344076, 'roc_auc_test': 0.8961765288294078}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977050705535742, 'f1_test': 0.8677390527256479, 'roc_auc_test': 0.9043947168755271}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.977050705535742, 'f1_test': 0.8677390527256479, 'roc_auc_test': 0.9043947168755271}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9737168553264072, 'f1_test': 0.8388017118402282, 'roc_auc_test': 0.8689957933841818}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9783687393394325, 'f1_test': 0.8767123287671234, 'roc_auc_test': 0.9130399998221334}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9784462707396495, 'f1_test': 0.8768821966341896, 'roc_auc_test': 0.912328497729727}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9784462707396495, 'f1_test': 0.8768821966341896, 'roc_auc_test': 0.912328497729727}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9842, 'f1_train': 0.9092996555683123, 'roc_auc_train': 0.9389985563645493, 'acc_test': 0.97790355093813, 'f1_test': 0.8748353096179183, 'roc_auc_test': 0.9142921660467515}, 'f1_dict': {'acc_train': 0.9842, 'f1_train': 0.9092996555683123, 'roc_auc_train': 0.9389985563645493, 'acc_test': 0.9780586137385641, 'f1_test': 0.8758227292672224, 'roc_auc_test': 0.915131796609304}, 'roc_auc_dict': {'acc_train': 0.9842, 'f1_train': 0.9092996555683123, 'roc_auc_train': 0.9389985563645493, 'acc_test': 0.9780586137385641, 'f1_test': 0.8758227292672224, 'roc_auc_test': 0.915131796609304}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9808, 'f1_train': 0.8846153846153847, 'roc_auc_train': 0.9122195066005425, 'acc_test': 0.9794541789424717, 'f1_test': 0.8833113166006165, 'roc_auc_test': 0.9165464135541543}, 'f1_dict': {'acc_train': 0.9808, 'f1_train': 0.8846153846153847, 'roc_auc_train': 0.9122195066005425, 'acc_test': 0.9794541789424717, 'f1_test': 0.8833113166006165, 'roc_auc_test': 0.9165464135541543}, 'roc_auc_dict': {'acc_train': 0.9802, 'f1_train': 0.88, 'roc_auc_train': 0.906808366751299, 'acc_test': 0.9789114591409521, 'f1_test': 0.8793256433007985, 'roc_auc_test': 0.9117391134912658}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9774383625368274, 'f1_test': 0.8695652173913043, 'roc_auc_test': 0.9030379507882863}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9774383625368274, 'f1_test': 0.8695652173913043, 'roc_auc_test': 0.9030379507882863}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9750348891300977, 'f1_test': 0.850371747211896, 'roc_auc_test': 0.8810507765952084}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9799193673437743, 'f1_test': 0.8874402433724469, 'roc_auc_test': 0.9235651052532037}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8841463414634145, 'roc_auc_test': 0.9210119209143169}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.883781439722463, 'roc_auc_test': 0.9224292165546365}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9786, 'f1_train': 0.8724672228843862, 'roc_auc_train': 0.9089795458320481, 'acc_test': 0.9765079857342224, 'f1_test': 0.8688879273041975, 'roc_auc_test': 0.9152985841702069}, 'f1_dict': {'acc_train': 0.9786, 'f1_train': 0.8724672228843862, 'roc_auc_train': 0.9089795458320481, 'acc_test': 0.9765079857342224, 'f1_test': 0.8688879273041975, 'roc_auc_test': 0.9152985841702069}, 'roc_auc_dict': {'acc_train': 0.981, 'f1_train': 0.8842874543239951, 'roc_auc_train': 0.9072473483140735, 'acc_test': 0.9784462707396495, 'f1_test': 0.8767730496453902, 'roc_auc_test': 0.910731397056221}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9802, 'f1_train': 0.8805790108564535, 'roc_auc_train': 0.9080267393529767, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9144710365452233}, 'f1_dict': {'acc_train': 0.9802, 'f1_train': 0.8805790108564535, 'roc_auc_train': 0.9080267393529767, 'acc_test': 0.9792215847418204, 'f1_test': 0.8815207780725022, 'roc_auc_test': 0.9144710365452233}, 'roc_auc_dict': {'acc_train': 0.9786, 'f1_train': 0.8703030303030304, 'roc_auc_train': 0.901065627351102, 'acc_test': 0.977981082338347, 'f1_test': 0.8726457399103138, 'roc_auc_test': 0.9047625487886}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9775158939370445, 'f1_test': 0.8687782805429863, 'roc_auc_test': 0.8996177057536485}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9775158939370445, 'f1_test': 0.8687782805429863, 'roc_auc_test': 0.8996177057536485}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9751899519305318, 'f1_test': 0.8507462686567163, 'roc_auc_test': 0.8802861711153168}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792991161420376, 'f1_test': 0.8828433523475209, 'roc_auc_test': 0.9178981172666705}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9796092417429059, 'f1_test': 0.8849015317286653, 'roc_auc_test': 0.919949199174272}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9799193673437743, 'f1_test': 0.8871459694989107, 'roc_auc_test': 0.9227523611449823}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9832, 'f1_train': 0.8992805755395683, 'roc_auc_train': 0.9198115418295736, 'acc_test': 0.97790355093813, 'f1_test': 0.8750548005260851, 'roc_auc_test': 0.914120829140004}, 'f1_dict': {'acc_train': 0.9832, 'f1_train': 0.8992805755395683, 'roc_auc_train': 0.9198115418295736, 'acc_test': 0.9780586137385641, 'f1_test': 0.8759316089434459, 'roc_auc_test': 0.9145823100464728}, 'roc_auc_dict': {'acc_train': 0.9832, 'f1_train': 0.8992805755395683, 'roc_auc_train': 0.9198115418295736, 'acc_test': 0.97790355093813, 'f1_test': 0.8750548005260851, 'roc_auc_test': 0.914120829140004}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9776, 'f1_train': 0.8774617067833698, 'roc_auc_train': 0.9125443821762284, 'acc_test': 0.97790355093813, 'f1_test': 0.8667601683029453, 'roc_auc_test': 0.898262793392125}, 'f1_dict': {'acc_train': 0.9776, 'f1_train': 0.8774617067833698, 'roc_auc_train': 0.9125443821762284, 'acc_test': 0.97790355093813, 'f1_test': 0.8667601683029453, 'roc_auc_test': 0.898262793392125}, 'roc_auc_dict': {'acc_train': 0.9776, 'f1_train': 0.8774617067833698, 'roc_auc_train': 0.9125443821762284, 'acc_test': 0.97790355093813, 'f1_test': 0.8667601683029453, 'roc_auc_test': 0.898262793392125}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9784462707396495, 'f1_test': 0.8709377901578459, 'roc_auc_test': 0.9028461234434922}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9784462707396495, 'f1_test': 0.8709377901578459, 'roc_auc_test': 0.9028461234434922}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9742595751279268, 'f1_test': 0.8374142997061703, 'roc_auc_test': 0.8682124861890573}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.8761552680221811, 'roc_auc_test': 0.907167644618946}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9793766475422546, 'f1_test': 0.8770794824399261, 'roc_auc_test': 0.9076423825468996}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9792215847418204, 'f1_test': 0.8789521228545619, 'roc_auc_test': 0.9169068022973812}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9778, 'f1_train': 0.875977653631285, 'roc_auc_train': 0.9043149669647826, 'acc_test': 0.9760427973329199, 'f1_test': 0.8519405845711548, 'roc_auc_test': 0.8824372142711124}, 'f1_dict': {'acc_train': 0.9778, 'f1_train': 0.875977653631285, 'roc_auc_train': 0.9043149669647826, 'acc_test': 0.9760427973329199, 'f1_test': 0.8519405845711548, 'roc_auc_test': 0.8824372142711124}, 'roc_auc_dict': {'acc_train': 0.982, 'f1_train': 0.9042553191489362, 'roc_auc_train': 0.9372193108616709, 'acc_test': 0.9780586137385641, 'f1_test': 0.8714220808723308, 'roc_auc_test': 0.9108140868413379}}}]\n"
     ]
    }
   ],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}