{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9032458e503ab28519db53568226f597adad35e1b11ccc360aee2243f83ff687"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessarily libraries for the binary classification task\n",
    "\n",
    "# libraries imported for data processing and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# libraries imported for learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import pipeline\n",
    "\n",
    "# libraries imported for performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0     2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1     9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2     8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3     0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4     3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  2.930406  9.487627  2.376523  6.187797  3.343416 -0.658054 -1.449106   \n",
       "9996  3.392299  1.274827  2.954947  6.894759  4.349512 -1.663661 -0.952437   \n",
       "9997  2.364034  2.842030  8.776391  1.008906  4.299976 -1.380719 -0.943884   \n",
       "9998  9.631511  3.994398  2.757071  7.821347  2.514755 -0.966330 -0.649915   \n",
       "9999  6.530527  6.781790  4.349695  8.673138  3.492807 -1.390285 -1.532193   \n",
       "\n",
       "            p4        g1        g2        g3        g4      stab  stabf  \n",
       "0    -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347      0  \n",
       "1    -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957      1  \n",
       "2    -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471      0  \n",
       "3    -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871      0  \n",
       "4    -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "9995 -1.236256  0.601709  0.779642  0.813512  0.608385  0.023892      0  \n",
       "9996 -1.733414  0.502079  0.567242  0.285880  0.366120 -0.025803      1  \n",
       "9997 -1.975373  0.487838  0.986505  0.149286  0.145984 -0.031810      1  \n",
       "9998 -0.898510  0.365246  0.587558  0.889118  0.818391  0.037789      0  \n",
       "9999 -0.570329  0.073056  0.505441  0.378761  0.942631  0.045263      0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tau1</th>\n      <th>tau2</th>\n      <th>tau3</th>\n      <th>tau4</th>\n      <th>p1</th>\n      <th>p2</th>\n      <th>p3</th>\n      <th>p4</th>\n      <th>g1</th>\n      <th>g2</th>\n      <th>g3</th>\n      <th>g4</th>\n      <th>stab</th>\n      <th>stabf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.959060</td>\n      <td>3.079885</td>\n      <td>8.381025</td>\n      <td>9.780754</td>\n      <td>3.763085</td>\n      <td>-0.782604</td>\n      <td>-1.257395</td>\n      <td>-1.723086</td>\n      <td>0.650456</td>\n      <td>0.859578</td>\n      <td>0.887445</td>\n      <td>0.958034</td>\n      <td>0.055347</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.304097</td>\n      <td>4.902524</td>\n      <td>3.047541</td>\n      <td>1.369357</td>\n      <td>5.067812</td>\n      <td>-1.940058</td>\n      <td>-1.872742</td>\n      <td>-1.255012</td>\n      <td>0.413441</td>\n      <td>0.862414</td>\n      <td>0.562139</td>\n      <td>0.781760</td>\n      <td>-0.005957</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.971707</td>\n      <td>8.848428</td>\n      <td>3.046479</td>\n      <td>1.214518</td>\n      <td>3.405158</td>\n      <td>-1.207456</td>\n      <td>-1.277210</td>\n      <td>-0.920492</td>\n      <td>0.163041</td>\n      <td>0.766689</td>\n      <td>0.839444</td>\n      <td>0.109853</td>\n      <td>0.003471</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.716415</td>\n      <td>7.669600</td>\n      <td>4.486641</td>\n      <td>2.340563</td>\n      <td>3.963791</td>\n      <td>-1.027473</td>\n      <td>-1.938944</td>\n      <td>-0.997374</td>\n      <td>0.446209</td>\n      <td>0.976744</td>\n      <td>0.929381</td>\n      <td>0.362718</td>\n      <td>0.028871</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.134112</td>\n      <td>7.608772</td>\n      <td>4.943759</td>\n      <td>9.857573</td>\n      <td>3.525811</td>\n      <td>-1.125531</td>\n      <td>-1.845975</td>\n      <td>-0.554305</td>\n      <td>0.797110</td>\n      <td>0.455450</td>\n      <td>0.656947</td>\n      <td>0.820923</td>\n      <td>0.049860</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>2.930406</td>\n      <td>9.487627</td>\n      <td>2.376523</td>\n      <td>6.187797</td>\n      <td>3.343416</td>\n      <td>-0.658054</td>\n      <td>-1.449106</td>\n      <td>-1.236256</td>\n      <td>0.601709</td>\n      <td>0.779642</td>\n      <td>0.813512</td>\n      <td>0.608385</td>\n      <td>0.023892</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>3.392299</td>\n      <td>1.274827</td>\n      <td>2.954947</td>\n      <td>6.894759</td>\n      <td>4.349512</td>\n      <td>-1.663661</td>\n      <td>-0.952437</td>\n      <td>-1.733414</td>\n      <td>0.502079</td>\n      <td>0.567242</td>\n      <td>0.285880</td>\n      <td>0.366120</td>\n      <td>-0.025803</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>2.364034</td>\n      <td>2.842030</td>\n      <td>8.776391</td>\n      <td>1.008906</td>\n      <td>4.299976</td>\n      <td>-1.380719</td>\n      <td>-0.943884</td>\n      <td>-1.975373</td>\n      <td>0.487838</td>\n      <td>0.986505</td>\n      <td>0.149286</td>\n      <td>0.145984</td>\n      <td>-0.031810</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>9.631511</td>\n      <td>3.994398</td>\n      <td>2.757071</td>\n      <td>7.821347</td>\n      <td>2.514755</td>\n      <td>-0.966330</td>\n      <td>-0.649915</td>\n      <td>-0.898510</td>\n      <td>0.365246</td>\n      <td>0.587558</td>\n      <td>0.889118</td>\n      <td>0.818391</td>\n      <td>0.037789</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>6.530527</td>\n      <td>6.781790</td>\n      <td>4.349695</td>\n      <td>8.673138</td>\n      <td>3.492807</td>\n      <td>-1.390285</td>\n      <td>-1.532193</td>\n      <td>-0.570329</td>\n      <td>0.073056</td>\n      <td>0.505441</td>\n      <td>0.378761</td>\n      <td>0.942631</td>\n      <td>0.045263</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# load 'Electrical Grid Stability' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/Grid_Stability/grid_stability.csv\")\n",
    "\n",
    "# clean data\n",
    "# replace string label classifiers into binary values\n",
    "df = df.replace(to_replace=\"stable\", value=1)\n",
    "df = df.replace(to_replace=\"unstable\", value=0)\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    9.8s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.7s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    9.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   23.6s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    9.6s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   23.4s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    9.6s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   24.4s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    9.9s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   23.9s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# pre-declared values/arrays/functions to be used once inside the trial loop\n",
    "# C values for logistic regression regularization in range of 10(-8) to 10(4)\n",
    "Cvals = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# K values for k-nearest neighbors in range of 1 to 105 in steps of 4\n",
    "Kvals = np.linspace(1, 105, num=26, dtype=int).tolist()\n",
    "# max feature values for random forest similar to CNM06\n",
    "max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "# max depth values for decision trees (shallower = better)\n",
    "max_depths = np.linspace(1, 5, num=5, dtype=int).tolist()\n",
    "# array of performance metrics\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "# build parameter grids to be passed into GridSearchCV\n",
    "logreg_pgrid = {'classifier__penalty': ['l1','l2'], 'classifier__C': Cvals, 'classifier__max_iter': [5000]}\n",
    "knn_pgrid = {'classifier__weights': ['distance'], 'classifier__n_neighbors': Kvals}\n",
    "rforest_pgrid = {'classifier__n_estimators': [1024], 'classifier__max_features': max_features}\n",
    "dtree_pgrid = {'classifier__max_depth': max_depths}\n",
    "\n",
    "# arrays + dictionaries to store scores\n",
    "score_dict = [{}, {}, {}, {}, {}]\n",
    "\n",
    "# loop through this entire trial FIVE (5) times\n",
    "for i in range(5):\n",
    "    # slice the dataframe to not include the binary classifier (label)\n",
    "    # last column is the label (income>50K)\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "    # randomly pick 5000 samples with replacement for training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "    # make pipeline for each algorithms to condense model call\n",
    "    logreg = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', LogisticRegression(n_jobs=-1))])\n",
    "    knn = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "    rforest = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "    dtree = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    # 5-fold cross validation using Stratified KFold\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "    # GridSearchCV classifier for each algorithm\n",
    "    logreg_clf = GridSearchCV(estimator=logreg, param_grid=logreg_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    knn_clf = GridSearchCV(estimator=knn, param_grid=knn_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    rforest_clf = GridSearchCV(estimator=rforest, param_grid=rforest_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    dtree_clf = GridSearchCV(estimator=dtree, param_grid=dtree_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "\n",
    "\n",
    "\n",
    "    # for each classifier\n",
    "    for clf, clf_name in zip([logreg_clf, knn_clf, rforest_clf, dtree_clf], \n",
    "                ['LogReg', 'KNN', 'Ran_For', 'Dec_Tree']):\n",
    "        # fit to training data of 5000 samples\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # get parameters for each scoring metric's best\n",
    "        best_acc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_accuracy']) ]\n",
    "        best_f1_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_f1_micro']) ]\n",
    "        best_roc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_roc_auc_ovr']) ]\n",
    "\n",
    "        # get pipeline based on current classifier\n",
    "        if (clf_name == 'LogReg'):\n",
    "            pipe = logreg\n",
    "        elif (clf_name == 'KNN'):\n",
    "            pipe = knn\n",
    "        elif (clf_name == 'Ran_For'):\n",
    "            pipe = rforest\n",
    "        elif (clf_name == 'Dec_Tree'):\n",
    "            pipe = dtree\n",
    "\n",
    "        # set pipeline parameters to the parameters for best accuracy\n",
    "        pipe.set_params(**best_acc_param)\n",
    "        # fit classifier with training data and new parameters for scoring metric\n",
    "        pipe.fit(X_train, y_train)\n",
    "        # get predictions for both training and testing data\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        # get scores for all metrics from both training and testing data\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        # store all scores into a dictionary for accuracy metric\n",
    "        acc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        \n",
    "        # do ^^^^^ all that for f1 score\n",
    "        pipe.set_params(**best_f1_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        f1_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "\n",
    "        # do ^^^^^ all that for roc_auc score\n",
    "        pipe.set_params(**best_roc_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        roc_auc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        # build final dictionary to store all scores from all three models and their best parameters\n",
    "        score_dict[i][clf_name] = {'acc_dict': acc_dict, 'f1_dict': f1_dict, 'roc_auc_dict': roc_auc_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'LogReg': {'acc_dict': {'acc_train': 0.9998, 'f1_train': 0.9997232216994187, 'roc_auc_train': 0.9997232982844494, 'acc_test': 0.9994, 'f1_test': 0.9991733259851198, 'roc_auc_test': 0.9995293379353625}, 'f1_dict': {'acc_train': 0.9998, 'f1_train': 0.9997232216994187, 'roc_auc_train': 0.9997232982844494, 'acc_test': 0.9994, 'f1_test': 0.9991733259851198, 'roc_auc_test': 0.9995293379353625}, 'roc_auc_dict': {'acc_train': 0.9998, 'f1_train': 0.9997232216994187, 'roc_auc_train': 0.9997232982844494, 'acc_test': 0.9994, 'f1_test': 0.9991733259851198, 'roc_auc_test': 0.9995293379353625}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9472, 'f1_test': 0.9227166276346604, 'roc_auc_test': 0.9304027617712677}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9472, 'f1_test': 0.9227166276346604, 'roc_auc_test': 0.9304027617712677}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.943, 'f1_test': 0.9159043965771613, 'roc_auc_test': 0.9242545600741845}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}}, 'Dec_Tree': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}}}, {'LogReg': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9992, 'f1_test': 0.9989065062875888, 'roc_auc_test': 0.9991378952125425}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9992, 'f1_test': 0.9989065062875888, 'roc_auc_test': 0.9991378952125425}, 'roc_auc_dict': {'acc_train': 0.9998, 'f1_train': 0.9997209042701646, 'roc_auc_train': 0.9998441882206295, 'acc_test': 0.9992, 'f1_test': 0.9989071038251366, 'roc_auc_test': 0.9992535896750192}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9448, 'f1_test': 0.9203233256351039, 'roc_auc_test': 0.9292924068051793}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9448, 'f1_test': 0.9203233256351039, 'roc_auc_test': 0.9292924068051793}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.943, 'f1_test': 0.9165446559297217, 'roc_auc_test': 0.9245181567027182}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997267012845039, 'roc_auc_test': 0.9998423210343741}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997267012845039, 'roc_auc_test': 0.9998423210343741}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997267012845039, 'roc_auc_test': 0.9998423210343741}}, 'Dec_Tree': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9992, 'f1_test': 0.9989077007099946, 'roc_auc_test': 0.999369284137496}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9992, 'f1_test': 0.9989077007099946, 'roc_auc_test': 0.999369284137496}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9992, 'f1_test': 0.9989077007099946, 'roc_auc_test': 0.999369284137496}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9994, 'f1_train': 0.9991769547325103, 'roc_auc_train': 0.9994109129795333, 'acc_test': 0.9988, 'f1_test': 0.9983314794215795, 'roc_auc_test': 0.9986972824965488}, 'f1_dict': {'acc_train': 0.9994, 'f1_train': 0.9991769547325103, 'roc_auc_train': 0.9994109129795333, 'acc_test': 0.9988, 'f1_test': 0.9983314794215795, 'roc_auc_test': 0.9986972824965488}, 'roc_auc_dict': {'acc_train': 0.9994, 'f1_train': 0.9991769547325103, 'roc_auc_train': 0.9994109129795333, 'acc_test': 0.9988, 'f1_test': 0.9983314794215795, 'roc_auc_test': 0.9986972824965488}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9474, 'f1_test': 0.9228512760340276, 'roc_auc_test': 0.9314966869288451}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9474, 'f1_test': 0.9228512760340276, 'roc_auc_test': 0.9314966869288451}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9392, 'f1_test': 0.9089275014979029, 'roc_auc_test': 0.9182661142681262}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}}, 'Dec_Tree': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 1.0, 'f1_test': 1.0, 'roc_auc_test': 1.0}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9998, 'f1_train': 0.9997268505872712, 'roc_auc_train': 0.9997269251774986, 'acc_test': 0.9986, 'f1_test': 0.9980441464096117, 'roc_auc_test': 0.9985386838388651}, 'f1_dict': {'acc_train': 0.9998, 'f1_train': 0.9997268505872712, 'roc_auc_train': 0.9997269251774986, 'acc_test': 0.9986, 'f1_test': 0.9980441464096117, 'roc_auc_test': 0.9985386838388651}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9984, 'f1_test': 0.9977641140301845, 'roc_auc_test': 0.9982591980926382}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9512, 'f1_test': 0.9284876905041032, 'roc_auc_test': 0.9366325475295496}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9512, 'f1_test': 0.9284876905041032, 'roc_auc_test': 0.9366325475295496}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9424, 'f1_test': 0.9139784946236558, 'roc_auc_test': 0.92309746453943}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997204361196533, 'roc_auc_test': 0.9997205142537731}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997204361196533, 'roc_auc_test': 0.9997205142537731}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997204361196533, 'roc_auc_test': 0.9997205142537731}}, 'Dec_Tree': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9996, 'f1_test': 0.9994407158836689, 'roc_auc_test': 0.9994410285075461}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9996, 'f1_test': 0.9994407158836689, 'roc_auc_test': 0.9994410285075461}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9996, 'f1_test': 0.9994407158836689, 'roc_auc_test': 0.9994410285075461}}}, {'LogReg': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9994, 'f1_test': 0.999173781327458, 'roc_auc_test': 0.9995291902071562}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9994, 'f1_test': 0.999173781327458, 'roc_auc_test': 0.9995291902071562}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9994, 'f1_test': 0.999173781327458, 'roc_auc_test': 0.9995291902071562}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.943, 'f1_test': 0.9165446559297219, 'roc_auc_test': 0.9257174269180697}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.943, 'f1_test': 0.9165446559297219, 'roc_auc_test': 0.9257174269180697}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9398, 'f1_test': 0.9102831594634874, 'roc_auc_test': 0.9186959416576519}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997242900468706, 'roc_auc_test': 0.9997243660418964}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997242900468706, 'roc_auc_test': 0.9997243660418964}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997242900468706, 'roc_auc_test': 0.9997243660418964}}, 'Dec_Tree': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997242900468706, 'roc_auc_test': 0.9997243660418964}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997242900468706, 'roc_auc_test': 0.9997243660418964}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9998, 'f1_test': 0.9997242900468706, 'roc_auc_test': 0.9997243660418964}}}]\n"
     ]
    }
   ],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}