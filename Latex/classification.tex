\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Comparison of Classification Algorithms}{Kim}
\firstpageno{1}

\begin{document}

\title{Supervised Learning Algorithms: Classification Comparison}

\author{\name Howard\ Kim \email hokim@ucsd.edu \\
       \addr Department of Cognitive Science\\
       University of California, San Diego\\
       La Jolla, CA 92093-5004, USA}
       
\editor{Howard Kim}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper describes the replication of a large-scale empirical 
comparison between supervised machine learning algorithms of \citet{cnm06} 
in a smaller scale.  For this particular comparison, four different 
supervised machine learning algorithms are compared and contrasted by their 
performance on binary classification tasks across four different data sets 
with five trials each.  This process boils down to total of eighty different 
trials across the following algorithms: logistic regression, k-nearest 
neighbors, decision trees, and random forests.  Each trial will be tuned by 
five-fold cross-validation on randomly chosen five-thousand data samples to 
select hyperparameters through a systematic grid searches. The results  
seem to be in parallel to the findings observed by \citet{cnm06} with 
performance measured through ACC, F1 score, and AUC.
\end{abstract}

\begin{keywords}
  binary classification, logistic regression, k-nearest neighbors, decision 
  trees, random forests, hyperparameters, cross-validation
  \\
\end{keywords}

\section{Introduction}

The precedent of this paper is the widely-known empirical comparison of 
supervised learning algorithms done by \cite{cnm:06}, hereafter referred 
to as the CNM06. Much like this paper, the CNM06 also had a precedent 

Much like the precedent to comparison and analysis done by \cite{cnm:06}, 
hereafter referred to as CNM06, 

% Probabilistic inference has become a core technology in AI,
% largely due to developments in graph-theoretic methods for the 
% representation and manipulation of complex probability 
% distributions~\citep{pearl:88}.  Whether in their guise as 
% directed graphs (Bayesian networks) or as undirected graphs (Markov 
% random fields), \emph{probabilistic graphical models} have a number 
% of virtues as representations of uncertainty and as inference engines.  
% Graphical models allow a separation between qualitative, structural
% aspects of uncertain knowledge and the quantitative, parametric aspects 
% of uncertainty...\\

%{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

% Acknowledgements should go at the end, before appendices and references

\acks{We would like to acknowledge support for this project
from the National Science Foundation (NSF grant IIS-9988642)
and the Multidisciplinary Research Program of the Department
of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{sample}

\end{document}