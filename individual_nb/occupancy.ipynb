{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9032458e503ab28519db53568226f597adad35e1b11ccc360aee2243f83ff687"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessarily libraries for the binary classification task\n",
    "\n",
    "# libraries imported for data processing and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# libraries imported for learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import pipeline\n",
    "\n",
    "# libraries imported for performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Temperature  Humidity       Light         CO2  HumidityRatio  Occupancy\n",
       "0          23.7000   26.2720  585.200000  749.200000       0.004764          1\n",
       "1          23.7180   26.2900  578.400000  760.400000       0.004773          1\n",
       "2          23.7300   26.2300  572.666667  769.666667       0.004765          1\n",
       "3          23.7225   26.1250  493.750000  774.750000       0.004744          1\n",
       "4          23.7540   26.2000  488.600000  779.000000       0.004767          1\n",
       "...            ...       ...         ...         ...            ...        ...\n",
       "20555      21.0500   36.0975  433.000000  787.250000       0.005579          1\n",
       "20556      21.0500   35.9950  433.000000  789.500000       0.005563          1\n",
       "20557      21.1000   36.0950  433.000000  798.500000       0.005596          1\n",
       "20558      21.1000   36.2600  433.000000  820.333333       0.005621          1\n",
       "20559      21.1000   36.2000  447.000000  821.000000       0.005612          1\n",
       "\n",
       "[20560 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Temperature</th>\n      <th>Humidity</th>\n      <th>Light</th>\n      <th>CO2</th>\n      <th>HumidityRatio</th>\n      <th>Occupancy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23.7000</td>\n      <td>26.2720</td>\n      <td>585.200000</td>\n      <td>749.200000</td>\n      <td>0.004764</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23.7180</td>\n      <td>26.2900</td>\n      <td>578.400000</td>\n      <td>760.400000</td>\n      <td>0.004773</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23.7300</td>\n      <td>26.2300</td>\n      <td>572.666667</td>\n      <td>769.666667</td>\n      <td>0.004765</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23.7225</td>\n      <td>26.1250</td>\n      <td>493.750000</td>\n      <td>774.750000</td>\n      <td>0.004744</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23.7540</td>\n      <td>26.2000</td>\n      <td>488.600000</td>\n      <td>779.000000</td>\n      <td>0.004767</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20555</th>\n      <td>21.0500</td>\n      <td>36.0975</td>\n      <td>433.000000</td>\n      <td>787.250000</td>\n      <td>0.005579</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20556</th>\n      <td>21.0500</td>\n      <td>35.9950</td>\n      <td>433.000000</td>\n      <td>789.500000</td>\n      <td>0.005563</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20557</th>\n      <td>21.1000</td>\n      <td>36.0950</td>\n      <td>433.000000</td>\n      <td>798.500000</td>\n      <td>0.005596</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20558</th>\n      <td>21.1000</td>\n      <td>36.2600</td>\n      <td>433.000000</td>\n      <td>820.333333</td>\n      <td>0.005621</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20559</th>\n      <td>21.1000</td>\n      <td>36.2000</td>\n      <td>447.000000</td>\n      <td>821.000000</td>\n      <td>0.005612</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>20560 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# load 'Occupancy' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "first = pd.read_csv(\"datasets/Occupancy/datatest.csv\")\n",
    "second = pd.read_csv(\"datasets/Occupancy/datatest2.csv\")\n",
    "third = pd.read_csv(\"datasets/Occupancy/datatraining.csv\")\n",
    "\n",
    "# # clean data\n",
    "# # replace string label classifiers into binary values\n",
    "# df = df.replace(to_replace=\"M\", value=1)\n",
    "# df = df.replace(to_replace=\"F\", value=0)\n",
    "# # drop all samples with NaN entries\n",
    "# df = df.dropna()\n",
    "\n",
    "# # move binary classifier(label) column to the end\n",
    "# # hold column\n",
    "# classifier = df['Gender']\n",
    "# # drop column from dataframe\n",
    "# df.drop(columns=['Gender', 'Name'], inplace=True)\n",
    "# # reinsert into dataframe at the end\n",
    "# df['Gender'] = classifier\n",
    "\n",
    "df = pd.concat([first, second, third])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(columns=['date'], inplace=True)\n",
    "df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.0s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.3s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.7s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.0s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    1.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.1s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# pre-declared values/arrays/functions to be used once inside the trial loop\n",
    "# C values for logistic regression regularization in range of 10(-8) to 10(4)\n",
    "Cvals = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# K values for k-nearest neighbors in range of 1 to 105 in steps of 4\n",
    "Kvals = np.linspace(1, 105, num=26, dtype=int).tolist()\n",
    "# max feature values for random forest similar to CNM06\n",
    "max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "# max depth values for decision trees (shallower = better)\n",
    "max_depths = np.linspace(1, 5, num=5, dtype=int).tolist()\n",
    "# array of performance metrics\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "# build parameter grids to be passed into GridSearchCV\n",
    "logreg_pgrid = {'classifier__penalty': ['l1','l2'], 'classifier__C': Cvals, 'classifier__max_iter': [5000]}\n",
    "knn_pgrid = {'classifier__weights': ['distance'], 'classifier__n_neighbors': Kvals}\n",
    "rforest_pgrid = {'classifier__n_estimators': [1024], 'classifier__max_features': max_features}\n",
    "dtree_pgrid = {'classifier__max_depth': max_depths}\n",
    "\n",
    "# arrays + dictionaries to store scores\n",
    "score_dict = [{}, {}, {}, {}, {}]\n",
    "\n",
    "# loop through this entire trial FIVE (5) times\n",
    "for i in range(5):\n",
    "    # slice the dataframe to not include the binary classifier (label)\n",
    "    # last column is the label (income>50K)\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "    # randomly pick 5000 samples with replacement for training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "    # make pipeline for each algorithms to condense model call\n",
    "    logreg = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', LogisticRegression(n_jobs=-1))])\n",
    "    knn = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "    rforest = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "    dtree = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    # 5-fold cross validation using Stratified KFold\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "    # GridSearchCV classifier for each algorithm\n",
    "    logreg_clf = GridSearchCV(estimator=logreg, param_grid=logreg_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    knn_clf = GridSearchCV(estimator=knn, param_grid=knn_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    rforest_clf = GridSearchCV(estimator=rforest, param_grid=rforest_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    dtree_clf = GridSearchCV(estimator=dtree, param_grid=dtree_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "\n",
    "\n",
    "\n",
    "    # for each classifier\n",
    "    for clf, clf_name in zip([logreg_clf, knn_clf, rforest_clf, dtree_clf], \n",
    "                ['LogReg', 'KNN', 'Ran_For', 'Dec_Tree']):\n",
    "        # fit to training data of 5000 samples\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # get parameters for each scoring metric's best\n",
    "        best_acc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_accuracy']) ]\n",
    "        best_f1_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_f1_micro']) ]\n",
    "        best_roc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_roc_auc_ovr']) ]\n",
    "\n",
    "        # get pipeline based on current classifier\n",
    "        if (clf_name == 'LogReg'):\n",
    "            pipe = logreg\n",
    "        elif (clf_name == 'KNN'):\n",
    "            pipe = knn\n",
    "        elif (clf_name == 'Ran_For'):\n",
    "            pipe = rforest\n",
    "        elif (clf_name == 'Dec_Tree'):\n",
    "            pipe = dtree\n",
    "\n",
    "        # set pipeline parameters to the parameters for best accuracy\n",
    "        pipe.set_params(**best_acc_param)\n",
    "        # fit classifier with training data and new parameters for scoring metric\n",
    "        pipe.fit(X_train, y_train)\n",
    "        # get predictions for both training and testing data\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        # get scores for all metrics from both training and testing data\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        # store all scores into a dictionary for accuracy metric\n",
    "        acc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        \n",
    "        # do ^^^^^ all that for f1 score\n",
    "        pipe.set_params(**best_f1_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        f1_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "\n",
    "        # do ^^^^^ all that for roc_auc score\n",
    "        pipe.set_params(**best_roc_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        roc_auc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        # build final dictionary to store all scores from all three models and their best parameters\n",
    "        score_dict[i][clf_name] = {'acc_dict': acc_dict, 'f1_dict': f1_dict, 'roc_auc_dict': roc_auc_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'LogReg': {'acc_dict': {'acc_train': 0.9888, 'f1_train': 0.9761295822676896, 'roc_auc_train': 0.9912027103331451, 'acc_test': 0.9892030848329049, 'f1_test': 0.9770804911323329, 'roc_auc_test': 0.9911320141211446}, 'f1_dict': {'acc_train': 0.9888, 'f1_train': 0.9761295822676896, 'roc_auc_train': 0.9912027103331451, 'acc_test': 0.9892030848329049, 'f1_test': 0.9770804911323329, 'roc_auc_test': 0.9911320141211446}, 'roc_auc_dict': {'acc_train': 0.9888, 'f1_train': 0.9761295822676896, 'roc_auc_train': 0.9912027103331451, 'acc_test': 0.9892030848329049, 'f1_test': 0.9770804911323329, 'roc_auc_test': 0.9911320141211446}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9910668380462725, 'f1_test': 0.9809562953829292, 'roc_auc_test': 0.9922473058342623}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9910668380462725, 'f1_test': 0.9809562953829292, 'roc_auc_test': 0.9922473058342623}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9899100257069409, 'f1_test': 0.9785665529010239, 'roc_auc_test': 0.9918831289483464}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9908097686375321, 'f1_test': 0.9803165863730212, 'roc_auc_test': 0.9902355072463769}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9906169665809769, 'f1_test': 0.9798953456348113, 'roc_auc_test': 0.9898188405797101}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.990745501285347, 'f1_test': 0.9801762114537446, 'roc_auc_test': 0.990096618357488}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9892, 'f1_train': 0.9770017035775128, 'roc_auc_train': 0.9920722755505363, 'acc_test': 0.9891388174807197, 'f1_test': 0.9769283276450511, 'roc_auc_test': 0.9907989594946116}, 'f1_dict': {'acc_train': 0.9892, 'f1_train': 0.9770017035775128, 'roc_auc_train': 0.9920722755505363, 'acc_test': 0.9891388174807197, 'f1_test': 0.9769283276450511, 'roc_auc_test': 0.9907989594946116}, 'roc_auc_dict': {'acc_train': 0.9896, 'f1_train': 0.9778346121057119, 'roc_auc_train': 0.9923320158102766, 'acc_test': 0.9893958868894601, 'f1_test': 0.977455936603361, 'roc_auc_test': 0.9908691007060572}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9888, 'f1_train': 0.9770303527481543, 'roc_auc_train': 0.9912062289651097, 'acc_test': 0.9887532133676092, 'f1_test': 0.9758919961427193, 'roc_auc_test': 0.991523488865209}, 'f1_dict': {'acc_train': 0.9888, 'f1_train': 0.9770303527481543, 'roc_auc_train': 0.9912062289651097, 'acc_test': 0.9887532133676092, 'f1_test': 0.9758919961427193, 'roc_auc_test': 0.991523488865209}, 'roc_auc_dict': {'acc_train': 0.9888, 'f1_train': 0.9770303527481543, 'roc_auc_train': 0.9912062289651097, 'acc_test': 0.9887532133676092, 'f1_test': 0.9758919961427193, 'roc_auc_test': 0.991523488865209}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9923521850899742, 'f1_test': 0.9834100097588179, 'roc_auc_test': 0.9923700455156028}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9923521850899742, 'f1_test': 0.9834100097588179, 'roc_auc_test': 0.9923700455156028}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9899100257069409, 'f1_test': 0.978317911890623, 'roc_auc_test': 0.9922731140526153}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9919665809768637, 'f1_test': 0.9825686794031516, 'roc_auc_test': 0.99172400762665}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9918380462724936, 'f1_test': 0.9822947162972258, 'roc_auc_test': 0.9916407159391604}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9919023136246786, 'f1_test': 0.9824316787506971, 'roc_auc_test': 0.9916823617829053}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9892, 'f1_train': 0.9777227722772277, 'roc_auc_train': 0.9897493924719271, 'acc_test': 0.9890102827763496, 'f1_test': 0.9763452759717803, 'roc_auc_test': 0.9904025430541153}, 'f1_dict': {'acc_train': 0.9892, 'f1_train': 0.9777227722772277, 'roc_auc_train': 0.9897493924719271, 'acc_test': 0.9890102827763496, 'f1_test': 0.9763452759717803, 'roc_auc_test': 0.9904025430541153}, 'roc_auc_dict': {'acc_train': 0.9914, 'f1_train': 0.9822972416632358, 'roc_auc_train': 0.9934881958438396, 'acc_test': 0.9890102827763496, 'f1_test': 0.9763321799307958, 'roc_auc_test': 0.9902044616408735}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9884, 'f1_train': 0.9748700173310225, 'roc_auc_train': 0.9909400653998307, 'acc_test': 0.9893958868894601, 'f1_test': 0.977645305514158, 'roc_auc_test': 0.9919355062605849}, 'f1_dict': {'acc_train': 0.9884, 'f1_train': 0.9748700173310225, 'roc_auc_train': 0.9909400653998307, 'acc_test': 0.9893958868894601, 'f1_test': 0.977645305514158, 'roc_auc_test': 0.9919355062605849}, 'roc_auc_dict': {'acc_train': 0.9886, 'f1_train': 0.9752925877763329, 'roc_auc_train': 0.991069264366239, 'acc_test': 0.9890102827763496, 'f1_test': 0.9768135593220338, 'roc_auc_test': 0.9911067769788168}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9906169665809769, 'f1_test': 0.9800437397484963, 'roc_auc_test': 0.9905175046503236}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9906169665809769, 'f1_test': 0.9800437397484963, 'roc_auc_test': 0.9905175046503236}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9899100257069409, 'f1_test': 0.9787234042553191, 'roc_auc_test': 0.9925592511359745}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.990745501285347, 'f1_test': 0.9802143446001649, 'roc_auc_test': 0.9888688377430429}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9908097686375321, 'f1_test': 0.9803490449361001, 'roc_auc_test': 0.9889107137899442}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9911311053984576, 'f1_test': 0.9810283200439923, 'roc_auc_test': 0.9892163395245103}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9884, 'f1_train': 0.9748263888888888, 'roc_auc_train': 0.9903135075804349, 'acc_test': 0.9895244215938304, 'f1_test': 0.9778743043301208, 'roc_auc_test': 0.991441785354026}, 'f1_dict': {'acc_train': 0.9884, 'f1_train': 0.9748263888888888, 'roc_auc_train': 0.9903135075804349, 'acc_test': 0.9895244215938304, 'f1_test': 0.9778743043301208, 'roc_auc_test': 0.991441785354026}, 'roc_auc_dict': {'acc_train': 0.9922, 'f1_train': 0.9829321663019693, 'roc_auc_train': 0.992768287942192, 'acc_test': 0.9888174807197944, 'f1_test': 0.9761774370208105, 'roc_auc_test': 0.9874200653358876}}}, {'LogReg': {'acc_dict': {'acc_train': 0.99, 'f1_train': 0.9797242497972425, 'roc_auc_train': 0.9922780292812807, 'acc_test': 0.9886246786632391, 'f1_test': 0.975481368610611, 'roc_auc_test': 0.9909430459082035}, 'f1_dict': {'acc_train': 0.99, 'f1_train': 0.9797242497972425, 'roc_auc_train': 0.9922780292812807, 'acc_test': 0.9886246786632391, 'f1_test': 0.975481368610611, 'roc_auc_test': 0.9909430459082035}, 'roc_auc_dict': {'acc_train': 0.9898, 'f1_train': 0.9793271179570328, 'roc_auc_train': 0.9921460335051455, 'acc_test': 0.9886246786632391, 'f1_test': 0.9755085097550851, 'roc_auc_test': 0.9913419753628617}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9898457583547557, 'f1_test': 0.9780555555555555, 'roc_auc_test': 0.9917332638419916}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9898457583547557, 'f1_test': 0.9780555555555555, 'roc_auc_test': 0.9917332638419916}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9883676092544987, 'f1_test': 0.9749619587771476, 'roc_auc_test': 0.991075881328926}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.990745501285347, 'f1_test': 0.9798149705634988, 'roc_auc_test': 0.9897224882326626}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.990681233933162, 'f1_test': 0.9796719472872564, 'roc_auc_test': 0.9895811654514303}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9910025706940874, 'f1_test': 0.9803701626472238, 'roc_auc_test': 0.9899885822665982}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.9914, 'f1_train': 0.9824989824989825, 'roc_auc_train': 0.9929214542362367, 'acc_test': 0.9888817480719795, 'f1_test': 0.9759220598469032, 'roc_auc_test': 0.9896134221235066}, 'f1_dict': {'acc_train': 0.9914, 'f1_train': 0.9824989824989825, 'roc_auc_train': 0.9929214542362367, 'acc_test': 0.9888817480719795, 'f1_test': 0.9759220598469032, 'roc_auc_test': 0.9896134221235066}, 'roc_auc_dict': {'acc_train': 0.9916, 'f1_train': 0.9829129373474369, 'roc_auc_train': 0.993333995490362, 'acc_test': 0.9888817480719795, 'f1_test': 0.9759287602615834, 'roc_auc_test': 0.9897131544871711}}}, {'LogReg': {'acc_dict': {'acc_train': 0.9914, 'f1_train': 0.9817564700890964, 'roc_auc_train': 0.9934985632183908, 'acc_test': 0.9881105398457584, 'f1_test': 0.9747440273037542, 'roc_auc_test': 0.9903222541032246}, 'f1_dict': {'acc_train': 0.9914, 'f1_train': 0.9817564700890964, 'roc_auc_train': 0.9934985632183908, 'acc_test': 0.9881105398457584, 'f1_test': 0.9747440273037542, 'roc_auc_test': 0.9903222541032246}, 'roc_auc_dict': {'acc_train': 0.9914, 'f1_train': 0.9817564700890964, 'roc_auc_train': 0.9934985632183908, 'acc_test': 0.9881105398457584, 'f1_test': 0.9747440273037542, 'roc_auc_test': 0.9903222541032246}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9924807197943445, 'f1_test': 0.9837882776777054, 'roc_auc_test': 0.9912125950903256}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9924807197943445, 'f1_test': 0.9837882776777054, 'roc_auc_test': 0.9912125950903256}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9886246786632391, 'f1_test': 0.9758691206543968, 'roc_auc_test': 0.9915339649029723}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9903598971722365, 'f1_test': 0.9793786087434699, 'roc_auc_test': 0.9910042050344059}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9904884318766067, 'f1_test': 0.9796591533809786, 'roc_auc_test': 0.9912827565664393}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.9906169665809769, 'f1_test': 0.9799395438307227, 'roc_auc_test': 0.9915613080984726}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.991, 'f1_train': 0.9808754781130472, 'roc_auc_train': 0.9923356681034483, 'acc_test': 0.9886889460154241, 'f1_test': 0.9759497130363487, 'roc_auc_test': 0.9907956986244628}, 'f1_dict': {'acc_train': 0.991, 'f1_train': 0.9808754781130472, 'roc_auc_train': 0.9923356681034483, 'acc_test': 0.9886889460154241, 'f1_test': 0.9759497130363487, 'roc_auc_test': 0.9907956986244628}, 'roc_auc_dict': {'acc_train': 0.9926, 'f1_train': 0.9842753931151722, 'roc_auc_train': 0.9945806393678162, 'acc_test': 0.9889460154241645, 'f1_test': 0.976483456385015, 'roc_auc_test': 0.9909627830020735}}}]\n"
     ]
    }
   ],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}