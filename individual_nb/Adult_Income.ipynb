{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9032458e503ab28519db53568226f597adad35e1b11ccc360aee2243f83ff687"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# importing necessarily libraries for the binary classification task\n",
    "\n",
    "# libraries imported for data processing and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# libraries imported for learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import pipeline\n",
    "\n",
    "# libraries imported for performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30162, 15)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# load 'Adult Income Census' data and names into pandas dataframe\n",
    "\n",
    "# make array of column names (based on adult.names)\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "    'martial-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', \n",
    "    'capital-loss', 'hours-per-week', 'native country', 'income>50K']\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/Adult_Income/adult.data\", names=column_names)\n",
    "\n",
    "# clean data\n",
    "# replace all '?' entries with NaN\n",
    "df = df.replace(to_replace=\" ?\", value=np.NaN)\n",
    "# change focus value (adult income > or <=50K) into binary value\n",
    "df = df.replace(to_replace=\" >50K\", value=1)\n",
    "df = df.replace(to_replace=\" <=50K\", value=0)\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "# save new cleaned up data into csv into dataset folder\n",
    "df.to_csv(\"datasets/Adult_Income/adult.csv\", index=False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30162, 105)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# one-hot encode the dataframe\n",
    "encoded = pd.get_dummies(df)\n",
    "\n",
    "# move binary classifier(label) column to the end\n",
    "# hold column\n",
    "classifier = encoded['income>50K']\n",
    "# drop column from dataframe\n",
    "encoded.drop(columns=['income>50K'], inplace=True)\n",
    "# reinsert into dataframe at the end\n",
    "encoded['income>50K'] = classifier\n",
    "\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    4.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   29.9s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.5s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.9s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   29.8s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.4s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    1.6s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.8s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   29.9s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   42.9s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    1.6s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.8s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   29.7s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   42.5s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.9s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:   29.8s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   43.8s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# pre-declared values/arrays/functions to be used once inside the trial loop\n",
    "# C values for logistic regression regularization in range of 10(-8) to 10(4)\n",
    "Cvals = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# K values for k-nearest neighbors in range of 1 to 105 in steps of 4\n",
    "Kvals = np.linspace(1, 105, num=26, dtype=int).tolist()\n",
    "# max feature values for random forest similar to CNM06\n",
    "max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "# max depth values for decision trees (shallower = better)\n",
    "max_depths = np.linspace(1, 5, num=5, dtype=int).tolist()\n",
    "# array of performance metrics\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "# build parameter grids to be passed into GridSearchCV\n",
    "logreg_pgrid = {'classifier__penalty': ['l1','l2'], 'classifier__C': Cvals, 'classifier__max_iter': [5000]}\n",
    "knn_pgrid = {'classifier__weights': ['distance'], 'classifier__n_neighbors': Kvals}\n",
    "rforest_pgrid = {'classifier__n_estimators': [1024], 'classifier__max_features': max_features}\n",
    "dtree_pgrid = {'classifier__max_depth': max_depths}\n",
    "\n",
    "# arrays + dictionaries to store scores\n",
    "score_dict = [{}, {}, {}, {}, {}]\n",
    "\n",
    "# loop through this entire trial FIVE (5) times\n",
    "for i in range(5):\n",
    "    # slice the dataframe to not include the binary classifier (label)\n",
    "    # last column is the label (income>50K)\n",
    "    X, y = encoded.iloc[:,:-1], encoded.iloc[:,-1]\n",
    "\n",
    "    # randomly pick 5000 samples with replacement for training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "    # make pipeline for each algorithms to condense model call\n",
    "    logreg = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', LogisticRegression(n_jobs=-1))])\n",
    "    knn = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "    rforest = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "    dtree = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    # 5-fold cross validation using Stratified KFold\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "    # GridSearchCV classifier for each algorithm\n",
    "    logreg_clf = GridSearchCV(estimator=logreg, param_grid=logreg_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    knn_clf = GridSearchCV(estimator=knn, param_grid=knn_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    rforest_clf = GridSearchCV(estimator=rforest, param_grid=rforest_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    dtree_clf = GridSearchCV(estimator=dtree, param_grid=dtree_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "\n",
    "\n",
    "\n",
    "    # for each classifier\n",
    "    for clf, clf_name in zip([logreg_clf, knn_clf, rforest_clf, dtree_clf], \n",
    "                ['LogReg', 'KNN', 'Ran_For', 'Dec_Tree']):\n",
    "        # fit to training data of 5000 samples\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # get parameters for each scoring metric's best\n",
    "        best_acc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_accuracy']) ]\n",
    "        best_f1_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_f1_micro']) ]\n",
    "        best_roc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_roc_auc_ovr']) ]\n",
    "\n",
    "        # get pipeline based on current classifier\n",
    "        if (clf_name == 'LogReg'):\n",
    "            pipe = logreg\n",
    "        elif (clf_name == 'KNN'):\n",
    "            pipe = knn\n",
    "        elif (clf_name == 'Ran_For'):\n",
    "            pipe = rforest\n",
    "        elif (clf_name == 'Dec_Tree'):\n",
    "            pipe = dtree\n",
    "\n",
    "        # set pipeline parameters to the parameters for best accuracy\n",
    "        pipe.set_params(**best_acc_param)\n",
    "        # fit classifier with training data and new parameters for scoring metric\n",
    "        pipe.fit(X_train, y_train)\n",
    "        # get predictions for both training and testing data\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        # get scores for all metrics from both training and testing data\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        # store all scores into a dictionary for accuracy metric\n",
    "        acc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        \n",
    "        # do ^^^^^ all that for f1 score\n",
    "        pipe.set_params(**best_f1_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        f1_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "\n",
    "        # do ^^^^^ all that for roc_auc score\n",
    "        pipe.set_params(**best_roc_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        roc_auc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        # build final dictionary to store all scores from all three models and their best parameters\n",
    "        score_dict[i][clf_name] = {'acc_dict': acc_dict, 'f1_dict': f1_dict, 'roc_auc_dict': roc_auc_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'LogReg': {'acc_dict': {'acc_train': 0.8544, 'f1_train': 0.6663611365719523, 'roc_auc_train': 0.768216438785954, 'acc_test': 0.8432954455130752, 'f1_test': 0.6487930880912086, 'roc_auc_test': 0.7550056099647564}, 'f1_dict': {'acc_train': 0.8544, 'f1_train': 0.6663611365719523, 'roc_auc_train': 0.768216438785954, 'acc_test': 0.8432954455130752, 'f1_test': 0.6487930880912086, 'roc_auc_test': 0.7550056099647564}, 'roc_auc_dict': {'acc_train': 0.8544, 'f1_train': 0.6663611365719523, 'roc_auc_train': 0.768216438785954, 'acc_test': 0.8432954455130752, 'f1_test': 0.6487930880912086, 'roc_auc_test': 0.7550056099647564}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8232652412367857, 'f1_test': 0.5769999048796729, 'roc_auc_test': 0.7094478298778829}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8232652412367857, 'f1_test': 0.5769999048796729, 'roc_auc_test': 0.7094478298778829}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8228678165487641, 'f1_test': 0.5698291670688157, 'roc_auc_test': 0.7049002290788561}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8485414513949606, 'f1_test': 0.6640810929925077, 'roc_auc_test': 0.7651133682674794}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8483427390509498, 'f1_test': 0.663254500529474, 'roc_auc_test': 0.7645049968297322}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8479850568317304, 'f1_test': 0.6626091558613391, 'roc_auc_test': 0.764160693043599}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.8422, 'f1_train': 0.6095992083127164, 'roc_auc_train': 0.7289052976674764, 'acc_test': 0.8397583657896829, 'f1_test': 0.6142365097588979, 'roc_auc_test': 0.7298063256170443}, 'f1_dict': {'acc_train': 0.8422, 'f1_train': 0.6095992083127164, 'roc_auc_train': 0.7289052976674764, 'acc_test': 0.8397583657896829, 'f1_test': 0.6142365097588979, 'roc_auc_test': 0.7298063256170443}, 'roc_auc_dict': {'acc_train': 0.8486, 'f1_train': 0.656065424806906, 'roc_auc_train': 0.7629832224202592, 'acc_test': 0.839321198632859, 'f1_test': 0.6430022075055187, 'roc_auc_test': 0.7523020473734977}}}, {'LogReg': {'acc_dict': {'acc_train': 0.856, 'f1_train': 0.6762589928057553, 'roc_auc_train': 0.7740970026739147, 'acc_test': 0.8439313250139099, 'f1_test': 0.6635249764373232, 'roc_auc_test': 0.7680073194734484}, 'f1_dict': {'acc_train': 0.856, 'f1_train': 0.6762589928057553, 'roc_auc_train': 0.7740970026739147, 'acc_test': 0.8439313250139099, 'f1_test': 0.6635249764373232, 'roc_auc_test': 0.7680073194734484}, 'roc_auc_dict': {'acc_train': 0.856, 'f1_train': 0.6762589928057553, 'roc_auc_train': 0.7740970026739147, 'acc_test': 0.8439313250139099, 'f1_test': 0.6635249764373232, 'roc_auc_test': 0.7680073194734484}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8241395755504332, 'f1_test': 0.5967374464594914, 'roc_auc_test': 0.7230593176495664}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8241395755504332, 'f1_test': 0.5967374464594914, 'roc_auc_test': 0.7230593176495664}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8252126222080916, 'f1_test': 0.5928531753379004, 'roc_auc_test': 0.7199504806576768}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8506875447102774, 'f1_test': 0.6819066971467276, 'roc_auc_test': 0.7807414006388983}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8497734679278277, 'f1_test': 0.6796610169491525, 'roc_auc_test': 0.7792294300638358}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.850091407678245, 'f1_test': 0.6797418916624215, 'roc_auc_test': 0.7790695284212628}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.84, 'f1_train': 0.6150144369586141, 'roc_auc_train': 0.73238419304056, 'acc_test': 0.8386853191320245, 'f1_test': 0.6241318640614871, 'roc_auc_test': 0.7378506200794306}, 'f1_dict': {'acc_train': 0.84, 'f1_train': 0.6150144369586141, 'roc_auc_train': 0.73238419304056, 'acc_test': 0.8386853191320245, 'f1_test': 0.6241318640614871, 'roc_auc_test': 0.7378506200794306}, 'roc_auc_dict': {'acc_train': 0.842, 'f1_train': 0.6223709369024856, 'roc_auc_train': 0.7370131862982837, 'acc_test': 0.839043001351244, 'f1_test': 0.6263148182321462, 'roc_auc_test': 0.7393635939919815}}}, {'LogReg': {'acc_dict': {'acc_train': 0.8584, 'f1_train': 0.6856127886323269, 'roc_auc_train': 0.7818659911806247, 'acc_test': 0.8446466894523488, 'f1_test': 0.6639731797472707, 'roc_auc_test': 0.7678304627334654}, 'f1_dict': {'acc_train': 0.8584, 'f1_train': 0.6856127886323269, 'roc_auc_train': 0.7818659911806247, 'acc_test': 0.8446466894523488, 'f1_test': 0.6639731797472707, 'roc_auc_test': 0.7678304627334654}, 'roc_auc_dict': {'acc_train': 0.8584, 'f1_train': 0.6856127886323269, 'roc_auc_train': 0.7818659911806247, 'acc_test': 0.8446466894523488, 'f1_test': 0.6639731797472707, 'roc_auc_test': 0.7678304627334654}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.822629361735951, 'f1_test': 0.6138271177641256, 'roc_auc_test': 0.7364476429339634}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.822629361735951, 'f1_test': 0.6138271177641256, 'roc_auc_test': 0.7364476429339634}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.82628566886575, 'f1_test': 0.608087510086972, 'roc_auc_test': 0.7306106261527782}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8485811938637629, 'f1_test': 0.6788062721294891, 'roc_auc_test': 0.7791513951315497}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8485414513949606, 'f1_test': 0.6793976613106755, 'roc_auc_test': 0.779761374517163}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8490183610205866, 'f1_test': 0.6797066014669926, 'roc_auc_test': 0.7797079814649592}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.856, 'f1_train': 0.6341463414634146, 'roc_auc_train': 0.739254108928626, 'acc_test': 0.8450838566091725, 'f1_test': 0.6246870787598691, 'roc_auc_test': 0.73534367669226}, 'f1_dict': {'acc_train': 0.856, 'f1_train': 0.6341463414634146, 'roc_auc_train': 0.739254108928626, 'acc_test': 0.8452825689531833, 'f1_test': 0.6256370804885086, 'roc_auc_test': 0.7359534791122646}, 'roc_auc_dict': {'acc_train': 0.856, 'f1_train': 0.6341463414634146, 'roc_auc_train': 0.739254108928626, 'acc_test': 0.8451633415467769, 'f1_test': 0.6253125601077131, 'roc_auc_test': 0.7357679306716162}}}, {'LogReg': {'acc_dict': {'acc_train': 0.8514, 'f1_train': 0.6923395445134576, 'roc_auc_train': 0.7843289203530384, 'acc_test': 0.8450441141403704, 'f1_test': 0.6701632687589884, 'roc_auc_test': 0.7753355663648704}, 'f1_dict': {'acc_train': 0.8514, 'f1_train': 0.6923395445134576, 'roc_auc_train': 0.7843289203530384, 'acc_test': 0.8450441141403704, 'f1_test': 0.6701632687589884, 'roc_auc_test': 0.7753355663648704}, 'roc_auc_dict': {'acc_train': 0.8514, 'f1_train': 0.6923395445134576, 'roc_auc_train': 0.7843289203530384, 'acc_test': 0.8450441141403704, 'f1_test': 0.6701632687589884, 'roc_auc_test': 0.7753355663648704}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8220332247039186, 'f1_test': 0.6205727842738519, 'roc_auc_test': 0.7438813803227051}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8220332247039186, 'f1_test': 0.6205727842738519, 'roc_auc_test': 0.7438813803227051}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8257292743025196, 'f1_test': 0.6220153435048701, 'roc_auc_test': 0.7434135820193106}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8433351879818775, 'f1_test': 0.671992011982027, 'roc_auc_test': 0.77836702135793}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8436928702010968, 'f1_test': 0.6731488406881077, 'roc_auc_test': 0.7792537381899362}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8441697798267228, 'f1_test': 0.6740377421232023, 'roc_auc_test': 0.7797867774857413}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.8336, 'f1_train': 0.6130232558139534, 'roc_auc_train': 0.7279209329920274, 'acc_test': 0.8401557904777045, 'f1_test': 0.6089830838032277, 'roc_auc_test': 0.7272378424939728}, 'f1_dict': {'acc_train': 0.8336, 'f1_train': 0.6130232558139534, 'roc_auc_train': 0.7279209329920274, 'acc_test': 0.8404339877593197, 'f1_test': 0.6099290780141844, 'roc_auc_test': 0.72780126812179}, 'roc_auc_dict': {'acc_train': 0.8336, 'f1_train': 0.6130232558139534, 'roc_auc_train': 0.7279209329920274, 'acc_test': 0.8404339877593197, 'f1_test': 0.6099290780141844, 'roc_auc_test': 0.72780126812179}}}, {'LogReg': {'acc_dict': {'acc_train': 0.8542, 'f1_train': 0.6705829191143244, 'roc_auc_train': 0.7719187045420487, 'acc_test': 0.8422223988554169, 'f1_test': 0.6500352609308886, 'roc_auc_test': 0.7567347763690447}, 'f1_dict': {'acc_train': 0.8542, 'f1_train': 0.6705829191143244, 'roc_auc_train': 0.7719187045420487, 'acc_test': 0.8422223988554169, 'f1_test': 0.6500352609308886, 'roc_auc_test': 0.7567347763690447}, 'roc_auc_dict': {'acc_train': 0.8544, 'f1_train': 0.6702898550724637, 'roc_auc_train': 0.7714896093622033, 'acc_test': 0.8436928702010968, 'f1_test': 0.651113279517431, 'roc_auc_test': 0.756815858813533}}, 'KNN': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8271600031793975, 'f1_test': 0.584979482775074, 'roc_auc_test': 0.7137784910795336}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8271600031793975, 'f1_test': 0.584979482775074, 'roc_auc_test': 0.7137784910795336}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8269215483665845, 'f1_test': 0.5774716212282915, 'roc_auc_test': 0.708910235781788}}, 'Ran_For': {'acc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8480247993005325, 'f1_test': 0.6663758506368871, 'roc_auc_test': 0.7675886604664934}, 'f1_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8474684047373022, 'f1_test': 0.6663769123783032, 'roc_auc_test': 0.7679584018014564}, 'roc_auc_dict': {'acc_train': 1.0, 'f1_train': 1.0, 'roc_auc_train': 1.0, 'acc_test': 0.8479055718941261, 'f1_test': 0.667304181517865, 'roc_auc_test': 0.7685144955306408}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.8536, 'f1_train': 0.6874466268146883, 'roc_auc_train': 0.789197082327029, 'acc_test': 0.8335585406565456, 'f1_test': 0.6496570185711896, 'roc_auc_test': 0.7613280955864613}, 'f1_dict': {'acc_train': 0.8536, 'f1_train': 0.6874466268146883, 'roc_auc_train': 0.789197082327029, 'acc_test': 0.8335585406565456, 'f1_test': 0.6496570185711896, 'roc_auc_test': 0.7613280955864613}, 'roc_auc_dict': {'acc_train': 0.8536, 'f1_train': 0.6874466268146883, 'roc_auc_train': 0.789197082327029, 'acc_test': 0.8335585406565456, 'f1_test': 0.6496570185711896, 'roc_auc_test': 0.7613280955864613}}}]\n"
     ]
    }
   ],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'encoded' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0dc2c8301231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# clean resulting score data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mencoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'column_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msome_value\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded' is not defined"
     ]
    }
   ],
   "source": [
    "# clean resulting score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}