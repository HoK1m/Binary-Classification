{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "9032458e503ab28519db53568226f597adad35e1b11ccc360aee2243f83ff687"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessarily libraries for the binary classification task\n",
    "\n",
    "# libraries imported for data processing and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "\n",
    "# libraries imported for learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import pipeline\n",
    "\n",
    "# libraries imported for performance metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Count   Probability  Gender\n",
       "0       5304407  1.451679e-02       1\n",
       "1       5260831  1.439753e-02       1\n",
       "2       4970386  1.360266e-02       1\n",
       "3       4579950  1.253414e-02       1\n",
       "4       4226608  1.156713e-02       1\n",
       "...         ...           ...     ...\n",
       "147264        1  2.736740e-09       1\n",
       "147265        1  2.736740e-09       1\n",
       "147266        1  2.736740e-09       1\n",
       "147267        1  2.736740e-09       1\n",
       "147268        1  2.736740e-09       1\n",
       "\n",
       "[147269 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Count</th>\n      <th>Probability</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5304407</td>\n      <td>1.451679e-02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5260831</td>\n      <td>1.439753e-02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4970386</td>\n      <td>1.360266e-02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4579950</td>\n      <td>1.253414e-02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4226608</td>\n      <td>1.156713e-02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>147264</th>\n      <td>1</td>\n      <td>2.736740e-09</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>147265</th>\n      <td>1</td>\n      <td>2.736740e-09</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>147266</th>\n      <td>1</td>\n      <td>2.736740e-09</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>147267</th>\n      <td>1</td>\n      <td>2.736740e-09</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>147268</th>\n      <td>1</td>\n      <td>2.736740e-09</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>147269 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# load 'Name-Gender' data and names into pandas dataframe\n",
    "\n",
    "# load data by using read_csv from .data file\n",
    "df = pd.read_csv(\"datasets/Name_Gender/name_gender.csv\")\n",
    "\n",
    "# clean data\n",
    "# replace string label classifiers into binary values\n",
    "df = df.replace(to_replace=\"M\", value=1)\n",
    "df = df.replace(to_replace=\"F\", value=0)\n",
    "# drop all samples with NaN entries\n",
    "df = df.dropna()\n",
    "\n",
    "# move binary classifier(label) column to the end\n",
    "# hold column\n",
    "classifier = df['Gender']\n",
    "# drop column from dataframe\n",
    "df.drop(columns=['Gender', 'Name'], inplace=True)\n",
    "# reinsert into dataframe at the end\n",
    "df['Gender'] = classifier\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    1.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   10.7s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    1.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   10.2s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 130 | elapsed:    1.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    9.8s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   10.9s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    0.3s finished\n",
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    1.2s finished\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   11.0s finished\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# pre-declared values/arrays/functions to be used once inside the trial loop\n",
    "# C values for logistic regression regularization in range of 10(-8) to 10(4)\n",
    "Cvals = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n",
    "# K values for k-nearest neighbors in range of 1 to 105 in steps of 4\n",
    "Kvals = np.linspace(1, 105, num=26, dtype=int).tolist()\n",
    "# max feature values for random forest similar to CNM06\n",
    "max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "# max depth values for decision trees (shallower = better)\n",
    "max_depths = np.linspace(1, 5, num=5, dtype=int).tolist()\n",
    "# array of performance metrics\n",
    "scoring = ['accuracy', 'f1_micro', 'roc_auc_ovr']\n",
    "\n",
    "# build parameter grids to be passed into GridSearchCV\n",
    "logreg_pgrid = {'classifier__penalty': ['l1','l2'], 'classifier__C': Cvals, 'classifier__max_iter': [5000]}\n",
    "knn_pgrid = {'classifier__weights': ['distance'], 'classifier__n_neighbors': Kvals}\n",
    "rforest_pgrid = {'classifier__n_estimators': [1024], 'classifier__max_features': max_features}\n",
    "dtree_pgrid = {'classifier__max_depth': max_depths}\n",
    "\n",
    "# arrays + dictionaries to store scores\n",
    "score_dict = [{}, {}, {}, {}, {}]\n",
    "\n",
    "# loop through this entire trial FIVE (5) times\n",
    "for i in range(5):\n",
    "    # slice the dataframe to not include the binary classifier (label)\n",
    "    # last column is the label (income>50K)\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "    # randomly pick 5000 samples with replacement for training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, shuffle=True)\n",
    "\n",
    "    # make pipeline for each algorithms to condense model call\n",
    "    logreg = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', LogisticRegression(n_jobs=-1))])\n",
    "    knn = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', KNeighborsClassifier(n_jobs=-1))])\n",
    "    rforest = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "    dtree = pipeline.Pipeline([('scale', StandardScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    # 5-fold cross validation using Stratified KFold\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\n",
    "    # GridSearchCV classifier for each algorithm\n",
    "    logreg_clf = GridSearchCV(estimator=logreg, param_grid=logreg_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    knn_clf = GridSearchCV(estimator=knn, param_grid=knn_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    rforest_clf = GridSearchCV(estimator=rforest, param_grid=rforest_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "    dtree_clf = GridSearchCV(estimator=dtree, param_grid=dtree_pgrid, scoring=scoring, \n",
    "                                n_jobs=-1, cv=k_fold, verbose=2, refit=False)\n",
    "\n",
    "\n",
    "\n",
    "    # for each classifier\n",
    "    for clf, clf_name in zip([logreg_clf, knn_clf, rforest_clf, dtree_clf], \n",
    "                ['LogReg', 'KNN', 'Ran_For', 'Dec_Tree']):\n",
    "        # fit to training data of 5000 samples\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # get parameters for each scoring metric's best\n",
    "        best_acc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_accuracy']) ]\n",
    "        best_f1_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_f1_micro']) ]\n",
    "        best_roc_param = clf.cv_results_['params'][ np.argmin(clf.cv_results_['rank_test_roc_auc_ovr']) ]\n",
    "\n",
    "        # get pipeline based on current classifier\n",
    "        if (clf_name == 'LogReg'):\n",
    "            pipe = logreg\n",
    "        elif (clf_name == 'KNN'):\n",
    "            pipe = knn\n",
    "        elif (clf_name == 'Ran_For'):\n",
    "            pipe = rforest\n",
    "        elif (clf_name == 'Dec_Tree'):\n",
    "            pipe = dtree\n",
    "\n",
    "        # set pipeline parameters to the parameters for best accuracy\n",
    "        pipe.set_params(**best_acc_param)\n",
    "        # fit classifier with training data and new parameters for scoring metric\n",
    "        pipe.fit(X_train, y_train)\n",
    "        # get predictions for both training and testing data\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        # get scores for all metrics from both training and testing data\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        # store all scores into a dictionary for accuracy metric\n",
    "        acc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        \n",
    "        # do ^^^^^ all that for f1 score\n",
    "        pipe.set_params(**best_f1_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        f1_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "\n",
    "        # do ^^^^^ all that for roc_auc score\n",
    "        pipe.set_params(**best_roc_param)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_train_pred = pipe.predict(X_train)\n",
    "        y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "        acc_train = accuracy_score(y_train, y_train_pred)\n",
    "        f1_train = f1_score(y_train, y_train_pred)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "        acc_test = accuracy_score(y_test, y_test_pred)\n",
    "        f1_test = f1_score(y_test, y_test_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        roc_auc_dict = {'acc_train': acc_train, 'f1_train': f1_train, 'roc_auc_train': roc_auc_train, \n",
    "                    'acc_test': acc_test, 'f1_test': f1_test, 'roc_auc_test': roc_auc_test}\n",
    "\n",
    "        # build final dictionary to store all scores from all three models and their best parameters\n",
    "        score_dict[i][clf_name] = {'acc_dict': acc_dict, 'f1_dict': f1_dict, 'roc_auc_dict': roc_auc_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'LogReg': {'acc_dict': {'acc_train': 0.6058, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6095495153547154, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'f1_dict': {'acc_train': 0.6058, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6095495153547154, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'roc_auc_dict': {'acc_train': 0.6058, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6095495153547154, 'f1_test': 0.0, 'roc_auc_test': 0.5}}, 'KNN': {'acc_dict': {'acc_train': 0.679, 'f1_train': 0.38054805094558086, 'roc_auc_train': 0.6040994050577212, 'acc_test': 0.5827903478621485, 'f1_test': 0.16681639528354855, 'roc_auc_test': 0.49727467092731326}, 'f1_dict': {'acc_train': 0.679, 'f1_train': 0.38054805094558086, 'roc_auc_train': 0.6040994050577212, 'acc_test': 0.5827903478621485, 'f1_test': 0.16681639528354855, 'roc_auc_test': 0.49727467092731326}, 'roc_auc_dict': {'acc_train': 0.679, 'f1_train': 0.38054805094558086, 'roc_auc_train': 0.6040994050577212, 'acc_test': 0.5825232482128925, 'f1_test': 0.16940761872797452, 'roc_auc_test': 0.4974276436252319}}, 'Ran_For': {'acc_dict': {'acc_train': 0.679, 'f1_train': 0.4088397790055249, 'roc_auc_train': 0.6095930610893279, 'acc_test': 0.5724226641081331, 'f1_test': 0.2162670548977673, 'roc_auc_test': 0.49670020408579807}, 'f1_dict': {'acc_train': 0.679, 'f1_train': 0.4075304540420819, 'roc_auc_train': 0.6093272390232822, 'acc_test': 0.5725983875615911, 'f1_test': 0.21335610235711144, 'roc_auc_test': 0.49636874537034853}, 'roc_auc_dict': {'acc_train': 0.679, 'f1_train': 0.40134278254382694, 'roc_auc_train': 0.6080867360484034, 'acc_test': 0.5752974998067042, 'f1_test': 0.20901188668377232, 'roc_auc_test': 0.4977318627103533}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.6058, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6095495153547154, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'f1_dict': {'acc_train': 0.6058, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6095495153547154, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'roc_auc_dict': {'acc_train': 0.609, 'f1_train': 0.03265710044532409, 'roc_auc_train': 0.5055651784148462, 'acc_test': 0.6082983643660952, 'f1_test': 0.022727671290531888, 'roc_auc_test': 0.5010702343826757}}}, {'LogReg': {'acc_dict': {'acc_train': 0.603, 'f1_train': 0.002011060834590246, 'roc_auc_train': 0.5005032712632109, 'acc_test': 0.6097463256225882, 'f1_test': 0.0006479831524380367, 'roc_auc_test': 0.5001274780842705}, 'f1_dict': {'acc_train': 0.603, 'f1_train': 0.002011060834590246, 'roc_auc_train': 0.5005032712632109, 'acc_test': 0.6097463256225882, 'f1_test': 0.0006479831524380367, 'roc_auc_test': 0.5001274780842705}, 'roc_auc_dict': {'acc_train': 0.6026, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6096619783649284, 'f1_test': 0.0, 'roc_auc_test': 0.5}}, 'KNN': {'acc_dict': {'acc_train': 0.6778, 'f1_train': 0.3684829478635829, 'roc_auc_train': 0.6026696761608938, 'acc_test': 0.5845967849636955, 'f1_test': 0.16709181875836798, 'roc_auc_test': 0.49864437622161584}, 'f1_dict': {'acc_train': 0.6778, 'f1_train': 0.3684829478635829, 'roc_auc_train': 0.6026696761608938, 'acc_test': 0.5845967849636955, 'f1_test': 0.16709181875836798, 'roc_auc_test': 0.49864437622161584}, 'roc_auc_dict': {'acc_train': 0.6742, 'f1_train': 0.4048227986846913, 'roc_auc_train': 0.6068804180375227, 'acc_test': 0.5747000400649473, 'f1_test': 0.21016356207657266, 'roc_auc_test': 0.4974010278278127}}, 'Ran_For': {'acc_dict': {'acc_train': 0.6778, 'f1_train': 0.396855110445526, 'roc_auc_train': 0.6078109604229684, 'acc_test': 0.5747000400649473, 'f1_test': 0.2113371827791609, 'roc_auc_test': 0.4975856529031916}, 'f1_dict': {'acc_train': 0.6778, 'f1_train': 0.39367707941287167, 'roc_auc_train': 0.6072111439257263, 'acc_test': 0.5754661943220237, 'f1_test': 0.20816508469243272, 'roc_auc_test': 0.49766983826534844}, 'roc_auc_dict': {'acc_train': 0.6778, 'f1_train': 0.4013377926421405, 'roc_auc_train': 0.6086678411333141, 'acc_test': 0.574446998291968, 'f1_test': 0.21349234186835028, 'roc_auc_test': 0.49773442062164563}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.606, 'f1_train': 0.017946161515453644, 'roc_auc_train': 0.504363493808327, 'acc_test': 0.6089309687985436, 'f1_test': 0.007563190096502024, 'roc_auc_test': 0.5000871553352598}, 'f1_dict': {'acc_train': 0.606, 'f1_train': 0.017946161515453644, 'roc_auc_train': 0.504363493808327, 'acc_test': 0.6089309687985436, 'f1_test': 0.007563190096502024, 'roc_auc_test': 0.5000871553352598}, 'roc_auc_dict': {'acc_train': 0.606, 'f1_train': 0.017946161515453644, 'roc_auc_train': 0.504363493808327, 'acc_test': 0.6089309687985436, 'f1_test': 0.007563190096502024, 'roc_auc_test': 0.5000871553352598}}}, {'LogReg': {'acc_dict': {'acc_train': 0.6142, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092542999529061, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'f1_dict': {'acc_train': 0.6142, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092542999529061, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'roc_auc_dict': {'acc_train': 0.6142, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092542999529061, 'f1_test': 0.0, 'roc_auc_test': 0.5}}, 'KNN': {'acc_dict': {'acc_train': 0.6884, 'f1_train': 0.370250606305578, 'roc_auc_train': 0.6045495925950871, 'acc_test': 0.5866703217144986, 'f1_test': 0.1593423874195854, 'roc_auc_test': 0.4994431863230621}, 'f1_dict': {'acc_train': 0.6884, 'f1_train': 0.370250606305578, 'roc_auc_train': 0.6045495925950871, 'acc_test': 0.5866703217144986, 'f1_test': 0.1593423874195854, 'roc_auc_test': 0.4994431863230621}, 'roc_auc_dict': {'acc_train': 0.6884, 'f1_train': 0.370250606305578, 'roc_auc_train': 0.6045495925950871, 'acc_test': 0.586276701178753, 'f1_test': 0.1645375574859479, 'roc_auc_test': 0.49983950174950703}}, 'Ran_For': {'acc_dict': {'acc_train': 0.6884, 'f1_train': 0.3899765074393109, 'roc_auc_train': 0.6084051223176933, 'acc_test': 0.5804637693383661, 'f1_test': 0.1911563427425366, 'roc_auc_test': 0.49912378377732874}, 'f1_dict': {'acc_train': 0.6884, 'f1_train': 0.3993831919814958, 'roc_auc_train': 0.6103328871789964, 'acc_test': 0.5784956666596377, 'f1_test': 0.20062119252969327, 'roc_auc_test': 0.4990311810059852}, 'roc_auc_dict': {'acc_train': 0.6884, 'f1_train': 0.3998459167950693, 'roc_auc_train': 0.6104292754220615, 'acc_test': 0.5786924769275106, 'f1_test': 0.19988520016552536, 'roc_auc_test': 0.49907011850724525}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.6154, 'f1_train': 0.0302571860816944, 'roc_auc_train': 0.5038685277869074, 'acc_test': 0.6075954705522637, 'f1_test': 0.02619965462505887, 'roc_auc_test': 0.501061204271041}, 'f1_dict': {'acc_train': 0.6154, 'f1_train': 0.0302571860816944, 'roc_auc_train': 0.5038685277869074, 'acc_test': 0.6075954705522637, 'f1_test': 0.02619965462505887, 'roc_auc_test': 0.501061204271041}, 'roc_auc_dict': {'acc_train': 0.6198, 'f1_train': 0.03746835443037975, 'roc_auc_train': 0.5081251406365236, 'acc_test': 0.607560325861572, 'f1_test': 0.018321201251890143, 'roc_auc_test': 0.5002904312857483}}}, {'LogReg': {'acc_dict': {'acc_train': 0.615, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092261842003529, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'f1_dict': {'acc_train': 0.615, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092261842003529, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'roc_auc_dict': {'acc_train': 0.615, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092261842003529, 'f1_test': 0.0, 'roc_auc_test': 0.5}}, 'KNN': {'acc_dict': {'acc_train': 0.686, 'f1_train': 0.3648867313915858, 'roc_auc_train': 0.601533101045296, 'acc_test': 0.5861080066634333, 'f1_test': 0.14546932141405933, 'roc_auc_test': 0.49718966334788384}, 'f1_dict': {'acc_train': 0.686, 'f1_train': 0.3648867313915858, 'roc_auc_train': 0.601533101045296, 'acc_test': 0.5861080066634333, 'f1_test': 0.14546932141405933, 'roc_auc_test': 0.49718966334788384}, 'roc_auc_dict': {'acc_train': 0.6736, 'f1_train': 0.4329395413481584, 'roc_auc_train': 0.6081596452328161, 'acc_test': 0.5652812629596047, 'f1_test': 0.2486180461906671, 'roc_auc_test': 0.4969307101508659}}, 'Ran_For': {'acc_dict': {'acc_train': 0.686, 'f1_train': 0.3970814132104454, 'roc_auc_train': 0.6079442508710801, 'acc_test': 0.5757192360950031, 'f1_test': 0.19502307097324834, 'roc_auc_test': 0.4960806670630952}, 'f1_dict': {'acc_train': 0.686, 'f1_train': 0.39846743295019155, 'roc_auc_train': 0.6082356667722522, 'acc_test': 0.5749319950235118, 'f1_test': 0.19851031119122092, 'roc_auc_test': 0.4960053702111942}, 'roc_auc_dict': {'acc_train': 0.686, 'f1_train': 0.41110277569392345, 'roc_auc_train': 0.6109555485165241, 'acc_test': 0.5725280981802079, 'f1_test': 0.21005871044838154, 'roc_auc_test': 0.49595770874428474}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.615, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092261842003529, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'f1_dict': {'acc_train': 0.615, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6092261842003529, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'roc_auc_dict': {'acc_train': 0.619, 'f1_train': 0.04319437468608739, 'roc_auc_train': 0.5074289937704571, 'acc_test': 0.6061896829246006, 'f1_test': 0.02621013296254454, 'roc_auc_test': 0.4999394565815998}}}, {'LogReg': {'acc_dict': {'acc_train': 0.616, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6091910395096612, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'f1_dict': {'acc_train': 0.616, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6091910395096612, 'f1_test': 0.0, 'roc_auc_test': 0.5}, 'roc_auc_dict': {'acc_train': 0.616, 'f1_train': 0.0, 'roc_auc_train': 0.5, 'acc_test': 0.6091910395096612, 'f1_test': 0.0, 'roc_auc_test': 0.5}}, 'KNN': {'acc_dict': {'acc_train': 0.6862, 'f1_train': 0.3561756257693886, 'roc_auc_train': 0.5995468073593074, 'acc_test': 0.5892710288256753, 'f1_test': 0.13875132649451713, 'roc_auc_test': 0.4988245387375155}, 'f1_dict': {'acc_train': 0.6862, 'f1_train': 0.3561756257693886, 'roc_auc_train': 0.5995468073593074, 'acc_test': 0.5892710288256753, 'f1_test': 0.13875132649451713, 'roc_auc_test': 0.4988245387375155}, 'roc_auc_dict': {'acc_train': 0.642, 'f1_train': 0.45493300852618757, 'roc_auc_train': 0.5943689123376623, 'acc_test': 0.5459305962648223, 'f1_test': 0.30533152676516767, 'roc_auc_test': 0.4938456201428224}}, 'Ran_For': {'acc_dict': {'acc_train': 0.6862, 'f1_train': 0.3795966785290629, 'roc_auc_train': 0.6040584415584416, 'acc_test': 0.5811034027089528, 'f1_test': 0.1825189981070478, 'roc_auc_test': 0.4983942522452287}, 'f1_dict': {'acc_train': 0.6862, 'f1_train': 0.3820401732965735, 'roc_auc_train': 0.6045488365800866, 'acc_test': 0.579950656854269, 'f1_test': 0.18567574196713268, 'roc_auc_test': 0.4979639207209502}, 'roc_auc_dict': {'acc_train': 0.6862, 'f1_train': 0.37614314115308145, 'roc_auc_train': 0.6033718885281385, 'acc_test': 0.5814407917395918, 'f1_test': 0.1810430190339971, 'roc_auc_test': 0.4984422830149039}}, 'Dec_Tree': {'acc_dict': {'acc_train': 0.6204, 'f1_train': 0.04044489383215368, 'roc_auc_train': 0.5074945887445886, 'acc_test': 0.6070190976249218, 'f1_test': 0.025856811806317843, 'roc_auc_test': 0.5006093635746885}, 'f1_dict': {'acc_train': 0.6204, 'f1_train': 0.04044489383215368, 'roc_auc_train': 0.5074945887445886, 'acc_test': 0.6070190976249218, 'f1_test': 0.025856811806317843, 'roc_auc_test': 0.5006093635746885}, 'roc_auc_dict': {'acc_train': 0.6204, 'f1_train': 0.04044489383215368, 'roc_auc_train': 0.5074945887445886, 'acc_test': 0.6070190976249218, 'f1_test': 0.025856811806317843, 'roc_auc_test': 0.5006093635746885}}}]\n"
     ]
    }
   ],
   "source": [
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}